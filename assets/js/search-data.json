{
  
    
        "post0": {
            "title": "[ML]로지스틱 회귀 유도 및 scikit-learn api 맛보기",
            "content": ". Intro . import matplotlib.pyplot as plt import numpy as np fig, ax = plt.subplots(1,3, figsize=(12,4)) # 시그모이드 def sigmoid(z): return 1/(1 + np.exp(-z)) z = np.linspace(-3, 3, 100) ax[0].plot(z, sigmoid(z)) ax[0].text(-2.7,0.78,&#39;$y = 1/(1+e^{-z})$&#39;, fontsize = 13) ax[0].axvline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[0].set_ylim(-0.1, 1.1) ax[0].set_title(&#39;Sigmoid&#39;) ax[0].set_xlabel(&#39;z = wx&#39;) ax[0].set_ylabel(&#39;phi(z)&#39;) ax[0].set_yticks([0, 0.5, 1]) ax[0].yaxis.grid() plt.tight_layout() # 로짓 def logit(p): return np.log(p/(1-p)) p = np.linspace(0.06, 0.94, 100) ax[1].plot(p, logit(p)) ax[1].text(0.08,1.6,&#39;$y = log(p/(1-p))$&#39;, fontsize = 13) ax[1].axhline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[1].set_xlim(-0.1, 1.1) ax[1].set_title(&#39;Logit&#39;) ax[1].set_xlabel(&#39;p&#39;) ax[1].set_ylabel(&#39;logit(p)&#39;) ax[1].set_xticks([0, 0.5, 1]) ax[1].xaxis.grid() plt.tight_layout() # 합성함수 p = np.linspace(0.06, 0.94, 100) ax[2].plot(p, sigmoid(logit(p))) ax[2].text(0.62,0.42,&#39;$y = p$&#39;, fontsize = 15) ax[2].set_xlim(0, 1) ax[2].set_ylim(0, 1) ax[2].set_title(&#39;Sigmoid(Logit)&#39;) ax[2].set_xlabel(&#39;p&#39;) ax[2].set_ylabel(&#39;phi(logit(p))&#39;) ax[2].grid() plt.tight_layout() plt.show() . . 나는 학교 회귀분석 과목에서 로지스틱 함수를 배울때 왜 &#39;오즈&#39; 라는 개념을 굳이 사용할까 라는 의문을 항상 가지면서 공부했다. 교수님께 여쭤보았을 땐 그저 &quot;오즈는 승산이죠~&quot; 라는 답변을 주셨지만 도통 그 의미를 이해할 수 없었다. 어차피 확률과 일대일 대응인 오즈라는 개념을 굳이 왜 쓰는 것인가? 확률이 더 직관적이고 좋은데 말이다. 고민 끝에 내린 답은 선형 회귀에서의 예측되는 확률이 엇나가는 것을 로짓 변환을 통해 미연에 방지할 수 있기 때문이라는 것이다. . 확인을 위해 확률을 단순 선형회귀로 예측하는 것과 로지스틱 회귀로 예측한 것을 비교해보자. . Data preparation . np.random.seed(1) data0_input = 0.3*np.random.randn(30) data1_input = 1 + 0.5*np.random.randn(30) data0_target = np.zeros(30) data1_target = np.ones(30) data0 = np.column_stack((data0_input, data0_target)) data1 = np.column_stack((data1_input, data1_target)) # 데이터 로드 data = np.row_stack((data0, data1)) data = np.random.permutation(data) print(data[:5]) . [[-0.20511836 0. ] [-0.03686707 0. ] [ 0.17484456 0. ] [ 0.44134483 1. ] [ 0.82532864 1. ]] . plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . &#54869;&#47456; &#50696;&#52769;&#49884; Linear Regression&#51032; &#47928;&#51228;&#51216; &#48143; activation function &#52628;&#47200; . input, target = data[:,0].reshape(-1,1), data[:,1].reshape(-1,) # 단순선형회귀 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(input, target) print(lr.score(input, target)) print(lr.coef_, lr.intercept_) . 0.6788575359335094 [0.64269595] 0.17212798057869722 . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, 0.6427*x + 0.1721, label=&#39;lr&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.vlines(-0.1721/0.6427, 0., 0.5, linestyles=&#39;--&#39;) plt.text(-0.5, 0.7, &#39;Does it looks like p=0.5?&#39;) plt.grid() plt.legend() plt.show() . 단순 선형회귀로 확률을 추정해보았으나 확률이 0과 1을 넘어가는 기이한 형상을 띤다. 이는 잘못된 추정이라 볼 수 있다. 시그모이드가 0과 1사이의 값으로 바꿔준다는 말에 혹해서 시그모이드 함수에 단순선형회귀식을 넣어보더라도 전혀 데이터의 분포를 말해주지 못하는 것 같다. . 문제를 해결할 방법으로는 단순선형회귀로 추정값이 로그값이면 된다. 단순선형회귀 추정값의 범위로 실수 전체가 타당해질 수 있다. 그렇다면 $ log(p) = mathbf{w^T x}$ 로 추정한다면 괜찮을까? 아쉽지만 로그확률을 다시 확률로 해석할 때엔 지수함수가 쓰인다(활성화 함수).따라서 $p = e^{ log(p)} = e^{ mathbf{w^T x}}$가 0 근처에선 좋은 확률의 추정치를 줄 수는 있지만 1을 넘어가는 외삽에선 그다지 쓸모가 없을 것이다. . | 활성화 함수의 범위도 중요하다는 것을 위에서 알았다. 이제 추정값과 활성화함수 짝꿍을 위한 조건은 아래와 같다. . 단순선형회귀로 추정하는 대상이 로그값이면서 | 확률로의 활성화함수가 0과 1사이의 값을 가져야한다. | | . 마침 로그오즈가 확률$p$의 함수이면서 동시에 역함수(시그모이드 함수)가 0과 1사이의 값인 것이다. 따라서 아래와 같은 추정이 가능해졌다. . $$inverse of log( frac{p}{1-p})= frac{1}{1+e^{-z}}$$ . $$ log( frac{p}{1-p}) = z = mathbf{w^T x}$$ . $$ frac{1}{1+e^{-z}} = frac{1}{1+e^{- mathbf{w^T x}}} = frac{1}{1+e^{- log(p/1-p)}} = hat{p}$$ . 추정과정만 보면 $ hat{p} = frac{1}{1+e^{- mathbf{w^T x}}}$ 이겠다. . L2&#44144;&#47532; &#48708;&#50857;&#54632;&#49688;&#51032; &#47928;&#51228;&#51216; . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(20*(x-0.5)), label=&#39;rule of thumb sigmoid&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) # 앞서 선형모형에서 최적화한 계수들을 그저 시그모이드 함수에 넣기만 했기 때문에 시그모이드 자체에 대해서 최적화가 이루어지진 않았음 plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.grid() plt.legend(loc=&#39;upper left&#39;) plt.show() . 그렇다면 $ mathbf{w^Tx} = log( frac{p}{1-p})$로 생각하여 이를 단순선형회귀로 추정하고 시그모이드 함수에 넣어 확률을 추정하는 셈이 되는 것인가?. . 그건 또 아니다. $ mathbf{w^Tx} = log( frac{p}{1-p})$에서 선형회귀로 추정해버린다면 위의 그래프에서 선형회귀식을 시그모이드 함수에 넣은 것과 같다. . (단순회귀로 추정한 것이 확률이다! 라고 정의한 것에서 사실 로그오즈로 추정한거였다! 로 바뀌었을 뿐이다.) . 결국 이 경우는 회귀직선과 데이터포인트들의 거리비용을 최적화 한 것이지 시그모이드에 회귀직선을 대입한 것과 데이터포인트들과의 거리비용을 최적화한 것이 아니기에 활성화 함수와 포인트들 간에 괴리가 있다. . 그렇다면 아예 비용함수를 $ sum(y^{(i)} - frac{1}{1+e^{- mathbf{w^T x^{(i)}}}})^2$로 정의하면 어떨까? 비용함수를 만들어보자. . $$J( mathbf{w}) = frac{1}{2} sum^{n}_{i=1}( phi( mathbf{w^Tx^{(i)}}) - y^{(i)})^2$$ . $y$들은 0 또는 1의 값을 가지고 비용함수가 L2(유클리디안) 거리비용함수로 정의되어 있으므로 최적화하여 적합되는 시그모이드 함수의 모양은 아래와 같을 것이다. 두 클래스를 구분하는 데에만 초점을 맞춘다면 의미가 없진 않겠지만 확률적 의미를 부여하기엔 너무나 부족해보인다. . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(1000*x - 500)) # 유클리디언 거리가 최소가 되도록 한다면 점에 붙으려 할 것이다. plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp . Likelihood, Log Likelyhood function&#44284; &#44221;&#49324;&#54616;&#44053;&#48277; . 비용함수를 다르게 정의해야할 필요성을 위에서 보았다. 데이터가 주어져 있으니 여러 데이터포인트들이 그렇게 나올법한 확률을 최대화 시키는 가중치들을 구하면 될 것이다. 가능도함수를 목적함수로 하여 최대화하는 가중치를 찾는 것이 적당해보인다. . 마침 $z^{(i)}$를 로그오즈로 생각하기로 했으니 $ phi(z^{(i)}) = hat{p}^{(i)}$ 즉, 각각을 i번째 데이터포인트의 양성 확률로 생각할 수 있다. 각 데이터포인트들이 서로 독립적이라는 가정하에 가능도함수는 아래와 같다. . $$L( mathbf{w}) = P( mathbf{y} | mathbf{x;w}) = prod^n_{i=1}( phi(z^{(i)}))^{y^{(i)}}(1- phi(z^{(i)}))^{1-y^{(i)}}$$ . 가능도함수를 최대화 하는 것은 로그 가능도함수를 최대화하는 것과 동일하므로 다루기 쉬운 로그 가능도함수를 최대화하자. . $$l( mathbf{w}) = sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 가능도함수에 로그를 적용하면 가능도가 매우 작을 때 0으로 생략되는 것을 미연에 방지한다. 도함수도 쉽게 구할 수 있으니 일석이조다. . 경사하강법 최적화 알고리즘 사용을 위해 로그 가능도함수를 비용함수로 표현하자. . $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 비용함수의 이해를 위해 샘플이 하나일 때의 비용을 계산해보자. 해석의 편의를 위해 가중치 대신 가중치마다의 시그모이드와 참값$y$를 변수로 생각하자.(사실 샘플이 하나라서 $ mathbf{w}$ 대신 $ phi(z)$의 함수로 봐도 좋다.) . $$J( phi(z), y; mathbf{w}) = -y log phi(z) - (1-y) log (1 - phi(z))$$ . $y=1$일 때와 $y=0$일 때를 나누어 생각하면 3차원 상의 비용함수 그래프를 2차원에 그릴 수 있다. . $$J( phi(z), y; mathbf{w}) = left { begin{matrix} - log phi(z)&amp; y=1 - log (1 - phi(z))&amp; y=0 end{matrix} right.$$ . def cost_1(z): return - np.log(sigmoid(z)) def cost_0(z): return -np.log(1 - sigmoid(z)) z = np.linspace(-4, 4, 100) phi_z = sigmoid(z) plt.plot(phi_z, cost_1(z), label=&#39;J(w) where y = 1&#39;) plt.plot(phi_z, cost_0(z), linestyle=&#39;--&#39;, label=&#39;J(w) where y = 0&#39;) plt.xlim([0, 1]) plt.ylim([0, 4.1]) plt.xlabel(&#39;$ phi$(z)&#39;) plt.ylabel(&#39;J(w)&#39;) plt.legend(loc = &#39;upper center&#39;) plt.tight_layout() plt.show() . 범주 1에 속하는 샘플이 범주 1에 속할 확률을 높게 예측할수록 그렇게 예측한 가중치의 비용은 0에 가까워졌고 범주 0에 속하는 샘플이 범주 0에 속할 확률을 높게 예측할수록 역시 비용이 0에 가까워졌다. 반대로 잘못된 예측확률에는 큰 비용을 부여한다. . 즉, (맞으면 비용감소, 틀리면 비용증가)이므로 직관과 일치한다. . 클래스 1을 1같다고 하면 비용이 작아지고 . | 클래스 1을 0이라고 하면 비용이 커진다. . | . (클래스 0의 경우 반대) . 알고있는 기존의 경사하강법 규칙으로부터 로지스틱 회귀에서의 경사하강법 규칙이 잘 일반화 되어있는지 확인해보자. . $J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$를 $w_j$에 대하여 편미분하면 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)} frac{1}{ phi(z^{(i)})} + (1-y^{(i)}) frac{1}{(1 - phi(z^{(i)}))} ] frac{ partial phi(z^{(i)})}{ partial w_j}$$ . 한편, $$ frac{ partial phi(z^{(i)})}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)})) frac{ partial z^{(i)}}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)}))x^{(i)}_j$$ 이므로 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)}(1 - phi(z^{(i)})) + (1-y^{(i)}) phi(z^{(i)}) ]x^{(i)}_j = - sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 이 된다. . 간단히 나타내면 $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum (y - phi(z))x_j $$ . | 선형대수적으로 쓰면 아래와 같다. $$ triangledown J_{p times 1} = - mathbf{X_{n times p}^T (y_{p times 1} - phi(X_{n times p}w_{p times 1}))} $$ . | . 다시 돌아와서 로지스틱 비용함수를 최소화하는 가중치를 찾는 것이 목표이므로 경사하강법의 방법을 적용하면 . $$ triangle w_j := - eta frac{ partial J( mathbf{w})}{ partial w_j}$$ . $$w_j = w_j + triangle w_j$$ . $$w_j := w_j + eta sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 인데, 이는 로그 가능도함수에 경사상승법을 적용하여 가능도 함수를 최대화 하는 가중치의 업데이트 방법과 동일하다. 즉, 가능도를 최대화하는 가중치와 로지스틱 비용함수를 최소화 하는 가중치는 서로 같다 . $$ mathbf{w := w + triangle w}$$ . $$ triangle mathbf{w} = - eta triangledown J( mathbf{w}) = eta triangledown l( mathbf{w})$$ . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#50508;&#44256;&#47532;&#51608; &#44396;&#54788; &#49892;&#54744; . #collapse-hide class LogisticRegressionGD(object): &quot;&quot;&quot;경사 하강법을 사용한 로지스틱 회귀 분류기 매개변수 eta : float 학습률(0.0 에서 1.0 사이) n_iter : int 훈련 데이터셋 반복횟수 random_state : int 가중치 무작위 초기화를 위한 난수 생성기 시드 속성 w_ : 1d-array 학습된 가중치 cost_ : list 에포크마다 누적된 로지스틱 비용 함수 값 &quot;&quot;&quot; def __init__(self, eta=0.01, n_iter=100, random_state=1): self.eta = eta self.n_iter = n_iter self.random_state = random_state def fit(self, X, y): &quot;&quot;&quot;훈련 데이터 학습 매개변수 -- X : {array-like}, shape = [n_samples, n_features] n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련데이터 y : array-like, shape = [n_samples] 타깃값 반환값 -- self : object &quot;&quot;&quot; rgen = np.random.RandomState(self.random_state) self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) self.cost_ = [] for _ in range(self.n_iter): net_input = self.net_input(X) output = self.activation(net_input) errors = (y-output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() # 오차제곱합 대신 로지스틱 비용을 계산합니다. cost = ( -y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))) ) self.cost_.append(cost) return self def net_input(self, X): &quot;&quot;&quot;입력 계산&quot;&quot;&quot; return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, z): &quot;&quot;&quot;로지스틱 시그모이드 활성화 계산&quot;&quot;&quot; return 1./(1.+ np.exp(-np.clip(z, -250, 250))) def predict(self, X): &quot;&quot;&quot;단위 계단 함수를 사용하여 클래스 레이블을 반환합니다.&quot;&quot;&quot; return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # 최종 입력값이 0보다 크면 시그모이드 값도 0.5보다 크다 # 아래와 동일합니다. # return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0) . . lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) coef = lgr.fit(input, target).w_ print(coef) . [-7.22145597 15.28156541] . fig, ax = plt.subplots(1, 2, figsize=(15, 4)) epochs = np.arange(0,1000) ax[0].plot(epochs, lgr.cost_) ax[0].set_title(&#39;Does it converges?&#39;) ax[0].set_xlabel(&#39;Epoch&#39;) ax[0].set_ylabel(&#39;Weights&#39;) ax[0].set_xlim((1,1000)) ax[0].set_ylim((3,35)) ax[0].grid() ax[1].plot(epochs, lgr.cost_) ax[1].set_title(&#39;Does it converges?(log scale)&#39;) ax[1].set_xlabel(&#39;Epoch(log scale)&#39;) ax[1].set_xscale(&#39;log&#39;) ax[1].set_xlim((1,1000)) ax[1].set_ylim((3,35)) ax[1].grid() plt.show() . . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(coef[0] + coef[1]*x)) plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . 로지스틱 모델을 Iris-setosa와 Iris-versicolor 붖꽃만 가지고 로지스틱 회귀의 분류모델 구현이 작동하는지 확인해보자. . from sklearn import datasets import numpy as np . iris = datasets.load_iris() print(iris.data[:3]) print(iris.target[:3]) . [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2]] [0 0 0] . X = iris.data[:, [2,3]] y = iris.target print(np.unique(y)) . [0 1 2] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std = ss.transform(X_test) . X_train_01subset = X_train_std[(y_train == 0) | (y_train == 1)] y_train_01subset = y_train[(y_train == 0) | (y_train == 1)] lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) lgr.fit(X_train_01subset, y_train_01subset) . &lt;__main__.LogisticRegressionGD at 0x7f603fa314d0&gt; . from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) plot_decision_regions(X_train_01subset, y_train_01subset, classifier=lgr) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . . scikit-learn &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 사이킷런 모듈은 로지스틱 회귀 모델을 지원한다. 이 모델은 위의 구현과 달리 세개이상의 다중분류도 지원한다. . from sklearn.linear_model import LogisticRegression lgr = LogisticRegression(C=100.0, random_state=1) lgr.fit(X_train_std, y_train) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X_combined_std, y_combined, classifier=lgr, test_idx=range(len(y_train),len(y))) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . print(lgr.coef_, lgr.intercept_) for i, w0, w1, w2 in zip(range(3), lgr.intercept_, lgr.coef_[:, 0], lgr.coef_[:, 1]): print(&#39;model{:} : {:.2f} + PL * {:.2f} + PW * {:.2f} &#39;.format(i, w0, w1, w2) ) . [[-6.93265988 -5.76495748] [-2.03192177 -0.03413691] [ 8.96458165 5.79909439]] [-0.9576182 5.70388044 -4.74626223] model0 : -0.96 + PL * -6.93 + PW * -5.76 model1 : 5.70 + PL * -2.03 + PW * -0.03 model2 : -4.75 + PL * 8.96 + PW * 5.80 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#50640;&#49436;&#51032; &#44508;&#51228; . 규제를 사용하여 과대적합을 피하는 것은 이미 다른 문서에서 많이 다루었으므로 살짝만 하고 지나가자(cf-편향과 분산 참고). . 높은 분산은 과대적합에 비례하고(과대적합은 일반화한 모델이 너무 많은 변동을 끌어갔기 때문) 높은 편향은 과소적합에 비례한다(과소적합은 적합이 덜돼서 구조적인 편향이 발생). 과대적합의 경우에는 모델이 가지는 파라미터의 수를 줄이거나 모델이 가지는 모수들의 크기를 제한함으로써(규제) 해결할 수 있다(이 과정을 모델의 복잡도를 줄인다고 표현하기도 한다.). 반대로 과소적합의 경우에는 모델이 가지는 파라미터의 수를 늘려보는 식으로 해결할 수 있겠다. 여기서는 로지스틱 회귀의 규제에 따른 회귀계수의 변화를 보자. . 회귀계수들의 제곱항을 패널티항으로 갖는 L2규제 로지스틱 비용은 다음과 같다. $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ] + frac{ alpha}{2} left | left | mathbf{w} right | right |^2$$ . 릿지 회귀과 라쏘 회귀에서 규제항을 제어하는 파라미터는 $ alpha$였지만 사이킷런의 로지스틱 회귀에서 규제항을 제어하는 파라미터는 C이다. 주의할 점은 C는 $ alpha$의 역수이다. C가 클수록 규제는 완화된다. . petal length와 petal width 계수들의 규제 크기에 따른 크기변화를 그래프로 나타내보자. 규제가 완화될수록(C가 커질수록) 회귀계수들의 크기도 커지는 경향이 있는 것을 볼 수 있다. . weights0, weights1, params = [], [], [] for C in np.arange(-5,5): lgr = LogisticRegression(C=10.**C, random_state=1, multi_class=&#39;ovr&#39;) lgr.fit(X_train_std, y_train) weights0.append(lgr.coef_[0]) weights1.append(lgr.coef_[1]) params.append(10.**C) weights0, weights1 = np.absolute(weights0), np.absolute(weights1) plt.plot(params, weights0[:,0], label=&#39;m0:petal length&#39;) plt.plot(params, weights1[:,0], label=&#39;m1:petal length&#39;, linestyle=&#39;--&#39;) plt.plot(params, weights0[:,1], label=&#39;m0:petal width&#39;, linestyle=&#39;-.&#39;) plt.plot(params, weights1[:,1], label=&#39;m1:petal width&#39;, linestyle=&#39;:&#39;) plt.title(&#39;Absolute weights&#39;) plt.xlabel(&#39;C&#39;) plt.ylabel(&#39;Absolute weight&#39;) plt.xscale(&#39;log&#39;) plt.legend() plt.show() . scikit-learn&#51032; &#48708;&#50857;&#54632;&#49688; &#52572;&#51201;&#54868; &#50508;&#44256;&#47532;&#51608; . 로지스틱 비용함수처럼 볼록한 손실함수를 최소화하는 데는 확률적 경사 하강법(SGD) 대신에 더 고급 방법을 사용하는 것이 좋다. 실제 사이킷런은 다양한 최적화 알고리즘을 제공하며 solver= 매개변수로는 아래와 같은 것들이 있다. . &#39;newton-cg&#39; | &#39;lbfgs&#39; | &#39;liblinear&#39; | &#39;sag&#39; | &#39;saga&#39; | . 다중분류 매개변수인 LogisticRegression의 multiclass=의 기본값은 &#39;auto&#39;이다. &#39;auto&#39;로 설정하면 이진 분류이거나 solver=&#39;liblinear&#39;일 경우에 &#39;ovr&#39;(One versus Rest)를 선택하고 그 외에는 &#39;multinomial&#39;을 선택한다. 이는 &#39;liblinear&#39; 최적화 알고리즘이 다항 로지스틱 회귀 손실을 다룰 수 없고 다중클래스 분류를 위해 OvR 방법을 사용해야하기 때문이다. . predict_proba() method&#50752; &#49368;&#54540; &#54616;&#45208; &#50696;&#52769;&#49884; &#51452;&#51032;&#49324;&#54637; . 확률을 예측하고 싶다면 predict_proba 메서드를 사용하여 계산하자. . print(lgr.predict_proba(X_test_std[:3, :])) . [[1.52213484e-12 3.85303417e-04 9.99614697e-01] [9.93560717e-01 6.43928295e-03 1.14112016e-15] [9.98655228e-01 1.34477208e-03 1.76178271e-17]] . print(lgr.predict_proba(X_test_std[:3, :]).argmax(axis=1)) print(lgr.predict(X_test_std[:3, :])) . [2 0 0] [2 0 0] . lgr.predict(X_test_std[0, :].reshape(1,-1)) # lgr.predict(X_test_std[0, :]) 이렇게 하면 에러난다. . array([2]) . print(&#39;단순 인덱싱 뽑기 : &#39;, X_test_std[0, :].shape) print(&#39;2차원 배열로 변환:&#39;, X_test_std[0, :].reshape(1,-1).shape) . 단순 인덱싱 뽑기 : (2,) 2차원 배열로 변환: (1, 2) .",
            "url": "https://edypidy.github.io/studyblog/jupyter/logistic%20regression/classifying/loss%20function/2021/12/25/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.html",
            "relUrl": "/jupyter/logistic%20regression/classifying/loss%20function/2021/12/25/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.html",
            "date": " • Dec 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "[ML] SVM, Kernel-SVM with Gaussian-rbf kernel와 기본적인 하이퍼파라미터들",
            "content": ". Intro . SVM 문제는 클래스가 다른 데이터들을 구분하는 초평면을 어떻게 정할 것인가에 대한 문제이다. 일반화 오차에 대한 성능을 높이기 위해 마진을 최대로 하는 초평면을 그린다. iris 데이터에 SVM을 적용해보자. . import numpy as np import pandas as pd from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) . . Data Preparation . from sklearn import datasets iris = datasets.load_iris() input = iris.data[:, [2, 3]] target = iris.target . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(input, target, test_size=0.3, stratify = target, random_state=1) . 2차원 평면에 시각화를 위해 iris data에서의 petal length와 petal width를 피쳐로 사용하자. . petal(꽃잎)의 길이와 너비로 꽃의 종류를 분류하는 문제가 되겠다.(sepal(꽃받침)보다는 그럴듯한 상관관계가 있을 것으로 예상된다.) . 잘 알려진 데이터니 만큼 전처리 과정이나 EDA는 건너뛰고 바로 피팅 해보자. . test_size=0.3 : train과 test는 7대 3으로 나누었고. stratify=target : target 꽃의 종류의 비율에 맞추어 train과 test를 나누었으며 random_state=1 : 이건 그냥 재현을 위한 시드다. . Fitting with sklearn . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std= ss.transform(X_test) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) . from sklearn.svm import SVC svm = SVC(kernel=&#39;linear&#39;, C=1.0, random_state=1) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150)) # 빈 동그라미가 쳐진 아이들이 test 셋이다. plt.xlabel(&#39;petal length[std]&#39;) plt.ylabel(&#39;petal width[std]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . train set에 피팅된 SVM 모델이 test_set에도 꽤나 잘 들어맞는 모양새다. 딱 두개의 피쳐만 썼는데도 말이다!. . (사실 그냥 데이터가 원래부터 잘 구분되어있긴 했다.) . 그런데 뭔가 조금 이상하다. SVM은 서로 다른 클래스를 구분하는 평면을 만든다고 했는데 어떻게 세개의 클래스는 어떻게 구분 해야하는가? . 이유인 즉슨 디폴트로 &#39;One Versus Rest&#39;가 적용되었기 때문이다. 2번 클래스의 결정 경계와 평면은 (0번 vs 1번, 2번), (1번 vs 0번, 2번)을 거치면서 &#39;0번과 1번의 영역이 정해졌으니 나머지는 2번이겠구나!&#39; 하는 식으로 결정한 셈이다. . Linear hyperlane&#51004;&#47196; &#44396;&#48516;&#46104;&#51648; &#50506;&#45716; &#45936;&#51060;&#53552; . 다 좋고 모델이 클래스를 잘 분류하는 것 같지만 실제 세계의 데이터는 이렇게 이상적이지 못한 경우가 대부분이다. 과연 &#39;직선&#39;, &#39;평면&#39; 과 같은 선형 결정경계만으로 데이터를 잘 나눌 수 있을까? . 아래의 데이터는 그냥 임의로 만들어 본(또 다른 이상적인 것 일지도 모르는) 데이터이다. 1사분면과 3사분면에는 1번이, 2사분면과 4사분면엔 -1번 클래스가 존재하도록 만든 데이터이다.(x, y의 기울기 탄젠트와 클래스가 관련이 있다면 충분히 있을 수 있는 데이터 셋이다) . np.random.seed(1) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &lt; 0) y_xor = np.where(y_xor, 1, -1) plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], color=&#39;b&#39;, marker=&#39;^&#39;, label=&#39;1&#39;) plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1], color=&#39;r&#39;, marker=&#39;v&#39;, label=&#39;-1&#39;) plt.xlim([-3, 3]) plt.ylim([-3, 3]) plt.hlines(0, -3, 3) plt.vlines(0, -3, 3) plt.legend() plt.tight_layout() plt.show() . 이런 데이터에 Linear SVM Classifier를 적용하면 어떻게 될까? . 당연한 이야기겠지만 선형 초평면으로는 1번 클래스와 -1번 클래스를 절대 나눌 수 없다. . svm = SVC(kernel=&#39;linear&#39;,random_state=1) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . 그러면 이렇게 해보자. 모든 데이터의 x,y 두 값을 곱해서 z열로 추가하는 것이다. . 투영된 데이터가 3차원 공간상에서 선형 초평면에 의해 분류되길 기대하는 것이다. . X_xor_3d = np.column_stack(( X_xor, X_xor[:, 0] * X_xor[:, 1])) x = np.linspace(-3, 3, 200) y = np.linspace(-3, -3, 200) x, y = np.meshgrid(x, y) fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;}) ax.scatter3D(X_xor_3d[y_xor == 1, 0], X_xor_3d[y_xor == 1, 1], X_xor_3d[y_xor == 1, 2], color=&#39;b&#39;, marker=&#39;^&#39;, label=&#39;1&#39;) ax.scatter3D(X_xor_3d[y_xor == -1, 0], X_xor_3d[y_xor == -1, 1], X_xor_3d[y_xor == -1, 2], color=&#39;r&#39;, marker=&#39;v&#39;, label=&#39;-1&#39;) ax.view_init(2,85) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;z&#39;) plt.legend() plt.tight_layout() plt.show() . 첫번째 열과 두번째 열을 곱하여 3차원 공간에 투영시킨 모습이다. . $z = 0$ 평면으로 두 클래스를 가를 수 있다. . Kenel-SVM&#44284; Kernel&#51032; &#51333;&#47448;&#46308; . 이와 같이 기존의 데이터를 통해 새로운 차원을 추가하여 선형적으로 구분이 되게끔 하는 모델을 Kernel-SVM이라고 한다. . 일반적인 커널 함수는 아래와 같다. . 선형 커널 : &#39;linear&#39; . | 다항 커널 : &#39;poly&#39; . | 가우시안 rbf : &#39;rbf&#39; . | 시그모이드 커널 : &#39;sigmoid&#39; . | . (callable한 객체를 대입할 수도 있다. 자세한 것은 도큐먼트를 참고하자.) . sklearn SVC의 디폴트 커널은 사실 linear가 아니라 가우시안 rbf 커널이다. 여기서는 rbf 커널과 규제 매개변수(하이퍼파라미터)들에 대해 살짝 알아보자. . svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=0.1, C=10, ) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend() plt.tight_layout() plt.show() . Gaussian rbf&#50752; gamma &#47588;&#44060;&#48320;&#49688; . def axes_decision_regions(X, y, classifier, axes, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) axes.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) axes.set_xlim(xx1.min(), xx1.max()) axes.set_ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): axes.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] axes.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) gammas = [0.01, 0.1, 0.5, 5] fig, axes = plt.subplots(1,4, figsize=(20,4)) for gamma, ax in zip(gammas, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=10) svm.fit(X_xor, y_xor) axes_decision_regions(X_xor, y_xor, axes=ax, classifier=svm) ax.set_title(&#39;gamma : {:}&#39;.format(gamma)) axes[0].legend() plt.show() . . gamma가 커질수록 SVM 분류기가 1번 데이터에 과적합 되어가는 모습이다. . 과적합이 되어 가면서 1번 데이터의 결정경계가 동글동글 해지는 것을 볼 수 있는데 이를 가우시안 구라고 부른다. . gamma는 가우시안 구의 크기를 제한하는 매개변수로 이해할 수 있다(커널이 rbf인 것을 기억하자). gamma값을 크게 하면 서포트 벡터(원래 결정경계 근처의 점이라고 여길 수 있다)의 영향 범위가 줄어든다. 샘플에 더욱 적합되고 결정경계는 구불구불해진다. . &#39;rbf&#39; 커널함수(방사 기저함수)는 다음과 같다. . ($ mathbf{x}^{(i)}$는 자료 행렬의 $i$행 즉, $i$번째 데이터이다.) . $$K( mathbf{x}^{(i)}, mathbf{x}^{(j)}) = exp left ( - frac{ left | mathbf{x}^{(i)}- mathbf{x}^{(j)} right |^2}{2 sigma^2} right ) = exp left ( - gamma left | mathbf{x}^{(i)}- mathbf{x}^{(j)} right |^2 right )$$ . 여기서 $ gamma = frac{1}{2 sigma^2}$는 최적화 대상 파라미터가 아닌 하이퍼파라미터이다. . 가우시안 rbf 커널이라 불리는 이유는 가우시안 분포함수와 닮아 있어서 붙은 이름이다. 이름보다 중요한 것은 $ exp$에 있다. 직관적으로 $ exp$ 함수는 매클로린 전개시 다항식의 무한합을 가지니까 무한차원으로의 투영이라 생각할 수 있겠다. 자세한 이론적인 부분은 이 블로그{:target=&quot;_blank&quot;} 이 논문{:target=&quot;_blank&quot;} 을 참고하길 바란다. . iris 데이터에서도 확인해보자. . gammas = [0.1, 1, 5, 50] fig, axes = plt.subplots(1,4, figsize=(20,4)) for gamma, ax in zip(gammas, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=1) svm.fit(X_train_std, y_train) axes_decision_regions(X_combined_std, y_combined, axes=ax, classifier=svm, test_idx=range(105,150)) ax.set_title(&#39;gamma : {:}, test_score : {:.2f}&#39;.format(gamma, svm.score(X_test_std, y_test))) axes[0].legend() plt.show() . gamma값이 커질수록 2번 타겟을 제외한 나머지 데이터에 과적합 되어가는 것을 볼 수 있다.(One Versus Rest에 의해 n가지의 클래스 레이블이 있으므로 n-1개의 가우시안 구로 적합되어 간다.) . train_scores, test_scores=[], [] gammas = np.arange(0.1, 1000) for gamma in gammas: svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=1).fit(X_train_std, y_train) train_scores.append(svm.score(X_train_std, y_train)); test_scores.append(svm.score(X_test_std, y_test)) differ = [4*np.abs(tr - ts) for tr, ts in zip(train_scores, test_scores)] plt.plot(gammas, train_scores, label=&#39;train&#39;); plt.plot(gammas, test_scores, label=&#39;test&#39;); plt.plot(gammas, differ, label=&#39;differ*4&#39;) plt.vlines(34, 0, 1, linestyle=&#39;--&#39;); plt.text(1, 0.6,&#39;$gamma = 34$&#39;, fontsize=12) plt.title(&#39;Train &amp; Test scores for Gammas&#39;); plt.xlabel(&#39;Gamma&#39;); plt.ylabel(&#39;score&#39;) plt.xscale(&#39;log&#39;); plt.legend(loc=&#39;center left&#39;); plt.grid() plt.show() . 역시 gamma값이 높아질수록 과적합되어가는 모습을 볼 수 있다. . 낮았을 때는 과소적합 되어있는데 gamma값을 키우는 것으로 해결을 볼 수 있겠다. gamma = 34에서 최적인듯 하다.(데이터 수가 적어서 일반화를 하기엔 아직은 검증이 다소 필요해보인다.) . Gaussian rbf&#50752; C&#47588;&#44060;&#48320;&#49688; . 매개변수 C는 sklearn의 여러 모델들에서 과적합 규제 매개변수로써 쓰인다. 보통 $ alpha$의 역수로 생각할 수 있는데 C가 커질수록 과대적합에 대한 규제가 완화된다고 이해할 수 있다.(모델은 과적합될 가능성이 커진다.) . C = [0.01, 1, 10, 100] fig, axes = plt.subplots(1,4, figsize=(20,4)) for c, ax in zip(C, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=1, C=c) svm.fit(X_train_std, y_train) axes_decision_regions(X_combined_std, y_combined, axes=ax, classifier=svm, test_idx=range(105,150)) ax.set_title(&#39;C : {:}, test_score : {:.2f}&#39;.format(c, svm.score(X_test_std, y_test))) axes[0].legend() plt.show() . C가 커질수록 최적화 해야하는 비용함수에서의 패널티 함수가 커진다. 즉, C는 커질수록 분류 오차에 대한 비용을 키운다. . 주의해야 할 점은 SVM의 패널티 함수는 Ridge나 Lasso와 달리 가중치 벡터 $ mathbf{w}$의 함수가 아니라 상수 C와 분류오차 $ xi^{(i)}$를 곱하여 더한 함수라는 것이다. . 그러므로 C가 커질수록 분류 오차에 대한 비용이 커진다는 것을 모델이 분류오차 $ xi^{(i)}$의 존재에 대해 더욱 민감하게 반응한다고 이해할 수 있다. C값이 크다면 분류오차를 크게 평가하고 배제하려는 쪽으로 과대적합될 것이다.(C값이 작아서 분류오차를 무시한다면 더울 일반화 된 모델이 나올 것이다.) . 따라서 C가 커질수록 훈련 데이터셋에 과대적합이 되어가고 C가 작아질수록 모델이 일반화 된다고 확인할 수 있겠다. . 릿지와 라쏘, 로지스틱 회귀에서 $ alpha$와 C를 통해 규제의 정도를 제어할 수 있었다. SVM에서의 C도 릿지와 라쏘에서늬 $ alpha$와 반비례하고 로지스틱의 C와 동일한 역할을 수행하는 하이퍼파라미터라고 할 수 있다. . &#50836;&#50557; . svm의 SVC에는 기본적인 파라미터 X, y, kernel=&#39;rbf&#39;, gamma=&#39;scale&#39;, C=1,(random_state=None, decision_function_shape=&#39;ovr) 가 있다. 하이퍼파라미터는 kernel, gamma와 C가 있다. . kernel : 커널함수이다. 상위 차원의 매핑함수($ phi$)를 통해 상위 차원에서의 선형 초평면을 구하고 그것을 매핑함수의 역함수($ phi^{-1}$)로 원래 차원으로 가져오는 수고를 줄인 하나의 함수 $K$이다. 머서의 조건만 만족하면 $ phi$를 모르더라도 $K$가 존재하여 커널함수로 사용할 수 있음이 보장된다. 기본값은 rbf(방사 기저 함수 또는 가우시안 커널)이다. | gamma : rbf에서는 gamma가 클수록 가우시안 구의 크기를 제한하는 것으로 이해할 수 있다. 결정경계가 샘플에 가까워지고 구불구불해진다.(클수록 과적합 된다고 이해할 수도 있다.) | C : 분류오차를 얼마나 신경쓸 것인지 조절한다. 클수록 분류오차에 대해 엄격해지고 작을수록 분류오차를 무시한다. 규제 측면에서 릿지와 라쏘에서의 $ alpha$ 와 반비례하고 로지스틱의 C와 동일한 역할을 수행하는 하이퍼파라미터이다. 기본값이 1인 것을 고려하면 SVC는 하드 마진svm이 아닌 소프트 마진 svm이다. 선형 초평면으로 완전히 구분되지 않는 데이터셋에 C=0을 설정하면 에러가 난다. | . &#45908; &#44277;&#48512;&#54644;&#50556; &#54624; &#44163;&#46308; . SVM을 이론적으로 이해하려면 추가로 공부해볼만한 것들을 정리해 보았다. . 최적화 문제 라그랑주 프리멀 함수 | 라그랑주 듀얼 함수 | KKT 조건 | . | SVM 유도(하드 마진) 기본적인 아이디어 | 마진 유도 | 최적화 대상 및 제약 조건 | 라그랑주 프리멀, 듀얼 라그랑주 프리멀 함수 유도 | 프리멀 함수 w, b 편미분 = 0 | 라그랑주 듀얼 함수 유도 | | 듀얼리티 갭 KKT 조건 확인 | | . | SVM 유도(소프트 마진) 분류오차 ξ | 최적화 대상 및 제약 조건 ξ와 C | . | 라그랑주 프리멀, 듀얼 | KKT 조건 확인 | . | 커널 SVM 커널의 필요성 | 프리멀, 듀얼 문제 필요성(핸즈온 223) | 매핑 함수 | 커널 | 커널 SVM 유도 최적화 대상 및 제약 조건 | 라그랑주 프리멀, 듀얼 | KKT 조건 확인 | | 머서의 정리와 조건 | 커널 예시 | . | 추가 온라인 SVM | SVM 회귀 | 쿼드래틱 프로그래밍 | . | .",
            "url": "https://edypidy.github.io/studyblog/jupyter/svm/kernel-svm/classifying/hyper-lane/kernel/gaussian-rbf/2021/12/25/_SVM(%EC%B5%9C%EC%A2%85).html",
            "relUrl": "/jupyter/svm/kernel-svm/classifying/hyper-lane/kernel/gaussian-rbf/2021/12/25/_SVM(%EC%B5%9C%EC%A2%85).html",
            "date": " • Dec 25, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Heart Desease 데이터 셋 분석과 train_test split에 따른 Accuracy",
            "content": ". Data copyright . 이번에 분석해볼 데이터는 캐글의 Heart Desease 데이터입니다. 출처는 아래와 같습니다. . https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset . Intro . 개인적으로 진행중인 kaggle review에서의 이번주차 데이터 셋입니다. 캐글 코드를 매우 많이 참고한 점을 밝힙니다. 참고한 코드 주소는 아래에 참고한 플랏에서 밝혀놓았습니다. . 아래의 바닐라 모델 중 Accuracy가 좋은 것을 튜닝으로 개선시켜 볼 것입니다. . XGboost, AdaBoost, Lgbm, Logistic Regression, RandomForest, MLPClassifier . Data description . Age : Age of the patient . | Sex : Sex of the patient . | exang: exercise induced angina (1 = yes; 0 = no) . | ca: number of major vessels (0-3) . | cp : Chest Pain type chest pain type . Value 1: typical angina Value 2: atypical angina Value 3: non-anginal pain Value 4: asymptomatic . | trtbps : resting blood pressure (in mm Hg) . | chol : cholestoral in mg/dl fetched via BMI sensor . | fbs : (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false) . | rest_ecg : resting electrocardiographic results . Value 0: normal Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV) Value 2: showing probable or definite left ventricular hypertrophy by Estes&#39; criteria . | thalach : maximum heart rate achieved . | target : 0= less chance of heart attack 1= more chance of heart attack . | . Preparation . Libraries &amp; Workspace setting . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) import scipy.stats as ss sns.set_palette(&#39;deep&#39;) sns.set_color_codes() sns.set_style(&#39;white&#39;) . . Load Data . df = pd.read_csv(&#39;https://raw.githubusercontent.com/edypidy/Datasets/main/Heart%20Attack%20Analysis%20%26%20Prediction%20Dataset/heart.csv&#39;) df.head(3) . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . Train Test Split . 여러 모델에 적용하기 위해선 스케일 조정 작업이 필요해 보이고, 연속형, 명목형, 순서형 변수들의 구분이 필요하다. 순서형 변수의 경우 스케일을 맞추기 위해 one-hot encoding을 하는 것이 좋아 보인다. . Continuous : age, trtbps, chol, thalachh, oldpeak Cat_Ordered : cp, restecg, slp, caa, thall Cat_Nominal : sex, fbs, exng . continuous = [&#39;age&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;thalachh&#39;, &#39;oldpeak&#39;] cat_ordered = [&#39;cp&#39;, &#39;restecg&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;] cat_nominal = [&#39;sex&#39;, &#39;fbs&#39;, &#39;exng&#39;] categorical = cat_ordered + cat_nominal . . # train_test가 나눠져 있지 않다. 따로 빼두자.. full = pd.get_dummies(df, columns=categorical) . . train, test로 나눠져있는 것이 아닌 feature - target 이 모두 있는 데이터 셋이다. 모델의 최종 검증용 데이터가 따로 있는 것이 아니기 때문에 데이터를 split 해놓을 필요가 있다.(최대한 실전 문제처럼 해보자) . from sklearn.model_selection import train_test_split train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42) train_full, test_full = train_test_split(full, test_size=0.2, shuffle=True, random_state=42) . stratify를 설정 해야할지 고민을 좀 했다. 하지만 실제 데이터 셋이라면 target(output)의 비율이 완전히 동일할리가 없다(분포는 비슷하겠지만). (default)shuffle=True 로 둔다. . print(&#39;train shape &amp; target ratio : &#39;, train.shape, train_full.shape, &#39;%.2f&#39;%train.output.mean()) print(&#39;test shape &amp; target ratio : &#39;, test.shape, test_full.shape, &#39;%.2f&#39;%test.output.mean()) . . train shape &amp; target ratio : (242, 14) (242, 31) 0.55 test shape &amp; target ratio : (61, 14) (61, 31) 0.52 . target의 비율이라도 잘 나눠졌는지 확인하고 시작하자. . EDA . Peek . Isna? . train.isna().apply(pd.value_counts) . . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . False 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | . Missing value는 없다. . General Stats . train.describe() . . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | . mean 54.462810 | 0.657025 | 0.991736 | 130.359504 | 246.842975 | 0.128099 | 0.553719 | 150.115702 | 0.314050 | 1.013223 | 1.421488 | 0.681818 | 2.301653 | 0.549587 | . std 9.204492 | 0.475687 | 1.022533 | 16.828858 | 52.795465 | 0.334893 | 0.530410 | 22.352398 | 0.465098 | 1.102577 | 0.607724 | 0.990620 | 0.593811 | 0.498566 | . min 29.000000 | 0.000000 | 0.000000 | 94.000000 | 131.000000 | 0.000000 | 0.000000 | 88.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 48.000000 | 0.000000 | 0.000000 | 120.000000 | 212.000000 | 0.000000 | 0.000000 | 136.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 55.500000 | 1.000000 | 1.000000 | 130.000000 | 239.500000 | 0.000000 | 1.000000 | 154.000000 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 61.000000 | 1.000000 | 2.000000 | 140.000000 | 274.750000 | 0.000000 | 1.000000 | 165.750000 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 77.000000 | 1.000000 | 3.000000 | 192.000000 | 564.000000 | 1.000000 | 2.000000 | 202.000000 | 1.000000 | 5.600000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . Univariate Analysis . 캐글 탐색 중에 바이올린 차트를 너무 세련되게 그려놓은 분이 있어서 참고 해봤다. 주소는 아래와 같다. . https://www.kaggle.com/abhinavgargacb/heart-attack-eda-predictor-95-accuracy-score#Exploratory-Data-Analysis- . fig = plt.figure(figsize=(24, 6)) axes = [fig.add_subplot(1,5,i) for i in range(1, 6)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(5): var = continuous[i] ax = axes[i] ax.grid(axis=&#39;y&#39;, linestyle=&#39;:&#39;) ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) color = sns.color_palette(&#39;deep&#39;)[i] sns.violinplot(data=df, y=var, ax=ax, color=color) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . age : 표본이 중장년층을 중심으로 퍼져있다. 아무래도 심장병이라는 질환의 특성상 병원까지 와서 표본으로 수집되는 어린 환자는 적고, 고령층의 경우 인구수가 적어서 그런 것으로 생각된다. | trtbps : 일정 범위 내에서 정규분포와 같은 모습을 보이나 오른쪽 꼬리가 꽤 길게 늘어져 있다. | chol : 콜레스테롤은 대표적인 심장병의 원인 중 하나로 꼽힌다. 콜레스테롤이 아주 높은 outlier가 보인다. | thalachh : 왼쪽 꼬리가 꽤나 두꺼운 분포이다. (최대 심박수가 60인 사람은 평소 심박수가 몇일지 궁금해진다) | oldpeak : 오른쪽 꼬리가 아주 두껍고 멀리 떨어진 outlier가 보인다. | . fig = plt.figure(figsize=(24, 10)) axes = [fig.add_subplot(2,4,i) for i in range(1, 9)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(8): var = categorical[i] ax = axes[i] ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) sns.countplot(data=df, x=var, ax=ax) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . 모든 특성에서 범주간의 불균형이 보인다. | 특히 restecg, slp, caa, thall 에서 아주 심하고 fbs와 cp 에서도 큰 불균형이 발견된다. exng는 다른 특성들에 비해 불균형이 심해보이진 않는다. | . Bivariate Analysis . 여기서부턴 test의 target값이 df에 포함 되었기 때문에 원본 데이터를 보지 못한다.(실전이라면 univariate analysis까지만 데이터를 합하여 볼 수 있을 것이다.) train 데이터로 보도록 하자.(실전처럼!) . fig = plt.figure(figsize=(24, 10)) axes = [fig.add_subplot(2,4,i) for i in range(1, 9)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(8): var = categorical[i] ax = axes[i] ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) colorIndex = (2*(i)) % 10 color1 = sns.color_palette(&#39;deep&#39;)[colorIndex] color2 = sns.color_palette(&#39;deep&#39;)[colorIndex + 1] sns.countplot(data=train, x=var, ax=ax, hue=&#39;output&#39;, palette=[color1, color2]) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . 1이 조금 더 많은 것은 train.output의 1이 0보다 조금 더 많았던 것 때문이라 생각할 수 있겠지만 조금 더 많은 정도가 아니거나 거꾸로 0이 많은 경우가 있다면 꽤나 유의미한 특성이라 볼 수 있을 것이다. . cp, slp, caa, thall, sex, exng 특성을 보면 특정 범주에 따라 1과 0의 비율이 역전 되는 모습이 많이 보인다. | restecg, fbs의 경우엔 그다지 유의미한 변화가 없다. | . Cramer&#39;s V . 그래프를 가져온 문서에서 거의 그대로 가져왔다. Cramer&#39;s V는 두 이산형 변수의 연관성 척도로 쓰인다. 특히 비교대상 범주가 3개 이상일 때 쓰인다. . Pearson 카이제곱 통계량을 기반으로 만들어진 테스트이다.(계산적인 부분이 궁금하다면 여기를 클릭하자) . def cramers_corrected_stat(x, y): result = -1 conf_matrix = pd.crosstab(x, y) if conf_matrix.shape[0] == 2: correct = False else: correct = True chi2, p = ss.chi2_contingency(conf_matrix, correction=correct)[0:2] n = sum(conf_matrix.sum()) phi2 = chi2/n r, k = conf_matrix.shape phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1)) rcorr = r - ((r-1)**2)/(n-1) kcorr = k - ((k-1)**2)/(n-1) result = np.sqrt(phi2corr / min((kcorr-1), (rcorr-1))) return round(result, 6), round(p, 6) for var in categorical: x = train[var] y = train[&#39;output&#39;] cramersV, p = cramers_corrected_stat(x, y) print(f&#39;For variable {var}, Cramer &#39;s V: {cramersV} and p value: {p}&#39;) . . For variable cp, Cramer&#39;s V: 0.463569 and p value: 0.0 For variable restecg, Cramer&#39;s V: 0.110035 and p value: 0.08517 For variable slp, Cramer&#39;s V: 0.36724 and p value: 0.0 For variable caa, Cramer&#39;s V: 0.487659 and p value: 0.0 For variable thall, Cramer&#39;s V: 0.504115 and p value: 0.0 For variable sex, Cramer&#39;s V: 0.297875 and p value: 2e-06 For variable fbs, Cramer&#39;s V: 0.0 and p value: 0.988529 For variable exng, Cramer&#39;s V: 0.439424 and p value: 0.0 . fbs는 연관성이 없어보이고 restecg는 유의수준 $ alpha = 0.05$에서 아쉽게 기각이 되지 않는다. | 위의 두 변수를 제외한 모든 변수에서 귀무가설이 기각되었다. | . fig = plt.figure(figsize=(24, 6)) axes = [fig.add_subplot(1,5,i) for i in range(1, 6)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(5): var = continuous[i] ax = axes[i] ax.grid(axis=&#39;y&#39;, linestyle=&#39;:&#39;) ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) colorIndex = (2*(i)) % 10 color1 = sns.color_palette(&#39;deep&#39;)[colorIndex] color2 = sns.color_palette(&#39;deep&#39;)[colorIndex + 1] sns.violinplot(data=train, y=var, x=&#39;output&#39;, ax=ax, palette=[color1, color2]) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . 수집된 표본에선 의외로 환자의 연령대가 다양하다. | trtbps, chol, oldpeak 에선 output에 따라 outlier가 다르게 관측된다. | . fig = plt.figure(figsize=(24, 6)) axes = [fig.add_subplot(1,5,i) for i in range(1, 6)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(5): var = continuous[i] ax = axes[i] ax.grid(axis=&#39;y&#39;, linestyle=&#39;:&#39;) ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) colorIndex = (2*(i - 1)) % 10 color1 = sns.color_palette(&#39;deep&#39;)[colorIndex] color2 = sns.color_palette(&#39;deep&#39;)[colorIndex + 1] sns.kdeplot(data=train, x=var, hue=&#39;output&#39;, ax=ax, fill=True, palette=[color1, color2]) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . age, thalachh, oldpeak 에서 큰 영향이 있는 것으로 보인다. | trtbps, chol 은 그다지 큰 영향이 있는 것으로 보이진 않는다. | . Kruskal-Wallis H-test . 분포 모형이 통계적으로 유의하게 차이가 있는지 확인해보자. 모든 분포가 정규성을 갖지는 않기에 평균보다는 중위수에 대한 검정을 하자. . for var in continuous: gp = train[[var, &#39;output&#39;]].groupby([&#39;output&#39;]) gp_array = [group[var].to_numpy() for name, group in gp] kstat, p = ss.kruskal(*gp_array) kstat, p = round(kstat, 6), round(p, 6) print(f&#39;For variable {var}, Kruskal-Wallis H-test: {kstat} and p value: {p}&#39;) . . For variable age, Kruskal-Wallis H-test: 14.700765 and p value: 0.000126 For variable trtbps, Kruskal-Wallis H-test: 1.288149 and p value: 0.256389 For variable chol, Kruskal-Wallis H-test: 2.346947 and p value: 0.125529 For variable thalachh, Kruskal-Wallis H-test: 36.863825 and p value: 0.0 For variable oldpeak, Kruskal-Wallis H-test: 42.479336 and p value: 0.0 . Point Biserial test . 다른 검정을 하고싶다면 피어슨 상관계수와 값이 동일하긴 하지만 Point Biserial test를 해보자 . for var in continuous: pbistat, p = ss.pointbiserialr(train[var], train[&#39;output&#39;]) pbistat, p = round(pbistat, 6), round(p, 6) print(f&#39;For variable {var}, Point Biserial : {pbistat} and p value: {p}&#39;) . . For variable age, Point Biserial : -0.233782 and p value: 0.000244 For variable trtbps, Point Biserial : -0.104257 and p value: 0.10569 For variable chol, Point Biserial : -0.057714 and p value: 0.371366 For variable thalachh, Point Biserial : 0.393415 and p value: 0.0 For variable oldpeak, Point Biserial : -0.447305 and p value: 0.0 . 두 검정 모두 동일하게 귀무가설을 채택&amp;기각 하였다. 두 결과에서 trtbps, chol 모두 생각보다 작은 값을 보인다. | . Correlation of continuous variables(pearson) . fig = plt.figure(figsize=(12, 8)) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) corr_matrix = train[continuous].corr() mask = np.triu(np.ones_like(corr_matrix)) sns.heatmap(corr_matrix, cmap=&#39;Reds&#39;, annot=True, mask=mask) plt.show() . . 크기가 커봐야 0.4 정도이다. 그다지 높은 선형 상관을 띠는 특성 쌍은 없다. | . Correlation of categorical variables(Cramer&#39;s V) . fig = plt.figure(figsize=(12, 8)) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) corr_matrix = train[categorical].corr(method=lambda x, y: cramers_corrected_stat(x, y)[0]) mask = np.triu(np.ones_like(corr_matrix)) sns.heatmap(corr_matrix, cmap=&#39;Reds&#39;, annot=True, mask=mask) plt.show() . . 커봐야 0.43정도이다. 강한 상관을 보이는 특성쌍은 없는 것으로 보인다. | . EDA Conclusion . Continuous . outlier가 많이 관측되었다. 모델 적합시 영향을 많이 줄 것으로 보인다. . | 스케일을 맞출 필요가 있다. . | oldpeak의 경우 분포가 오른쪽 꼬리를 길게 달고 있다. 하지만 변수에 대한 정확한 설명이 없어 변환이 어려울듯 하다. . | p-value가 생각보다 작아 trtbps, chol이 output 값과 아예 연관이 없다고 보기는 어려울 수 있다. . | 특성들 간의 선형 상관관계는 없는 것으로 보인다. . | . Categorical . 모든 특성에서 범주 간의 심한 불균형이 관측된다. . | fbs의 경우 심한 불균형이 있고 output과 연관이 거의 없는 것으로 검정 되었으므로 제거할 필요가 있어보인다. . | 특성들 간의 상관관계는 없는 것으로 보인다. . | . Model comparison . # Preprocessing from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, make_scorer from sklearn.metrics import mean_squared_error,r2_score from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler # Basic Model from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_validate # Boosting Model from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier from xgboost import XGBClassifier # Neural Network Model from sklearn.neural_network import MLPClassifier . . X_train = train_full.drop([&#39;fbs_0&#39;, &#39;fbs_1&#39;, &#39;output&#39;], axis=1) y_train = train_full[&#39;output&#39;] X_test = test_full.drop([&#39;fbs_0&#39;, &#39;fbs_1&#39;, &#39;output&#39;], axis=1) y_test = test_full[&#39;output&#39;] scaler = StandardScaler() scaler.fit(X_train[continuous]) X_train[continuous] = scaler.transform(X_train[continuous]) X_test[continuous] = scaler.transform(X_test[continuous]) # one-hot encoding is already done . models = { &#39;SVM&#39;: SVC(), &#39;Random Forest&#39;: RandomForestClassifier(), &#39;Logistic Regression&#39;: LogisticRegression(), &#39;Gradient Boosting&#39;: GradientBoostingClassifier(), &#39;AdaBoost Classifier&#39;: AdaBoostClassifier(), &#39;XGBoost Classifier&#39; : XGBClassifier(), &#39;MultiLayer Perceptron Classifier&#39; : MLPClassifier() } scoring = {&#39;Accuracy&#39;: make_scorer(accuracy_score), &#39;F1_score&#39;: make_scorer(f1_score), &#39;Recall&#39; : make_scorer(recall_score), &#39;Precision&#39; : make_scorer(precision_score)} . scores = pd.DataFrame({}) for name, model in models.items(): score = cross_validate(model, X_train, y_train, scoring=scoring) temp = pd.DataFrame(score).mean() scores[name] = temp scores = scores.T.drop(&#39;score_time&#39;, axis=1) scores . fit_time test_Accuracy test_F1_score test_Recall test_Precision . SVM 0.005021 | 0.826020 | 0.842251 | 0.856125 | 0.832761 | . Random Forest 0.150434 | 0.805782 | 0.819559 | 0.811681 | 0.832945 | . Logistic Regression 0.008422 | 0.834439 | 0.851540 | 0.871795 | 0.835409 | . Gradient Boosting 0.091337 | 0.818027 | 0.836435 | 0.849288 | 0.826028 | . AdaBoost Classifier 0.076684 | 0.768367 | 0.795258 | 0.819088 | 0.775694 | . XGBoost Classifier 0.058072 | 0.801531 | 0.821968 | 0.834473 | 0.810124 | . MultiLayer Perceptron Classifier 0.260454 | 0.834779 | 0.848088 | 0.841880 | 0.857307 | . fig = plt.figure(figsize=(15, 4)) fig.add_subplot(121) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) sns.set_style(&#39;whitegrid&#39;) plt.title(&#39;Models CV Accuracy&#39;) sns.barplot(scores.test_Accuracy, scores.index, alpha=0.85) plt.xlim((0.75, 0.86)) fig.add_subplot(122) plt.title(&#39;Models CV F1-score&#39;) sns.barplot(scores.test_F1_score, scores.index, alpha=0.9) plt.yticks([]) plt.xlim((0.75, 0.86)) plt.tight_layout() plt.show() . . Logistic Regression 모델이 Accuracy Score, F1 Score 모두 가장 높다. | . GridSearchCV . from sklearn.model_selection import GridSearchCV model = LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=500, random_state=42) params = {&#39;C&#39; : np.arange(0.1, 1, 0.1), &#39;l1_ratio&#39; : np.arange(0, 1, 0.05)} gs = GridSearchCV(estimator=model, param_grid=params, scoring=make_scorer(accuracy_score), cv=5, n_jobs=-1) gs.fit(X_train, y_train) . . GridSearchCV(cv=5, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=500, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;elasticnet&#39;, random_state=42, solver=&#39;saga&#39;, tol=0.0001, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=-1, param_grid={&#39;C&#39;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), &#39;l1_ratio&#39;: array([0. , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=make_scorer(accuracy_score), verbose=0) . print(gs.best_params_) . {&#39;C&#39;: 0.30000000000000004, &#39;l1_ratio&#39;: 0.1} . 혹시나 해서 elasticnet penalty를 적용해봤지만 l2 만으로도 충분해보인다. . model= LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=500, C=0.3, l1_ratio=0.1, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) df_cm = pd.DataFrame(cm) sns.heatmap(df_cm, annot=True, cmap=&#39;Reds&#39;) plt.title(&#39;Confusion Matrix for Logistic&#39;, fontsize=15) plt.xlabel(&quot;Predicted&quot;) plt.ylabel(&quot;True&quot;) plt.show() print(&#39;Accuracy : &#39;,accuracy_score(y_test, y_pred)) print(&#39;F1-score : &#39;,f1_score(y_test, y_pred)) . . Accuracy : 0.8852459016393442 F1-score : 0.8888888888888888 . 점수가 기대한 것 보다 잘 나오지 않았다.혹시 데이터셋이 얼마나 잘 나눠졌느냐에 따라 모델의 점수가 달라지지 않을까? 라는 의심이 든다. 데이터 셋의 크기가 작기 때문에 random_state에 따라 점수가 크게 달라질 수도 있겠다. 우선 random_state = 65로 나눠진 데이터 셋에 대해 실험해보자. . | . &#49892;&#54744; . random_state=65 &#47196; &#49892;&#54744; . # random_state=65 dataset prep X = df[[&#39;sex&#39;, &#39;restecg&#39;, &#39;cp&#39;, &#39;exng&#39;, &#39;thall&#39;, &#39;caa&#39;, &#39;slp&#39;, &#39;age&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;thalachh&#39;, &#39;oldpeak&#39;]] y = df[&#39;output&#39;] # models &amp; scoring models = { &#39;SVM&#39;: SVC(), &#39;Random Forest&#39;: RandomForestClassifier(), &#39;Logistic Regression&#39;: LogisticRegression(), &#39;Gradient Boosting&#39;: GradientBoostingClassifier(), &#39;AdaBoost Classifier&#39;: AdaBoostClassifier(), &#39;XGBoost Classifier&#39; : XGBClassifier(), &#39;MultiLayer Perceptron Classifier&#39; : MLPClassifier() } scoring = {&#39;Accuracy&#39;: make_scorer(accuracy_score), &#39;F1_score&#39;: make_scorer(f1_score), &#39;Recall&#39; : make_scorer(recall_score), &#39;Precision&#39; : make_scorer(precision_score)} . . encode_columns = categorical.copy() encode_columns.remove(&#39;fbs&#39;) X = pd.get_dummies(X, columns=encode_columns) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65) #this scaler = StandardScaler() scaler.fit(X_train[continuous]) X_train[continuous] = scaler.transform(X_train[continuous]) X_test[continuous] = scaler.transform(X_test[continuous]) . scores = pd.DataFrame({}) for name, model in models.items(): score = cross_validate(model, X_train, y_train, scoring=scoring) temp = pd.DataFrame(score).mean() scores[name] = temp scores = scores.T.drop(&#39;score_time&#39;, axis=1) scores . . fit_time test_Accuracy test_F1_score test_Recall test_Precision . SVM 0.004664 | 0.810034 | 0.829819 | 0.856410 | 0.806929 | . Random Forest 0.150327 | 0.789456 | 0.811416 | 0.833903 | 0.792085 | . Logistic Regression 0.010863 | 0.834864 | 0.853075 | 0.886610 | 0.825573 | . Gradient Boosting 0.088992 | 0.743793 | 0.776820 | 0.818519 | 0.741690 | . AdaBoost Classifier 0.076208 | 0.747874 | 0.772512 | 0.788319 | 0.763996 | . XGBoost Classifier 0.034040 | 0.776956 | 0.805907 | 0.856695 | 0.763046 | . MultiLayer Perceptron Classifier 0.293770 | 0.809949 | 0.830393 | 0.856125 | 0.807823 | . CV라서 그런지 생각보다 드라마틱한 변화는 없다. 오히려 내려간 느낌이다. 데이터 셋이 작기 때문에 당연한 것이라 생각된다. . from sklearn.model_selection import GridSearchCV model = LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=500, random_state=42) params = {&#39;C&#39; : np.arange(0.1, 1, 0.1), &#39;l1_ratio&#39; : np.arange(0, 1, 0.05)} gs = GridSearchCV(estimator=model, param_grid=params, scoring=make_scorer(accuracy_score), cv=5, n_jobs=-1) gs.fit(X_train, y_train) . . GridSearchCV(cv=5, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=500, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;elasticnet&#39;, random_state=42, solver=&#39;saga&#39;, tol=0.0001, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=-1, param_grid={&#39;C&#39;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), &#39;l1_ratio&#39;: array([0. , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=make_scorer(accuracy_score), verbose=0) . print(gs.best_params_) . . {&#39;C&#39;: 0.1, &#39;l1_ratio&#39;: 0.0} . model= LogisticRegression(penalty=&#39;l2&#39;, max_iter=500, C=0.1, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) df_cm = pd.DataFrame(cm) sns.heatmap(df_cm, annot=True, cmap=&#39;Reds&#39;) plt.title(&#39;Confusion Matrix for Logistic&#39;, fontsize=15) plt.xlabel(&quot;Predicted&quot;) plt.ylabel(&quot;True&quot;) plt.show() print(&#39;Accuracy : &#39;,accuracy_score(y_test, y_pred)) print(&#39;F1-score : &#39;,f1_score(y_test, y_pred)) . . Accuracy : 0.9672131147540983 F1-score : 0.967741935483871 . random_state=65 에선 점수가 무려 96.72%로 나온다. 혹시 더 잘 나눠주는 random_state는 없을까? . random_state = 0 ~ 999 . Accuarcy_list = [] F1_list = [] for i in range(1000): X = df[[&#39;sex&#39;, &#39;restecg&#39;, &#39;cp&#39;, &#39;exng&#39;, &#39;thall&#39;, &#39;caa&#39;, &#39;slp&#39;, &#39;age&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;thalachh&#39;, &#39;oldpeak&#39;]] y = df[&#39;output&#39;] encode_columns = categorical.copy() encode_columns.remove(&#39;fbs&#39;) X = pd.get_dummies(X, columns=encode_columns) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) scaler = StandardScaler() scaler.fit(X_train[continuous]) X_train[continuous] = scaler.transform(X_train[continuous]) X_test[continuous] = scaler.transform(X_test[continuous]) model = LogisticRegression(penalty=&#39;l2&#39;, max_iter=500) params = {&#39;C&#39; : np.arange(0.1, 1, 0.1),} gs = GridSearchCV(estimator=model, param_grid=params, scoring=make_scorer(accuracy_score), cv=5, n_jobs=-1) gs.fit(X_train, y_train) C = gs.best_params_[&#39;C&#39;] model= LogisticRegression(penalty=&#39;l2&#39;, max_iter=500, C=C) model.fit(X_train, y_train) y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) Accuarcy_list.append(acc) F1_list.append(f1) . . plt.bar(range(100),Accuarcy_list[0:100]) plt.ylim((0.78, 1)) . (0.78, 1.0) . max(Accuarcy_list) . 0.9672131147540983 . random_state = 0 ~ 99 까지의 그래프만 그려보아도 65번이 가장 높아보이는 것을 알 수 있다. random_state=65 에서의 Accuracy가 96.72% 였으니 0~999에서도 random_state=65 에서의 점수가 가장 잘 나오는 것이라 결론지을 수 있겠다. . &#45712;&#45184;&#51216; . 데이터셋이 작은만큼 어떻게 데이터가 나눠졌는가에 따라 점수가 크게 갈린다. . 오히려 모델을 잘 선정하고 튜닝하는 것보다 데이터 셋이 잘 나눠졌을 때 점수 상승 폭이 더욱 컸다. 아무래도 이번 분석은 모델, 튜닝, 이런 것들 보다도 데이터셋의 크기의 중요성과 잘 나눠진 데이터셋이란 무엇인지에 대해 생각할 수 있던 기회였다. . &#48152;&#49457;&#47928; . 의료데이터의 데이터 포인트 하나하나는 숫자가 아니라 환자다. . 현실의 문제에선 &quot;데이터셋이 잘 정제되어있는가?&quot;, &quot;오류가 있는가?&quot;에 대한 문제보다도 먼저 고민해야할 점은 &quot;과연 데이터셋이 존재하는가? 없다면 수집 비용은 얼마인가?&quot;이다. 모델의 비교와 튜닝은 당연히 데이터셋이 존재하고 정제가 된 다음의 문제이다. 의료데이터와 같이 데이터 수집 비용이 큰 데이터의 경우 데이터의 크기가 작은 것이 당연했고 outlier가 있는 이상 데이터 분할을 섬세하게 했어야 했다. EDA 후에 outlier와 범주 불균형의 존재를 확인하고 어떻게 나눌지를 다시 고민했어야했다. . 요즘의 나는 분석 문제를 자주 마주하다보니 데이터의 가치를 종종 잊곤 한다. 그저 풀어야하는 문제로 데이터의 가치가 전락하는 것이다. 빨리빨리 데이터를 통해 모델을 적용해보고 싶은 마음에 그만 분석의 목적을 잊는 것이다. 데이터의 가치를 누구보다도 잘 알아야 할 분석자로썬 아주 아이러니한 상황인 것이다. . 데이터를 분석할 때엔 그것을 나의 문제라고 생각할 때 정말 사소한 부분까지 확인하게 된다. 하지만 최근 데이터를 문제라고 생각하다보니 데이터가 수집 되기까지의 과정을 생각해보지 않았고, 데이터 속 환자를 단순히 숫자로만 인식해버렸다. . 데이터를 분석하기 위해선 당연히 수치화가 필요하다. 객관적인 분석을 위해선 편견을 되도록 배제하는 것 또한 필요하다. 하지만 그 과정속에서 목적을 잃은체 문제 풀이식으로 모델 적용하는 것은 지양해야한다. 데이터 하나하나가 갖는 의미를 잃은체 믹서기에 갈리는 일은 없어야겠다. . 조금 웃기지만 앞으로는 수집된 표본들과 데이터 제공자에게 항상 감사한 마음으로 내 일이다 생각하고 분석을 해야겠다. . 적다보니 느낀점보단 반성문이 되었버렸다.. 잘못했으니 반성하는 것이라 생각하자. .",
            "url": "https://edypidy.github.io/studyblog/kaggle/heart%20desease/jupyter/classification/xgboost/adaboost/lgbm/logistic%20regression/randomforest/mlpclassifier/2021/12/25/kaggle_study-_Heart_Attack_Analysis_&_Prediction_Dataset.html",
            "relUrl": "/kaggle/heart%20desease/jupyter/classification/xgboost/adaboost/lgbm/logistic%20regression/randomforest/mlpclassifier/2021/12/25/kaggle_study-_Heart_Attack_Analysis_&_Prediction_Dataset.html",
            "date": " • Dec 25, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "관심사1 . 학습이론, 성공학 | 메타인지 . | 인지심리학 | 행동심리학 . | 테세우스의 배 딜레마 . | 세계사 | 국제 정치 | 경제, 마케팅(플랫폼 마케팅, 라이브 커머스) | 주식, 비트코인 . | 해석학, 선형대수학, 다변수미적분학, 벡터미적분학 | 양자역학 | 다중 우주론, 시뮬레이션 우주론 | . 관심사2 . 인디음악 | 락 | 뮤지컬 | 연극 | 클래식피아노 . | 헬스 | . 좋아하는 것1 . 요리하기 | 허튼 생각하기 | 책읽기 | 글쓰기 | 노래하기 | 산책하기 | 멍때리기 | . 좋아하는 것2 . 효율화 시키기 | 구조화 시키기 | 비틀어서 생각하기 | 당연해 보이는 것에 의문 가지기 | 일반화 시키기 | . 좋아하는 것3 . 비빔냉면, 물냉면, 제육볶음, 마라샹궈 | 육류, 샐러드, 회 | . 좋아하는 것4 . 골든 리트리버, 사모예드, 시바견, 웰시코기, 프렌치 불독, 시고르자브종 | 코리안숏헤어, 스코티쉬폴드, 먼치킨 | 고슴도치, 기니피그 | 오리, 청둥오리 | 거북이, 자라, 도마뱀 | 청개구리, 팩맨 | . 좋아하는 것5 . 프랑크 소세지 두개 + 블랑 맥주 한캔 | 쥬씨 코코넛 망고/천도복숭아 마시면서 산책하기 | 뜨뜻한 목욕탕 가서 누워있다가 시원한 바나나 우유나 웰치스 한 캔 마시면서 집에오기 | 뜨거운 라면에다 시원한 김치 감싸먹기 | 괜찮은 책 하나 사서 책장에 꽂아두기 | .",
          "url": "https://edypidy.github.io/studyblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://edypidy.github.io/studyblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}