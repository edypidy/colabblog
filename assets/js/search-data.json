{
  
    
        "post0": {
            "title": "딥러닝 포스팅",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pidyology.github.io/testcolabblog/deep%20learning/2021/08/16/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4_%ED%85%8C%EC%8A%A4%ED%8A%B8.html",
            "relUrl": "/deep%20learning/2021/08/16/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4_%ED%85%8C%EC%8A%A4%ED%8A%B8.html",
            "date": " • Aug 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "[ML]로지스틱 회귀의 유도와 구현",
            "content": ". Intro . | Data preparation . | 확률 예측시 Linear Regression의 문제점 및 activation function 추론 . | L2거리 비용함수의 문제점 . | Likelihood, Log Likelyhood function과 경사하강법 . | 로지스틱 회귀 알고리즘 구현 실험 . | scikit-learn 로지스틱 회귀 . | 로지스틱 회귀에서의 규제 . | scikit-learn의 비용함수 최적화 알고리즘 . | predict_proba() method와 샘플 하나 예측시 주의사항 . | Intro . import matplotlib.pyplot as plt import numpy as np fig, ax = plt.subplots(1,3, figsize=(12,4)) # 시그모이드 def sigmoid(z): return 1/(1 + np.exp(-z)) z = np.linspace(-3, 3, 100) ax[0].plot(z, sigmoid(z)) ax[0].text(-2.7,0.78,&#39;$y = 1/(1+e^{-z})$&#39;, fontsize = 13) ax[0].axvline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[0].set_ylim(-0.1, 1.1) ax[0].set_title(&#39;Sigmoid&#39;) ax[0].set_xlabel(&#39;z = wx&#39;) ax[0].set_ylabel(&#39;phi(z)&#39;) ax[0].set_yticks([0, 0.5, 1]) ax[0].yaxis.grid() plt.tight_layout() # 로짓 def logit(p): return np.log(p/(1-p)) p = np.linspace(0.06, 0.94, 100) ax[1].plot(p, logit(p)) ax[1].text(0.08,1.6,&#39;$y = log(p/(1-p))$&#39;, fontsize = 13) ax[1].axhline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[1].set_xlim(-0.1, 1.1) ax[1].set_title(&#39;Logit&#39;) ax[1].set_xlabel(&#39;p&#39;) ax[1].set_ylabel(&#39;logit(p)&#39;) ax[1].set_xticks([0, 0.5, 1]) ax[1].xaxis.grid() plt.tight_layout() # 합성함수 p = np.linspace(0.06, 0.94, 100) ax[2].plot(p, sigmoid(logit(p))) ax[2].text(0.62,0.42,&#39;$y = p$&#39;, fontsize = 15) ax[2].set_xlim(0, 1) ax[2].set_ylim(0, 1) ax[2].set_title(&#39;Sigmoid(Logit)&#39;) ax[2].set_xlabel(&#39;p&#39;) ax[2].set_ylabel(&#39;phi(logit(p))&#39;) ax[2].grid() plt.tight_layout() plt.show() . 나는 학교 회귀분석 과목에서 로지스틱 함수를 배울때 왜 &#39;오즈&#39; 라는 개념을 굳이 사용할까 라는 의문을 항상 가지면서 공부했다. 교수님께 여쭤보았을 땐 그저 &quot;오즈는 승산이죠~&quot; 라는 답변을 주셨지만 도통 그 의미를 이해할 수 없었다. 어차피 확률과 일대일 대응인 오즈라는 개념을 굳이 왜 쓰는 것인가? 확률이 더 직관적이고 좋은데 말이다. 고민 끝에 내린 답은 선형 회귀에서의 예측되는 확률이 엇나가는 것을 로짓 변환을 통해 미연에 방지할 수 있기 때문이라는 것이다. . 확인을 위해 확률을 단순 선형회귀로 예측하는 것과 로지스틱 회귀로 예측한 것을 비교해보자. . Data preparation . np.random.seed(1) data0_input = 0.3*np.random.randn(30) data1_input = 1 + 0.5*np.random.randn(30) data0_target = np.zeros(30) data1_target = np.ones(30) data0 = np.column_stack((data0_input, data0_target)) data1 = np.column_stack((data1_input, data1_target)) # 데이터 로드 data = np.row_stack((data0, data1)) data = np.random.permutation(data) print(data[:5]) . [[-0.20511836 0. ] [-0.03686707 0. ] [ 0.17484456 0. ] [ 0.44134483 1. ] [ 0.82532864 1. ]] . plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . &#54869;&#47456; &#50696;&#52769;&#49884; Linear Regression&#51032; &#47928;&#51228;&#51216; &#48143; activation function &#52628;&#47200; . input, target = data[:,0].reshape(-1,1), data[:,1].reshape(-1,) # 단순선형회귀 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(input, target) print(lr.score(input, target)) print(lr.coef_, lr.intercept_) . 0.6788575359335094 [0.64269595] 0.17212798057869722 . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, 0.6427*x + 0.1721, label=&#39;lr&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.vlines(-0.1721/0.6427, 0., 0.5, linestyles=&#39;--&#39;) plt.text(-0.5, 0.7, &#39;Does it looks like p=0.5?&#39;) plt.grid() plt.legend() plt.show() . 단순 선형회귀로 확률을 추정해보았으나 확률이 0과 1을 넘어가는 기이한 형상을 띤다. 이는 잘못된 추정이라 볼 수 있다. 시그모이드가 0과 1사이의 값으로 바꿔준다는 말에 혹해서 시그모이드 함수에 단순선형회귀식을 넣어보더라도 전혀 데이터의 분포를 말해주지 못하는 것 같다. . 문제를 해결할 방법으로는 단순선형회귀로 추정값이 로그값이면 된다. 단순선형회귀 추정값의 범위로 실수 전체가 타당해질 수 있다. 그렇다면 $ log(p) = mathbf{w^T x}$ 로 추정한다면 괜찮을까? 아쉽지만 로그확률을 다시 확률로 해석할 때엔 지수함수가 쓰인다(활성화 함수).따라서 $p = e^{ log(p)} = e^{ mathbf{w^T x}}$가 0 근처에선 좋은 확률의 추정치를 줄 수는 있지만 1을 넘어가는 외삽에선 그다지 쓸모가 없을 것이다. . | 활성화 함수의 범위도 중요하다는 것을 위에서 알았다. 이제 추정값과 활성화함수 짝꿍을 위한 조건은 아래와 같다. . 단순선형회귀로 추정하는 대상이 로그값이면서 | 확률로의 활성화함수가 0과 1사이의 값을 가져야한다. | | . 마침 로그오즈가 확률$p$의 함수이면서 동시에 역함수(시그모이드 함수)가 0과 1사이의 값인 것이다. 따라서 아래와 같은 추정이 가능해졌다. . $$inverse of log( frac{p}{1-p})= frac{1}{1+e^{-z}}$$ . $$ log( frac{p}{1-p}) = z = mathbf{w^T x}$$ . $$ frac{1}{1+e^{-z}} = frac{1}{1+e^{- mathbf{w^T x}}} = frac{1}{1+e^{- log(p/1-p)}} = hat{p}$$ . 추정과정만 보면 $ hat{p} = frac{1}{1+e^{- mathbf{w^T x}}}$ 이겠다. . L2&#44144;&#47532; &#48708;&#50857;&#54632;&#49688;&#51032; &#47928;&#51228;&#51216; . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(20*(x-0.5)), label=&#39;rule of thumb sigmoid&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) # 앞서 선형모형에서 최적화한 계수들을 그저 시그모이드 함수에 넣기만 했기 때문에 시그모이드 자체에 대해서 최적화가 이루어지진 않았음 plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.grid() plt.legend(loc=&#39;upper left&#39;) plt.show() . 그렇다면 $ mathbf{w^Tx} = log( frac{p}{1-p})$로 생각하여 이를 단순선형회귀로 추정하고 시그모이드 함수에 넣어 확률을 추정하는 셈이 되는 것인가?. . 그건 또 아니다. $ mathbf{w^Tx} = log( frac{p}{1-p})$에서 선형회귀로 추정해버린다면 위의 그래프에서 선형회귀식을 시그모이드 함수에 넣은 것과 같다. . (단순회귀로 추정한 것이 확률이다! 라고 정의한 것에서 사실 로그오즈로 추정한거였다! 로 바뀌었을 뿐이다.) . 결국 이 경우는 회귀직선과 데이터포인트들의 거리비용을 최적화 한 것이지 시그모이드에 회귀직선을 대입한 것과 데이터포인트들과의 거리비용을 최적화한 것이 아니기에 활성화 함수와 포인트들 간에 괴리가 있다. . 그렇다면 아예 비용함수를 $ sum(y^{(i)} - frac{1}{1+e^{- mathbf{w^T x^{(i)}}}})^2$로 정의하면 어떨까? 비용함수를 만들어보자. . $$J( mathbf{w}) = frac{1}{2} sum^{n}_{i=1}( phi( mathbf{w^Tx^{(i)}}) - y^{(i)})^2$$ . $y$들은 0 또는 1의 값을 가지고 비용함수가 L2(유클리디안) 거리비용함수로 정의되어 있으므로 최적화하여 적합되는 시그모이드 함수의 모양은 아래와 같을 것이다. 두 클래스를 구분하는 데에만 초점을 맞춘다면 의미가 없진 않겠지만 확률적 의미를 부여하기엔 너무나 부족해보인다. . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(1000*x - 500)) # 유클리디언 거리가 최소가 되도록 한다면 점에 붙으려 할 것이다. plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp . Likelihood, Log Likelyhood function&#44284; &#44221;&#49324;&#54616;&#44053;&#48277; . 비용함수를 다르게 정의해야할 필요성을 위에서 보았다. 데이터가 주어져 있으니 여러 데이터포인트들이 그렇게 나올법한 확률을 최대화 시키는 가중치들을 구하면 될 것이다. 가능도함수를 목적함수로 하여 최대화하는 가중치를 찾는 것이 적당해보인다. . 마침 $z^{(i)}$를 로그오즈로 생각하기로 했으니 $ phi(z^{(i)}) = hat{p}^{(i)}$ 즉, 각각을 i번째 데이터포인트의 양성 확률로 생각할 수 있다. 각 데이터포인트들이 서로 독립적이라는 가정하에 가능도함수는 아래와 같다. . $$L( mathbf{w}) = P( mathbf{y} | mathbf{x;w}) = prod^n_{i=1}( phi(z^{(i)}))^{y^{(i)}}(1- phi(z^{(i)}))^{1-y^{(i)}}$$ . 가능도함수를 최대화 하는 것은 로그 가능도함수를 최대화하는 것과 동일하므로 다루기 쉬운 로그 가능도함수를 최대화하자. . $$l( mathbf{w}) = sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 가능도함수에 로그를 적용하면 가능도가 매우 작을 때 0으로 생략되는 것을 미연에 방지한다. 도함수도 쉽게 구할 수 있으니 일석이조다. . 경사하강법 최적화 알고리즘 사용을 위해 로그 가능도함수를 비용함수로 표현하자. . $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 비용함수의 이해를 위해 샘플이 하나일 때의 비용을 계산해보자. 해석의 편의를 위해 가중치 대신 가중치마다의 시그모이드와 참값$y$를 변수로 생각하자.(사실 샘플이 하나라서 $ mathbf{w}$ 대신 $ phi(z)$의 함수로 봐도 좋다.) . $$J( phi(z), y; mathbf{w}) = -y log phi(z) - (1-y) log (1 - phi(z))$$ . $y=1$일 때와 $y=0$일 때를 나누어 생각하면 3차원 상의 비용함수 그래프를 2차원에 그릴 수 있다. . $$J( phi(z), y; mathbf{w}) = left { begin{matrix} - log phi(z)&amp; y=1 - log (1 - phi(z))&amp; y=0 end{matrix} right.$$ . def cost_1(z): return - np.log(sigmoid(z)) def cost_0(z): return -np.log(1 - sigmoid(z)) z = np.linspace(-4, 4, 100) phi_z = sigmoid(z) plt.plot(phi_z, cost_1(z), label=&#39;J(w) where y = 1&#39;) plt.plot(phi_z, cost_0(z), linestyle=&#39;--&#39;, label=&#39;J(w) where y = 0&#39;) plt.xlim([0, 1]) plt.ylim([0, 4.1]) plt.xlabel(&#39;$ phi$(z)&#39;) plt.ylabel(&#39;J(w)&#39;) plt.legend(loc = &#39;upper center&#39;) plt.tight_layout() plt.show() . 범주 1에 속하는 샘플이 범주 1에 속할 확률을 높게 예측할수록 그렇게 예측한 가중치의 비용은 0에 가까워졌고 범주 0에 속하는 샘플이 범주 0에 속할 확률을 높게 예측할수록 역시 비용이 0에 가까워졌다. 반대로 잘못된 예측확률에는 큰 비용을 부여한다. . 즉, (맞으면 비용감소, 틀리면 비용증가)이므로 직관과 일치한다. . 클래스 1을 1같다고 하면 비용이 작아지고 . | 클래스 1을 0이라고 하면 비용이 커진다. . | . (클래스 0의 경우 반대) . 알고있는 기존의 경사하강법 규칙으로부터 로지스틱 회귀에서의 경사하강법 규칙이 잘 일반화 되어있는지 확인해보자. . $J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$를 $w_j$에 대하여 편미분하면 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)} frac{1}{ phi(z^{(i)})} + (1-y^{(i)}) frac{1}{(1 - phi(z^{(i)}))} ] frac{ partial phi(z^{(i)})}{ partial w_j}$$ . 한편, $$ frac{ partial phi(z^{(i)})}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)})) frac{ partial z^{(i)}}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)}))x^{(i)}_j$$ 이므로 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)}(1 - phi(z^{(i)})) + (1-y^{(i)}) phi(z^{(i)}) ]x^{(i)}_j = - sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 이 된다. . 간단히 나타내면 $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum (y - phi(z))x_j $$ . | 선형대수적으로 쓰면 아래와 같다. $$ triangledown J_{p times 1} = - mathbf{X_{n times p}^T (y_{p times 1} - phi(X_{n times p}w_{p times 1}))} $$ . | . 다시 돌아와서 로지스틱 비용함수를 최소화하는 가중치를 찾는 것이 목표이므로 경사하강법의 방법을 적용하면 . $$ triangle w_j := - eta frac{ partial J( mathbf{w})}{ partial w_j}$$ . $$w_j = w_j + triangle w_j$$ . $$w_j := w_j + eta sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 인데, 이는 로그 가능도함수에 경사상승법을 적용하여 가능도 함수를 최대화 하는 가중치의 업데이트 방법과 동일하다. 즉, 가능도를 최대화하는 가중치와 로지스틱 비용함수를 최소화 하는 가중치는 서로 같다 . $$ mathbf{w := w + triangle w}$$ . $$ triangle mathbf{w} = - eta triangledown J( mathbf{w}) = eta triangledown l( mathbf{w})$$ . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#50508;&#44256;&#47532;&#51608; &#44396;&#54788; &#49892;&#54744; . class LogisticRegressionGD(object): &quot;&quot;&quot;경사 하강법을 사용한 로지스틱 회귀 분류기 매개변수 eta : float 학습률(0.0 에서 1.0 사이) n_iter : int 훈련 데이터셋 반복횟수 random_state : int 가중치 무작위 초기화를 위한 난수 생성기 시드 속성 w_ : 1d-array 학습된 가중치 cost_ : list 에포크마다 누적된 로지스틱 비용 함수 값 &quot;&quot;&quot; def __init__(self, eta=0.01, n_iter=100, random_state=1): self.eta = eta self.n_iter = n_iter self.random_state = random_state def fit(self, X, y): &quot;&quot;&quot;훈련 데이터 학습 매개변수 -- X : {array-like}, shape = [n_samples, n_features] n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련데이터 y : array-like, shape = [n_samples] 타깃값 반환값 -- self : object &quot;&quot;&quot; rgen = np.random.RandomState(self.random_state) self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) self.cost_ = [] for _ in range(self.n_iter): net_input = self.net_input(X) output = self.activation(net_input) errors = (y-output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() # 오차제곱합 대신 로지스틱 비용을 계산합니다. cost = ( -y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))) ) self.cost_.append(cost) return self def net_input(self, X): &quot;&quot;&quot;입력 계산&quot;&quot;&quot; return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, z): &quot;&quot;&quot;로지스틱 시그모이드 활성화 계산&quot;&quot;&quot; return 1./(1.+ np.exp(-np.clip(z, -250, 250))) def predict(self, X): &quot;&quot;&quot;단위 계단 함수를 사용하여 클래스 레이블을 반환합니다.&quot;&quot;&quot; return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # 최종 입력값이 0보다 크면 시그모이드 값도 0.5보다 크다 # 아래와 동일합니다. # return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0) . lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) coef = lgr.fit(input, target).w_ print(coef) . [-7.22145597 15.28156541] . fig, ax = plt.subplots(1, 2, figsize=(15, 4)) epochs = np.arange(0,1000) ax[0].plot(epochs, lgr.cost_) ax[0].set_title(&#39;Does it converges?&#39;) ax[0].set_xlabel(&#39;Epoch&#39;) ax[0].set_ylabel(&#39;Weights&#39;) ax[0].set_xlim((1,1000)) ax[0].set_ylim((3,35)) ax[0].grid() ax[1].plot(epochs, lgr.cost_) ax[1].set_title(&#39;Does it converges?(log scale)&#39;) ax[1].set_xlabel(&#39;Epoch(log scale)&#39;) ax[1].set_xscale(&#39;log&#39;) ax[1].set_xlim((1,1000)) ax[1].set_ylim((3,35)) ax[1].grid() plt.show() . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(coef[0] + coef[1]*x)) plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . 로지스틱 모델을 Iris-setosa와 Iris-versicolor 붖꽃만 가지고 로지스틱 회귀의 분류모델 구현이 작동하는지 확인해보자. . from sklearn import datasets import numpy as np . iris = datasets.load_iris() print(iris.data[:3]) print(iris.target[:3]) . [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2]] [0 0 0] . X = iris.data[:, [2,3]] y = iris.target print(np.unique(y)) . [0 1 2] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std = ss.transform(X_test) . X_train_01subset = X_train_std[(y_train == 0) | (y_train == 1)] y_train_01subset = y_train[(y_train == 0) | (y_train == 1)] lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) lgr.fit(X_train_01subset, y_train_01subset) . &lt;__main__.LogisticRegressionGD at 0x7f603fa314d0&gt; . from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) plot_decision_regions(X_train_01subset, y_train_01subset, classifier=lgr) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . scikit-learn &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 사이킷런 모듈은 로지스틱 회귀 모델을 지원한다. 이 모델은 위의 구현과 달리 세개이상의 다중분류도 지원한다. . from sklearn.linear_model import LogisticRegression lgr = LogisticRegression(C=100.0, random_state=1) lgr.fit(X_train_std, y_train) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X_combined_std, y_combined, classifier=lgr, test_idx=range(len(y_train),len(y))) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . print(lgr.coef_, lgr.intercept_) for i, w0, w1, w2 in zip(range(3), lgr.intercept_, lgr.coef_[:, 0], lgr.coef_[:, 1]): print(&#39;model{:} : {:.2f} + PL * {:.2f} + PW * {:.2f} &#39;.format(i, w0, w1, w2) ) . [[-6.93265988 -5.76495748] [-2.03192177 -0.03413691] [ 8.96458165 5.79909439]] [-0.9576182 5.70388044 -4.74626223] model0 : -0.96 + PL * -6.93 + PW * -5.76 model1 : 5.70 + PL * -2.03 + PW * -0.03 model2 : -4.75 + PL * 8.96 + PW * 5.80 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#50640;&#49436;&#51032; &#44508;&#51228; . 규제를 사용하여 과대적합을 피하는 것은 이미 다른 문서에서 많이 다루었으므로 살짝만 하고 지나가자(cf-편향과 분산 참고). . 높은 분산은 과대적합에 비례하고(과대적합은 일반화한 모델이 너무 많은 변동을 끌어갔기 때문) 높은 편향은 과소적합에 비례한다(과소적합은 적합이 덜돼서 구조적인 편향이 발생). 과대적합의 경우에는 모델이 가지는 파라미터의 수를 줄이거나 모델이 가지는 모수들의 크기를 제한함으로써(규제) 해결할 수 있다(이 과정을 모델의 복잡도를 줄인다고 표현하기도 한다.). 반대로 과소적합의 경우에는 모델이 가지는 파라미터의 수를 늘려보는 식으로 해결할 수 있겠다. 여기서는 로지스틱 회귀의 규제에 따른 회귀계수의 변화를 보자. . 회귀계수들의 제곱항을 패널티항으로 갖는 L2규제 로지스틱 비용은 다음과 같다. $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ] + frac{ alpha}{2} left | left | mathbf{w} right | right |^2$$ . 릿지 회귀과 라쏘 회귀에서 규제항을 제어하는 파라미터는 $ alpha$였지만 사이킷런의 로지스틱 회귀에서 규제항을 제어하는 파라미터는 C이다. 주의할 점은 C는 $ alpha$의 역수이다. C가 클수록 규제는 완화된다. . petal length와 petal width 계수들의 규제 크기에 따른 크기변화를 그래프로 나타내보자. 규제가 완화될수록(C가 커질수록) 회귀계수들의 크기도 커지는 경향이 있는 것을 볼 수 있다. . weights0, weights1, params = [], [], [] for C in np.arange(-5,5): lgr = LogisticRegression(C=10.**C, random_state=1, multi_class=&#39;ovr&#39;) lgr.fit(X_train_std, y_train) weights0.append(lgr.coef_[0]) weights1.append(lgr.coef_[1]) params.append(10.**C) weights0, weights1 = np.absolute(weights0), np.absolute(weights1) plt.plot(params, weights0[:,0], label=&#39;m0:petal length&#39;) plt.plot(params, weights1[:,0], label=&#39;m1:petal length&#39;, linestyle=&#39;--&#39;) plt.plot(params, weights0[:,1], label=&#39;m0:petal width&#39;, linestyle=&#39;-.&#39;) plt.plot(params, weights1[:,1], label=&#39;m1:petal width&#39;, linestyle=&#39;:&#39;) plt.title(&#39;Absolute weights&#39;) plt.xlabel(&#39;C&#39;) plt.ylabel(&#39;Absolute weight&#39;) plt.xscale(&#39;log&#39;) plt.legend() plt.show() . scikit-learn&#51032; &#48708;&#50857;&#54632;&#49688; &#52572;&#51201;&#54868; &#50508;&#44256;&#47532;&#51608; . 로지스틱 비용함수처럼 볼록한 손실함수를 최소화하는 데는 확률적 경사 하강법(SGD) 대신에 더 고급 방법을 사용하는 것이 좋다. 실제 사이킷런은 다양한 최적화 알고리즘을 제공하며 solver= 매개변수로는 아래와 같은 것들이 있다. . &#39;newton-cg&#39; | &#39;lbfgs&#39; | &#39;liblinear&#39; | &#39;sag&#39; | &#39;saga&#39; | . 다중분류 매개변수인 LogisticRegression의 multiclass=의 기본값은 &#39;auto&#39;이다. &#39;auto&#39;로 설정하면 이진 분류이거나 solver=&#39;liblinear&#39;일 경우에 &#39;ovr&#39;(One versus Rest)를 선택하고 그 외에는 &#39;multinomial&#39;을 선택한다. 이는 &#39;liblinear&#39; 최적화 알고리즘이 다항 로지스틱 회귀 손실을 다룰 수 없고 다중클래스 분류를 위해 OvR 방법을 사용해야하기 때문이다. . predict_proba() method&#50752; &#49368;&#54540; &#54616;&#45208; &#50696;&#52769;&#49884; &#51452;&#51032;&#49324;&#54637; . 확률을 예측하고 싶다면 predict_proba 메서드를 사용하여 계산하자. . print(lgr.predict_proba(X_test_std[:3, :])) . [[1.52213484e-12 3.85303417e-04 9.99614697e-01] [9.93560717e-01 6.43928295e-03 1.14112016e-15] [9.98655228e-01 1.34477208e-03 1.76178271e-17]] . print(lgr.predict_proba(X_test_std[:3, :]).argmax(axis=1)) print(lgr.predict(X_test_std[:3, :])) . [2 0 0] [2 0 0] . lgr.predict(X_test_std[0, :].reshape(1,-1)) # lgr.predict(X_test_std[0, :]) 이렇게 하면 에러난다. . array([2]) . print(&#39;단순 인덱싱 뽑기 : &#39;, X_test_std[0, :].shape) print(&#39;2차원 배열로 변환:&#39;, X_test_std[0, :].reshape(1,-1).shape) . 단순 인덱싱 뽑기 : (2,) 2차원 배열로 변환: (1, 2) .",
            "url": "https://pidyology.github.io/testcolabblog/jupyter/logistic%20regression/classifying/loss%20function/2021/08/16/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80(%EC%B5%9C%EC%A2%85).html",
            "relUrl": "/jupyter/logistic%20regression/classifying/loss%20function/2021/08/16/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80(%EC%B5%9C%EC%A2%85).html",
            "date": " • Aug 16, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "실로 삶은 기적이다. 기적의 연속이다. 나에게 베풀어진 삶이 그러했고 앞으로 남은 삶의 하루하루도 그렇게 주어질 것이기 때문이다. .",
          "url": "https://pidyology.github.io/testcolabblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pidyology.github.io/testcolabblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}