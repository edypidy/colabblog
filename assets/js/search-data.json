{
  
    
        "post0": {
            "title": "[ML]로지스틱 회귀 유도 및 scikit-learn api 맛보기",
            "content": ". Intro . import matplotlib.pyplot as plt import numpy as np fig, ax = plt.subplots(1,3, figsize=(12,4)) # 시그모이드 def sigmoid(z): return 1/(1 + np.exp(-z)) z = np.linspace(-3, 3, 100) ax[0].plot(z, sigmoid(z)) ax[0].text(-2.7,0.78,&#39;$y = 1/(1+e^{-z})$&#39;, fontsize = 13) ax[0].axvline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[0].set_ylim(-0.1, 1.1) ax[0].set_title(&#39;Sigmoid&#39;) ax[0].set_xlabel(&#39;z = wx&#39;) ax[0].set_ylabel(&#39;phi(z)&#39;) ax[0].set_yticks([0, 0.5, 1]) ax[0].yaxis.grid() plt.tight_layout() # 로짓 def logit(p): return np.log(p/(1-p)) p = np.linspace(0.06, 0.94, 100) ax[1].plot(p, logit(p)) ax[1].text(0.08,1.6,&#39;$y = log(p/(1-p))$&#39;, fontsize = 13) ax[1].axhline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[1].set_xlim(-0.1, 1.1) ax[1].set_title(&#39;Logit&#39;) ax[1].set_xlabel(&#39;p&#39;) ax[1].set_ylabel(&#39;logit(p)&#39;) ax[1].set_xticks([0, 0.5, 1]) ax[1].xaxis.grid() plt.tight_layout() # 합성함수 p = np.linspace(0.06, 0.94, 100) ax[2].plot(p, sigmoid(logit(p))) ax[2].text(0.62,0.42,&#39;$y = p$&#39;, fontsize = 15) ax[2].set_xlim(0, 1) ax[2].set_ylim(0, 1) ax[2].set_title(&#39;Sigmoid(Logit)&#39;) ax[2].set_xlabel(&#39;p&#39;) ax[2].set_ylabel(&#39;phi(logit(p))&#39;) ax[2].grid() plt.tight_layout() plt.show() . . 나는 학교 회귀분석 과목에서 로지스틱 함수를 배울때 왜 &#39;오즈&#39; 라는 개념을 굳이 사용할까 라는 의문을 항상 가지면서 공부했다. 교수님께 여쭤보았을 땐 그저 &quot;오즈는 승산이죠~&quot; 라는 답변을 주셨지만 도통 그 의미를 이해할 수 없었다. 어차피 확률과 일대일 대응인 오즈라는 개념을 굳이 왜 쓰는 것인가? 확률이 더 직관적이고 좋은데 말이다. 고민 끝에 내린 답은 선형 회귀에서의 예측되는 확률이 엇나가는 것을 로짓 변환을 통해 미연에 방지할 수 있기 때문이라는 것이다. . 확인을 위해 확률을 단순 선형회귀로 예측하는 것과 로지스틱 회귀로 예측한 것을 비교해보자. . Data preparation . np.random.seed(1) data0_input = 0.3*np.random.randn(30) data1_input = 1 + 0.5*np.random.randn(30) data0_target = np.zeros(30) data1_target = np.ones(30) data0 = np.column_stack((data0_input, data0_target)) data1 = np.column_stack((data1_input, data1_target)) # 데이터 로드 data = np.row_stack((data0, data1)) data = np.random.permutation(data) print(data[:5]) . [[-0.20511836 0. ] [-0.03686707 0. ] [ 0.17484456 0. ] [ 0.44134483 1. ] [ 0.82532864 1. ]] . plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . &#54869;&#47456; &#50696;&#52769;&#49884; Linear Regression&#51032; &#47928;&#51228;&#51216; &#48143; activation function &#52628;&#47200; . input, target = data[:,0].reshape(-1,1), data[:,1].reshape(-1,) # 단순선형회귀 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(input, target) print(lr.score(input, target)) print(lr.coef_, lr.intercept_) . 0.6788575359335094 [0.64269595] 0.17212798057869722 . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, 0.6427*x + 0.1721, label=&#39;lr&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.vlines(-0.1721/0.6427, 0., 0.5, linestyles=&#39;--&#39;) plt.text(-0.5, 0.7, &#39;Does it looks like p=0.5?&#39;) plt.grid() plt.legend() plt.show() . 단순 선형회귀로 확률을 추정해보았으나 확률이 0과 1을 넘어가는 기이한 형상을 띤다. 이는 잘못된 추정이라 볼 수 있다. 시그모이드가 0과 1사이의 값으로 바꿔준다는 말에 혹해서 시그모이드 함수에 단순선형회귀식을 넣어보더라도 전혀 데이터의 분포를 말해주지 못하는 것 같다. . 문제를 해결할 방법으로는 단순선형회귀로 추정값이 로그값이면 된다. 단순선형회귀 추정값의 범위로 실수 전체가 타당해질 수 있다. 그렇다면 $ log(p) = mathbf{w^T x}$ 로 추정한다면 괜찮을까? 아쉽지만 로그확률을 다시 확률로 해석할 때엔 지수함수가 쓰인다(활성화 함수).따라서 $p = e^{ log(p)} = e^{ mathbf{w^T x}}$가 0 근처에선 좋은 확률의 추정치를 줄 수는 있지만 1을 넘어가는 외삽에선 그다지 쓸모가 없을 것이다. . | 활성화 함수의 범위도 중요하다는 것을 위에서 알았다. 이제 추정값과 활성화함수 짝꿍을 위한 조건은 아래와 같다. . 단순선형회귀로 추정하는 대상이 로그값이면서 | 확률로의 활성화함수가 0과 1사이의 값을 가져야한다. | | . 마침 로그오즈가 확률$p$의 함수이면서 동시에 역함수(시그모이드 함수)가 0과 1사이의 값인 것이다. 따라서 아래와 같은 추정이 가능해졌다. . $$inverse of log( frac{p}{1-p})= frac{1}{1+e^{-z}}$$ . $$ log( frac{p}{1-p}) = z = mathbf{w^T x}$$ . $$ frac{1}{1+e^{-z}} = frac{1}{1+e^{- mathbf{w^T x}}} = frac{1}{1+e^{- log(p/1-p)}} = hat{p}$$ . 추정과정만 보면 $ hat{p} = frac{1}{1+e^{- mathbf{w^T x}}}$ 이겠다. . L2&#44144;&#47532; &#48708;&#50857;&#54632;&#49688;&#51032; &#47928;&#51228;&#51216; . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(20*(x-0.5)), label=&#39;rule of thumb sigmoid&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) # 앞서 선형모형에서 최적화한 계수들을 그저 시그모이드 함수에 넣기만 했기 때문에 시그모이드 자체에 대해서 최적화가 이루어지진 않았음 plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.grid() plt.legend(loc=&#39;upper left&#39;) plt.show() . 그렇다면 $ mathbf{w^Tx} = log( frac{p}{1-p})$로 생각하여 이를 단순선형회귀로 추정하고 시그모이드 함수에 넣어 확률을 추정하는 셈이 되는 것인가?. . 그건 또 아니다. $ mathbf{w^Tx} = log( frac{p}{1-p})$에서 선형회귀로 추정해버린다면 위의 그래프에서 선형회귀식을 시그모이드 함수에 넣은 것과 같다. . (단순회귀로 추정한 것이 확률이다! 라고 정의한 것에서 사실 로그오즈로 추정한거였다! 로 바뀌었을 뿐이다.) . 결국 이 경우는 회귀직선과 데이터포인트들의 거리비용을 최적화 한 것이지 시그모이드에 회귀직선을 대입한 것과 데이터포인트들과의 거리비용을 최적화한 것이 아니기에 활성화 함수와 포인트들 간에 괴리가 있다. . 그렇다면 아예 비용함수를 $ sum(y^{(i)} - frac{1}{1+e^{- mathbf{w^T x^{(i)}}}})^2$로 정의하면 어떨까? 비용함수를 만들어보자. . $$J( mathbf{w}) = frac{1}{2} sum^{n}_{i=1}( phi( mathbf{w^Tx^{(i)}}) - y^{(i)})^2$$ . $y$들은 0 또는 1의 값을 가지고 비용함수가 L2(유클리디안) 거리비용함수로 정의되어 있으므로 최적화하여 적합되는 시그모이드 함수의 모양은 아래와 같을 것이다. 두 클래스를 구분하는 데에만 초점을 맞춘다면 의미가 없진 않겠지만 확률적 의미를 부여하기엔 너무나 부족해보인다. . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(1000*x - 500)) # 유클리디언 거리가 최소가 되도록 한다면 점에 붙으려 할 것이다. plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp . Likelihood, Log Likelyhood function&#44284; &#44221;&#49324;&#54616;&#44053;&#48277; . 비용함수를 다르게 정의해야할 필요성을 위에서 보았다. 데이터가 주어져 있으니 여러 데이터포인트들이 그렇게 나올법한 확률을 최대화 시키는 가중치들을 구하면 될 것이다. 가능도함수를 목적함수로 하여 최대화하는 가중치를 찾는 것이 적당해보인다. . 마침 $z^{(i)}$를 로그오즈로 생각하기로 했으니 $ phi(z^{(i)}) = hat{p}^{(i)}$ 즉, 각각을 i번째 데이터포인트의 양성 확률로 생각할 수 있다. 각 데이터포인트들이 서로 독립적이라는 가정하에 가능도함수는 아래와 같다. . $$L( mathbf{w}) = P( mathbf{y} | mathbf{x;w}) = prod^n_{i=1}( phi(z^{(i)}))^{y^{(i)}}(1- phi(z^{(i)}))^{1-y^{(i)}}$$ . 가능도함수를 최대화 하는 것은 로그 가능도함수를 최대화하는 것과 동일하므로 다루기 쉬운 로그 가능도함수를 최대화하자. . $$l( mathbf{w}) = sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 가능도함수에 로그를 적용하면 가능도가 매우 작을 때 0으로 생략되는 것을 미연에 방지한다. 도함수도 쉽게 구할 수 있으니 일석이조다. . 경사하강법 최적화 알고리즘 사용을 위해 로그 가능도함수를 비용함수로 표현하자. . $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 비용함수의 이해를 위해 샘플이 하나일 때의 비용을 계산해보자. 해석의 편의를 위해 가중치 대신 가중치마다의 시그모이드와 참값$y$를 변수로 생각하자.(사실 샘플이 하나라서 $ mathbf{w}$ 대신 $ phi(z)$의 함수로 봐도 좋다.) . $$J( phi(z), y; mathbf{w}) = -y log phi(z) - (1-y) log (1 - phi(z))$$ . $y=1$일 때와 $y=0$일 때를 나누어 생각하면 3차원 상의 비용함수 그래프를 2차원에 그릴 수 있다. . $$J( phi(z), y; mathbf{w}) = left { begin{matrix} - log phi(z)&amp; y=1 - log (1 - phi(z))&amp; y=0 end{matrix} right.$$ . def cost_1(z): return - np.log(sigmoid(z)) def cost_0(z): return -np.log(1 - sigmoid(z)) z = np.linspace(-4, 4, 100) phi_z = sigmoid(z) plt.plot(phi_z, cost_1(z), label=&#39;J(w) where y = 1&#39;) plt.plot(phi_z, cost_0(z), linestyle=&#39;--&#39;, label=&#39;J(w) where y = 0&#39;) plt.xlim([0, 1]) plt.ylim([0, 4.1]) plt.xlabel(&#39;$ phi$(z)&#39;) plt.ylabel(&#39;J(w)&#39;) plt.legend(loc = &#39;upper center&#39;) plt.tight_layout() plt.show() . 범주 1에 속하는 샘플이 범주 1에 속할 확률을 높게 예측할수록 그렇게 예측한 가중치의 비용은 0에 가까워졌고 범주 0에 속하는 샘플이 범주 0에 속할 확률을 높게 예측할수록 역시 비용이 0에 가까워졌다. 반대로 잘못된 예측확률에는 큰 비용을 부여한다. . 즉, (맞으면 비용감소, 틀리면 비용증가)이므로 직관과 일치한다. . 클래스 1을 1같다고 하면 비용이 작아지고 . | 클래스 1을 0이라고 하면 비용이 커진다. . | . (클래스 0의 경우 반대) . 알고있는 기존의 경사하강법 규칙으로부터 로지스틱 회귀에서의 경사하강법 규칙이 잘 일반화 되어있는지 확인해보자. . $J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$를 $w_j$에 대하여 편미분하면 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)} frac{1}{ phi(z^{(i)})} + (1-y^{(i)}) frac{1}{(1 - phi(z^{(i)}))} ] frac{ partial phi(z^{(i)})}{ partial w_j}$$ . 한편, $$ frac{ partial phi(z^{(i)})}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)})) frac{ partial z^{(i)}}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)}))x^{(i)}_j$$ 이므로 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)}(1 - phi(z^{(i)})) + (1-y^{(i)}) phi(z^{(i)}) ]x^{(i)}_j = - sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 이 된다. . 간단히 나타내면 $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum (y - phi(z))x_j $$ . | 선형대수적으로 쓰면 아래와 같다. $$ triangledown J_{p times 1} = - mathbf{X_{n times p}^T (y_{p times 1} - phi(X_{n times p}w_{p times 1}))} $$ . | . 다시 돌아와서 로지스틱 비용함수를 최소화하는 가중치를 찾는 것이 목표이므로 경사하강법의 방법을 적용하면 . $$ triangle w_j := - eta frac{ partial J( mathbf{w})}{ partial w_j}$$ . $$w_j = w_j + triangle w_j$$ . $$w_j := w_j + eta sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 인데, 이는 로그 가능도함수에 경사상승법을 적용하여 가능도 함수를 최대화 하는 가중치의 업데이트 방법과 동일하다. 즉, 가능도를 최대화하는 가중치와 로지스틱 비용함수를 최소화 하는 가중치는 서로 같다 . $$ mathbf{w := w + triangle w}$$ . $$ triangle mathbf{w} = - eta triangledown J( mathbf{w}) = eta triangledown l( mathbf{w})$$ . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#50508;&#44256;&#47532;&#51608; &#44396;&#54788; &#49892;&#54744; . #collapse-hide class LogisticRegressionGD(object): &quot;&quot;&quot;경사 하강법을 사용한 로지스틱 회귀 분류기 매개변수 eta : float 학습률(0.0 에서 1.0 사이) n_iter : int 훈련 데이터셋 반복횟수 random_state : int 가중치 무작위 초기화를 위한 난수 생성기 시드 속성 w_ : 1d-array 학습된 가중치 cost_ : list 에포크마다 누적된 로지스틱 비용 함수 값 &quot;&quot;&quot; def __init__(self, eta=0.01, n_iter=100, random_state=1): self.eta = eta self.n_iter = n_iter self.random_state = random_state def fit(self, X, y): &quot;&quot;&quot;훈련 데이터 학습 매개변수 -- X : {array-like}, shape = [n_samples, n_features] n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련데이터 y : array-like, shape = [n_samples] 타깃값 반환값 -- self : object &quot;&quot;&quot; rgen = np.random.RandomState(self.random_state) self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) self.cost_ = [] for _ in range(self.n_iter): net_input = self.net_input(X) output = self.activation(net_input) errors = (y-output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() # 오차제곱합 대신 로지스틱 비용을 계산합니다. cost = ( -y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))) ) self.cost_.append(cost) return self def net_input(self, X): &quot;&quot;&quot;입력 계산&quot;&quot;&quot; return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, z): &quot;&quot;&quot;로지스틱 시그모이드 활성화 계산&quot;&quot;&quot; return 1./(1.+ np.exp(-np.clip(z, -250, 250))) def predict(self, X): &quot;&quot;&quot;단위 계단 함수를 사용하여 클래스 레이블을 반환합니다.&quot;&quot;&quot; return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # 최종 입력값이 0보다 크면 시그모이드 값도 0.5보다 크다 # 아래와 동일합니다. # return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0) . . lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) coef = lgr.fit(input, target).w_ print(coef) . [-7.22145597 15.28156541] . fig, ax = plt.subplots(1, 2, figsize=(15, 4)) epochs = np.arange(0,1000) ax[0].plot(epochs, lgr.cost_) ax[0].set_title(&#39;Does it converges?&#39;) ax[0].set_xlabel(&#39;Epoch&#39;) ax[0].set_ylabel(&#39;Weights&#39;) ax[0].set_xlim((1,1000)) ax[0].set_ylim((3,35)) ax[0].grid() ax[1].plot(epochs, lgr.cost_) ax[1].set_title(&#39;Does it converges?(log scale)&#39;) ax[1].set_xlabel(&#39;Epoch(log scale)&#39;) ax[1].set_xscale(&#39;log&#39;) ax[1].set_xlim((1,1000)) ax[1].set_ylim((3,35)) ax[1].grid() plt.show() . . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(coef[0] + coef[1]*x)) plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . 로지스틱 모델을 Iris-setosa와 Iris-versicolor 붖꽃만 가지고 로지스틱 회귀의 분류모델 구현이 작동하는지 확인해보자. . from sklearn import datasets import numpy as np . iris = datasets.load_iris() print(iris.data[:3]) print(iris.target[:3]) . [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2]] [0 0 0] . X = iris.data[:, [2,3]] y = iris.target print(np.unique(y)) . [0 1 2] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std = ss.transform(X_test) . X_train_01subset = X_train_std[(y_train == 0) | (y_train == 1)] y_train_01subset = y_train[(y_train == 0) | (y_train == 1)] lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) lgr.fit(X_train_01subset, y_train_01subset) . &lt;__main__.LogisticRegressionGD at 0x7f603fa314d0&gt; . from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) plot_decision_regions(X_train_01subset, y_train_01subset, classifier=lgr) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . . scikit-learn &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 사이킷런 모듈은 로지스틱 회귀 모델을 지원한다. 이 모델은 위의 구현과 달리 세개이상의 다중분류도 지원한다. . from sklearn.linear_model import LogisticRegression lgr = LogisticRegression(C=100.0, random_state=1) lgr.fit(X_train_std, y_train) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X_combined_std, y_combined, classifier=lgr, test_idx=range(len(y_train),len(y))) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . print(lgr.coef_, lgr.intercept_) for i, w0, w1, w2 in zip(range(3), lgr.intercept_, lgr.coef_[:, 0], lgr.coef_[:, 1]): print(&#39;model{:} : {:.2f} + PL * {:.2f} + PW * {:.2f} &#39;.format(i, w0, w1, w2) ) . [[-6.93265988 -5.76495748] [-2.03192177 -0.03413691] [ 8.96458165 5.79909439]] [-0.9576182 5.70388044 -4.74626223] model0 : -0.96 + PL * -6.93 + PW * -5.76 model1 : 5.70 + PL * -2.03 + PW * -0.03 model2 : -4.75 + PL * 8.96 + PW * 5.80 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#50640;&#49436;&#51032; &#44508;&#51228; . 규제를 사용하여 과대적합을 피하는 것은 이미 다른 문서에서 많이 다루었으므로 살짝만 하고 지나가자(cf-편향과 분산 참고). . 높은 분산은 과대적합에 비례하고(과대적합은 일반화한 모델이 너무 많은 변동을 끌어갔기 때문) 높은 편향은 과소적합에 비례한다(과소적합은 적합이 덜돼서 구조적인 편향이 발생). 과대적합의 경우에는 모델이 가지는 파라미터의 수를 줄이거나 모델이 가지는 모수들의 크기를 제한함으로써(규제) 해결할 수 있다(이 과정을 모델의 복잡도를 줄인다고 표현하기도 한다.). 반대로 과소적합의 경우에는 모델이 가지는 파라미터의 수를 늘려보는 식으로 해결할 수 있겠다. 여기서는 로지스틱 회귀의 규제에 따른 회귀계수의 변화를 보자. . 회귀계수들의 제곱항을 패널티항으로 갖는 L2규제 로지스틱 비용은 다음과 같다. $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ] + frac{ alpha}{2} left | left | mathbf{w} right | right |^2$$ . 릿지 회귀과 라쏘 회귀에서 규제항을 제어하는 파라미터는 $ alpha$였지만 사이킷런의 로지스틱 회귀에서 규제항을 제어하는 파라미터는 C이다. 주의할 점은 C는 $ alpha$의 역수이다. C가 클수록 규제는 완화된다. . petal length와 petal width 계수들의 규제 크기에 따른 크기변화를 그래프로 나타내보자. 규제가 완화될수록(C가 커질수록) 회귀계수들의 크기도 커지는 경향이 있는 것을 볼 수 있다. . weights0, weights1, params = [], [], [] for C in np.arange(-5,5): lgr = LogisticRegression(C=10.**C, random_state=1, multi_class=&#39;ovr&#39;) lgr.fit(X_train_std, y_train) weights0.append(lgr.coef_[0]) weights1.append(lgr.coef_[1]) params.append(10.**C) weights0, weights1 = np.absolute(weights0), np.absolute(weights1) plt.plot(params, weights0[:,0], label=&#39;m0:petal length&#39;) plt.plot(params, weights1[:,0], label=&#39;m1:petal length&#39;, linestyle=&#39;--&#39;) plt.plot(params, weights0[:,1], label=&#39;m0:petal width&#39;, linestyle=&#39;-.&#39;) plt.plot(params, weights1[:,1], label=&#39;m1:petal width&#39;, linestyle=&#39;:&#39;) plt.title(&#39;Absolute weights&#39;) plt.xlabel(&#39;C&#39;) plt.ylabel(&#39;Absolute weight&#39;) plt.xscale(&#39;log&#39;) plt.legend() plt.show() . scikit-learn&#51032; &#48708;&#50857;&#54632;&#49688; &#52572;&#51201;&#54868; &#50508;&#44256;&#47532;&#51608; . 로지스틱 비용함수처럼 볼록한 손실함수를 최소화하는 데는 확률적 경사 하강법(SGD) 대신에 더 고급 방법을 사용하는 것이 좋다. 실제 사이킷런은 다양한 최적화 알고리즘을 제공하며 solver= 매개변수로는 아래와 같은 것들이 있다. . &#39;newton-cg&#39; | &#39;lbfgs&#39; | &#39;liblinear&#39; | &#39;sag&#39; | &#39;saga&#39; | . 다중분류 매개변수인 LogisticRegression의 multiclass=의 기본값은 &#39;auto&#39;이다. &#39;auto&#39;로 설정하면 이진 분류이거나 solver=&#39;liblinear&#39;일 경우에 &#39;ovr&#39;(One versus Rest)를 선택하고 그 외에는 &#39;multinomial&#39;을 선택한다. 이는 &#39;liblinear&#39; 최적화 알고리즘이 다항 로지스틱 회귀 손실을 다룰 수 없고 다중클래스 분류를 위해 OvR 방법을 사용해야하기 때문이다. . predict_proba() method&#50752; &#49368;&#54540; &#54616;&#45208; &#50696;&#52769;&#49884; &#51452;&#51032;&#49324;&#54637; . 확률을 예측하고 싶다면 predict_proba 메서드를 사용하여 계산하자. . print(lgr.predict_proba(X_test_std[:3, :])) . [[1.52213484e-12 3.85303417e-04 9.99614697e-01] [9.93560717e-01 6.43928295e-03 1.14112016e-15] [9.98655228e-01 1.34477208e-03 1.76178271e-17]] . print(lgr.predict_proba(X_test_std[:3, :]).argmax(axis=1)) print(lgr.predict(X_test_std[:3, :])) . [2 0 0] [2 0 0] . lgr.predict(X_test_std[0, :].reshape(1,-1)) # lgr.predict(X_test_std[0, :]) 이렇게 하면 에러난다. . array([2]) . print(&#39;단순 인덱싱 뽑기 : &#39;, X_test_std[0, :].shape) print(&#39;2차원 배열로 변환:&#39;, X_test_std[0, :].reshape(1,-1).shape) . 단순 인덱싱 뽑기 : (2,) 2차원 배열로 변환: (1, 2) .",
            "url": "https://edypidy.github.io/studyblog/jupyter/logistic%20regression/classifying/loss%20function/2021/12/26/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.html",
            "relUrl": "/jupyter/logistic%20regression/classifying/loss%20function/2021/12/26/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "[ML] SVM, Kernel-SVM with Gaussian-rbf kernel와 기본적인 하이퍼파라미터들",
            "content": ". Intro . SVM 문제는 클래스가 다른 데이터들을 구분하는 초평면을 어떻게 정할 것인가에 대한 문제이다. 일반화 오차에 대한 성능을 높이기 위해 마진을 최대로 하는 초평면을 그린다. iris 데이터에 SVM을 적용해보자. . import numpy as np import pandas as pd from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) . . Data Preparation . from sklearn import datasets iris = datasets.load_iris() input = iris.data[:, [2, 3]] target = iris.target . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(input, target, test_size=0.3, stratify = target, random_state=1) . 2차원 평면에 시각화를 위해 iris data에서의 petal length와 petal width를 피쳐로 사용하자. . petal(꽃잎)의 길이와 너비로 꽃의 종류를 분류하는 문제가 되겠다.(sepal(꽃받침)보다는 그럴듯한 상관관계가 있을 것으로 예상된다.) . 잘 알려진 데이터니 만큼 전처리 과정이나 EDA는 건너뛰고 바로 피팅 해보자. . test_size=0.3 : train과 test는 7대 3으로 나누었고. stratify=target : target 꽃의 종류의 비율에 맞추어 train과 test를 나누었으며 random_state=1 : 이건 그냥 재현을 위한 시드다. . Fitting with sklearn . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std= ss.transform(X_test) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) . from sklearn.svm import SVC svm = SVC(kernel=&#39;linear&#39;, C=1.0, random_state=1) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150)) # 빈 동그라미가 쳐진 아이들이 test 셋이다. plt.xlabel(&#39;petal length[std]&#39;) plt.ylabel(&#39;petal width[std]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . train set에 피팅된 SVM 모델이 test_set에도 꽤나 잘 들어맞는 모양새다. 딱 두개의 피쳐만 썼는데도 말이다!. . (사실 그냥 데이터가 원래부터 잘 구분되어있긴 했다.) . 그런데 뭔가 조금 이상하다. SVM은 서로 다른 클래스를 구분하는 평면을 만든다고 했는데 어떻게 세개의 클래스는 어떻게 구분 해야하는가? . 이유인 즉슨 디폴트로 &#39;One Versus Rest&#39;가 적용되었기 때문이다. 2번 클래스의 결정 경계와 평면은 (0번 vs 1번, 2번), (1번 vs 0번, 2번)을 거치면서 &#39;0번과 1번의 영역이 정해졌으니 나머지는 2번이겠구나!&#39; 하는 식으로 결정한 셈이다. . Linear hyperlane&#51004;&#47196; &#44396;&#48516;&#46104;&#51648; &#50506;&#45716; &#45936;&#51060;&#53552; . 다 좋고 모델이 클래스를 잘 분류하는 것 같지만 실제 세계의 데이터는 이렇게 이상적이지 못한 경우가 대부분이다. 과연 &#39;직선&#39;, &#39;평면&#39; 과 같은 선형 결정경계만으로 데이터를 잘 나눌 수 있을까? . 아래의 데이터는 그냥 임의로 만들어 본(또 다른 이상적인 것 일지도 모르는) 데이터이다. 1사분면과 3사분면에는 1번이, 2사분면과 4사분면엔 -1번 클래스가 존재하도록 만든 데이터이다.(x, y의 기울기 탄젠트와 클래스가 관련이 있다면 충분히 있을 수 있는 데이터 셋이다) . np.random.seed(1) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &lt; 0) y_xor = np.where(y_xor, 1, -1) plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], color=&#39;b&#39;, marker=&#39;^&#39;, label=&#39;1&#39;) plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1], color=&#39;r&#39;, marker=&#39;v&#39;, label=&#39;-1&#39;) plt.xlim([-3, 3]) plt.ylim([-3, 3]) plt.hlines(0, -3, 3) plt.vlines(0, -3, 3) plt.legend() plt.tight_layout() plt.show() . 이런 데이터에 Linear SVM Classifier를 적용하면 어떻게 될까? . 당연한 이야기겠지만 선형 초평면으로는 1번 클래스와 -1번 클래스를 절대 나눌 수 없다. . svm = SVC(kernel=&#39;linear&#39;,random_state=1) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . 그러면 이렇게 해보자. 모든 데이터의 x,y 두 값을 곱해서 z열로 추가하는 것이다. . 투영된 데이터가 3차원 공간상에서 선형 초평면에 의해 분류되길 기대하는 것이다. . X_xor_3d = np.column_stack(( X_xor, X_xor[:, 0] * X_xor[:, 1])) x = np.linspace(-3, 3, 200) y = np.linspace(-3, -3, 200) x, y = np.meshgrid(x, y) fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;}) ax.scatter3D(X_xor_3d[y_xor == 1, 0], X_xor_3d[y_xor == 1, 1], X_xor_3d[y_xor == 1, 2], color=&#39;b&#39;, marker=&#39;^&#39;, label=&#39;1&#39;) ax.scatter3D(X_xor_3d[y_xor == -1, 0], X_xor_3d[y_xor == -1, 1], X_xor_3d[y_xor == -1, 2], color=&#39;r&#39;, marker=&#39;v&#39;, label=&#39;-1&#39;) ax.view_init(2,85) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;z&#39;) plt.legend() plt.tight_layout() plt.show() . 첫번째 열과 두번째 열을 곱하여 3차원 공간에 투영시킨 모습이다. . $z = 0$ 평면으로 두 클래스를 가를 수 있다. . Kenel-SVM&#44284; Kernel&#51032; &#51333;&#47448;&#46308; . 이와 같이 기존의 데이터를 통해 새로운 차원을 추가하여 선형적으로 구분이 되게끔 하는 모델을 Kernel-SVM이라고 한다. . 일반적인 커널 함수는 아래와 같다. . 선형 커널 : &#39;linear&#39; . | 다항 커널 : &#39;poly&#39; . | 가우시안 rbf : &#39;rbf&#39; . | 시그모이드 커널 : &#39;sigmoid&#39; . | . (callable한 객체를 대입할 수도 있다. 자세한 것은 도큐먼트를 참고하자.) . sklearn SVC의 디폴트 커널은 사실 linear가 아니라 가우시안 rbf 커널이다. 여기서는 rbf 커널과 규제 매개변수(하이퍼파라미터)들에 대해 살짝 알아보자. . svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=0.1, C=10, ) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend() plt.tight_layout() plt.show() . Gaussian rbf&#50752; gamma &#47588;&#44060;&#48320;&#49688; . def axes_decision_regions(X, y, classifier, axes, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) axes.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) axes.set_xlim(xx1.min(), xx1.max()) axes.set_ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): axes.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] axes.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) gammas = [0.01, 0.1, 0.5, 5] fig, axes = plt.subplots(1,4, figsize=(20,4)) for gamma, ax in zip(gammas, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=10) svm.fit(X_xor, y_xor) axes_decision_regions(X_xor, y_xor, axes=ax, classifier=svm) ax.set_title(&#39;gamma : {:}&#39;.format(gamma)) axes[0].legend() plt.show() . . gamma가 커질수록 SVM 분류기가 1번 데이터에 과적합 되어가는 모습이다. . 과적합이 되어 가면서 1번 데이터의 결정경계가 동글동글 해지는 것을 볼 수 있는데 이를 가우시안 구라고 부른다. . gamma는 가우시안 구의 크기를 제한하는 매개변수로 이해할 수 있다(커널이 rbf인 것을 기억하자). gamma값을 크게 하면 서포트 벡터(원래 결정경계 근처의 점이라고 여길 수 있다)의 영향 범위가 줄어든다. 샘플에 더욱 적합되고 결정경계는 구불구불해진다. . &#39;rbf&#39; 커널함수(방사 기저함수)는 다음과 같다. . ($ mathbf{x}^{(i)}$는 자료 행렬의 $i$행 즉, $i$번째 데이터이다.) . $$K( mathbf{x}^{(i)}, mathbf{x}^{(j)}) = exp left ( - frac{ left | mathbf{x}^{(i)}- mathbf{x}^{(j)} right |^2}{2 sigma^2} right ) = exp left ( - gamma left | mathbf{x}^{(i)}- mathbf{x}^{(j)} right |^2 right )$$ . 여기서 $ gamma = frac{1}{2 sigma^2}$는 최적화 대상 파라미터가 아닌 하이퍼파라미터이다. . 가우시안 rbf 커널이라 불리는 이유는 가우시안 분포함수와 닮아 있어서 붙은 이름이다. 이름보다 중요한 것은 $ exp$에 있다. 직관적으로 $ exp$ 함수는 매클로린 전개시 다항식의 무한합을 가지니까 무한차원으로의 투영이라 생각할 수 있겠다. 자세한 이론적인 부분은 이 블로그{:target=&quot;_blank&quot;} 이 논문{:target=&quot;_blank&quot;} 을 참고하길 바란다. . iris 데이터에서도 확인해보자. . gammas = [0.1, 1, 5, 50] fig, axes = plt.subplots(1,4, figsize=(20,4)) for gamma, ax in zip(gammas, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=1) svm.fit(X_train_std, y_train) axes_decision_regions(X_combined_std, y_combined, axes=ax, classifier=svm, test_idx=range(105,150)) ax.set_title(&#39;gamma : {:}, test_score : {:.2f}&#39;.format(gamma, svm.score(X_test_std, y_test))) axes[0].legend() plt.show() . gamma값이 커질수록 2번 타겟을 제외한 나머지 데이터에 과적합 되어가는 것을 볼 수 있다.(One Versus Rest에 의해 n가지의 클래스 레이블이 있으므로 n-1개의 가우시안 구로 적합되어 간다.) . train_scores, test_scores=[], [] gammas = np.arange(0.1, 1000) for gamma in gammas: svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=1).fit(X_train_std, y_train) train_scores.append(svm.score(X_train_std, y_train)); test_scores.append(svm.score(X_test_std, y_test)) differ = [4*np.abs(tr - ts) for tr, ts in zip(train_scores, test_scores)] plt.plot(gammas, train_scores, label=&#39;train&#39;); plt.plot(gammas, test_scores, label=&#39;test&#39;); plt.plot(gammas, differ, label=&#39;differ*4&#39;) plt.vlines(34, 0, 1, linestyle=&#39;--&#39;); plt.text(1, 0.6,&#39;$gamma = 34$&#39;, fontsize=12) plt.title(&#39;Train &amp; Test scores for Gammas&#39;); plt.xlabel(&#39;Gamma&#39;); plt.ylabel(&#39;score&#39;) plt.xscale(&#39;log&#39;); plt.legend(loc=&#39;center left&#39;); plt.grid() plt.show() . 역시 gamma값이 높아질수록 과적합되어가는 모습을 볼 수 있다. . 낮았을 때는 과소적합 되어있는데 gamma값을 키우는 것으로 해결을 볼 수 있겠다. gamma = 34에서 최적인듯 하다.(데이터 수가 적어서 일반화를 하기엔 아직은 검증이 다소 필요해보인다.) . Gaussian rbf&#50752; C&#47588;&#44060;&#48320;&#49688; . 매개변수 C는 sklearn의 여러 모델들에서 과적합 규제 매개변수로써 쓰인다. 보통 $ alpha$의 역수로 생각할 수 있는데 C가 커질수록 과대적합에 대한 규제가 완화된다고 이해할 수 있다.(모델은 과적합될 가능성이 커진다.) . C = [0.01, 1, 10, 100] fig, axes = plt.subplots(1,4, figsize=(20,4)) for c, ax in zip(C, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=1, C=c) svm.fit(X_train_std, y_train) axes_decision_regions(X_combined_std, y_combined, axes=ax, classifier=svm, test_idx=range(105,150)) ax.set_title(&#39;C : {:}, test_score : {:.2f}&#39;.format(c, svm.score(X_test_std, y_test))) axes[0].legend() plt.show() . C가 커질수록 최적화 해야하는 비용함수에서의 패널티 함수가 커진다. 즉, C는 커질수록 분류 오차에 대한 비용을 키운다. . 주의해야 할 점은 SVM의 패널티 함수는 Ridge나 Lasso와 달리 가중치 벡터 $ mathbf{w}$의 함수가 아니라 상수 C와 분류오차 $ xi^{(i)}$를 곱하여 더한 함수라는 것이다. . 그러므로 C가 커질수록 분류 오차에 대한 비용이 커진다는 것을 모델이 분류오차 $ xi^{(i)}$의 존재에 대해 더욱 민감하게 반응한다고 이해할 수 있다. C값이 크다면 분류오차를 크게 평가하고 배제하려는 쪽으로 과대적합될 것이다.(C값이 작아서 분류오차를 무시한다면 더울 일반화 된 모델이 나올 것이다.) . 따라서 C가 커질수록 훈련 데이터셋에 과대적합이 되어가고 C가 작아질수록 모델이 일반화 된다고 확인할 수 있겠다. . 릿지와 라쏘, 로지스틱 회귀에서 $ alpha$와 C를 통해 규제의 정도를 제어할 수 있었다. SVM에서의 C도 릿지와 라쏘에서늬 $ alpha$와 반비례하고 로지스틱의 C와 동일한 역할을 수행하는 하이퍼파라미터라고 할 수 있다. . &#50836;&#50557; . svm의 SVC에는 기본적인 파라미터 X, y, kernel=&#39;rbf&#39;, gamma=&#39;scale&#39;, C=1,(random_state=None, decision_function_shape=&#39;ovr) 가 있다. 하이퍼파라미터는 kernel, gamma와 C가 있다. . kernel : 커널함수이다. 상위 차원의 매핑함수($ phi$)를 통해 상위 차원에서의 선형 초평면을 구하고 그것을 매핑함수의 역함수($ phi^{-1}$)로 원래 차원으로 가져오는 수고를 줄인 하나의 함수 $K$이다. 머서의 조건만 만족하면 $ phi$를 모르더라도 $K$가 존재하여 커널함수로 사용할 수 있음이 보장된다. 기본값은 rbf(방사 기저 함수 또는 가우시안 커널)이다. | gamma : rbf에서는 gamma가 클수록 가우시안 구의 크기를 제한하는 것으로 이해할 수 있다. 결정경계가 샘플에 가까워지고 구불구불해진다.(클수록 과적합 된다고 이해할 수도 있다.) | C : 분류오차를 얼마나 신경쓸 것인지 조절한다. 클수록 분류오차에 대해 엄격해지고 작을수록 분류오차를 무시한다. 규제 측면에서 릿지와 라쏘에서의 $ alpha$ 와 반비례하고 로지스틱의 C와 동일한 역할을 수행하는 하이퍼파라미터이다. 기본값이 1인 것을 고려하면 SVC는 하드 마진svm이 아닌 소프트 마진 svm이다. 선형 초평면으로 완전히 구분되지 않는 데이터셋에 C=0을 설정하면 에러가 난다. | . &#45908; &#44277;&#48512;&#54644;&#50556; &#54624; &#44163;&#46308; . SVM을 이론적으로 이해하려면 추가로 공부해볼만한 것들을 정리해 보았다. . 최적화 문제 라그랑주 프리멀 함수 | 라그랑주 듀얼 함수 | KKT 조건 | . | SVM 유도(하드 마진) 기본적인 아이디어 | 마진 유도 | 최적화 대상 및 제약 조건 | 라그랑주 프리멀, 듀얼 라그랑주 프리멀 함수 유도 | 프리멀 함수 w, b 편미분 = 0 | 라그랑주 듀얼 함수 유도 | | 듀얼리티 갭 KKT 조건 확인 | | . | SVM 유도(소프트 마진) 분류오차 ξ | 최적화 대상 및 제약 조건 ξ와 C | . | 라그랑주 프리멀, 듀얼 | KKT 조건 확인 | . | 커널 SVM 커널의 필요성 | 프리멀, 듀얼 문제 필요성(핸즈온 223) | 매핑 함수 | 커널 | 커널 SVM 유도 최적화 대상 및 제약 조건 | 라그랑주 프리멀, 듀얼 | KKT 조건 확인 | | 머서의 정리와 조건 | 커널 예시 | . | 추가 온라인 SVM | SVM 회귀 | 쿼드래틱 프로그래밍 | . | .",
            "url": "https://edypidy.github.io/studyblog/jupyter/svm/kernel-svm/classifying/hyper-lane/kernel/gaussian-rbf/2021/12/26/_SVM(%EC%B5%9C%EC%A2%85).html",
            "relUrl": "/jupyter/svm/kernel-svm/classifying/hyper-lane/kernel/gaussian-rbf/2021/12/26/_SVM(%EC%B5%9C%EC%A2%85).html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "[Colab] 코랩 plot 한국어 깨짐 현상 해결",
            "content": "Intro . 코랩의 matplotlib에선 plot의 한글이 깨지는 현상이 있습니다. 가끔은 한글 타이틀이나 ticks를 설정해야 할 때는 정말 불편한데요. . 이번 포스팅에선 코랩의 한글이 깨지는 현상을 해결하는 방법을 알아보겠습니다. 방법은 아주 간단합니다. 2가지 단계를 차례로 진행하시면 됩니다. . 1&#45800;&#44228; : &#45208;&#45588; &#54256;&#53944; &#49444;&#52824;&#54980; &#49892;&#54665; . 나눔 폰트를 설치해줍니다. . !sudo apt-get install -y fonts-nanum !sudo fc-cache -fv !rm ~/.cache/matplotlib -rf . Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: fonts-nanum 0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded. Need to get 9,604 kB of archives. After this operation, 29.5 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fonts-nanum all 20170925-1 [9,604 kB] Fetched 9,604 kB in 1s (6,566 kB/s) debconf: unable to initialize frontend: Dialog debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, &lt;&gt; line 1.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (This frontend requires a controlling tty.) debconf: falling back to frontend: Teletype dpkg-preconfigure: unable to re-open stdin: Selecting previously unselected package fonts-nanum. (Reading database ... 155222 files and directories currently installed.) Preparing to unpack .../fonts-nanum_20170925-1_all.deb ... Unpacking fonts-nanum (20170925-1) ... Setting up fonts-nanum (20170925-1) ... Processing triggers for fontconfig (2.12.6-0ubuntu2) ... /usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs /usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs /usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs /usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs /usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs /usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs /root/.local/share/fonts: skipping, no such directory /root/.fonts: skipping, no such directory /var/cache/fontconfig: cleaning cache directory /root/.cache/fontconfig: not cleaning non-existent cache directory /root/.fontconfig: not cleaning non-existent cache directory fc-cache: succeeded . 이후 아래의 코드를 실행해보면 실망스러운 결과가 나옵니다. . import matplotlib.pyplot as plt import matplotlib.font_manager as fm plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) plt.plot() plt.title(&#39;한국어&#39;) plt.show() . findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44397 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font. font.set_text(s, 0.0, flags=flags) findfont: Font family [&#39;NanumBarunGothic&#39;] not found. Falling back to DejaVu Sans. /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44397 missing from current font. font.set_text(s, 0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font. font.set_text(s, 0, flags=flags) . 2&#45800;&#44228; : &#47088;&#53440;&#51076; &#45796;&#49884;&#49892;&#54665; &#54980; &#49892;&#54665; . 여기서 포기하지 않고 런타일 다시실행을 눌러줍니다. . import matplotlib.pyplot as plt import matplotlib.font_manager as fm plt.rc(&#39;font&#39;, family=&#39;NanumBarunGothic&#39;) plt.plot() plt.title(&#39;한국어&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0.0, flags=flags) /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 8722 missing from current font. font.set_text(s, 0, flags=flags) . warning 메세지가 보기 불편하다면 아래의 코드를 실행해줍니다. . import warnings; warnings.filterwarnings(&#39;ignore&#39;) . plt.plot() plt.title(&#39;한국어&#39;) plt.show() . 너무나 간단히 해결되었습니다. :) .",
            "url": "https://edypidy.github.io/studyblog/code/colab/%ED%95%9C%EA%B5%AD%EC%96%B4/%ED%95%9C%EA%B5%AD%EC%96%B4%20%EA%B9%A8%EC%A7%90/matplotlib/font_manager/2021/12/26/%EC%BD%94%EB%9E%A9_plot_%ED%95%9C%EA%B8%80_%EA%B9%A8%EC%A7%90_%ED%95%B4%EA%B2%B0.html",
            "relUrl": "/code/colab/%ED%95%9C%EA%B5%AD%EC%96%B4/%ED%95%9C%EA%B5%AD%EC%96%B4%20%EA%B9%A8%EC%A7%90/matplotlib/font_manager/2021/12/26/%EC%BD%94%EB%9E%A9_plot_%ED%95%9C%EA%B8%80_%EA%B9%A8%EC%A7%90_%ED%95%B4%EA%B2%B0.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "[Code] 네이버 금융 api를 이용한 금융데이터 크롤링. ",
            "content": "아주 흔하디 흔한 네이버 금융 api를 이용한 금융 데이터 크롤링을 해보겠습니다. . import requests import json import pandas as pd . corp_code = &#39;000020&#39; startTime = 20160104 endTime = 20210730 timeframe = &#39;day&#39; url = f&#39;https://api.finance.naver.com/ siseJson.naver?symbol={corp_code}&amp; requestType=1&amp; startTime={startTime}&amp; endTime={endTime}&amp; timeframe={timeframe}&#39; result = requests.post(url) . url . &#39;https://api.finance.naver.com/siseJson.naver?symbol=000020&amp;requestType=1&amp;startTime=20160104&amp;endTime=20210730&amp;timeframe=day&#39; . 위의 url은 아래와 같습니다. 참고로 는 줄바꿈을 위해 쓴 것이니 . f&#39;https://api.finance.naver.com/siseJson.naver?symbol={corp_code}&amp;requestType=1&amp;startTime={startTime}&amp;endTime={endTime}&amp;timeframe={timeframe}&#39; . 와 같이 써도 됩니다. . result . &lt;Response [200]&gt; . response로 200을 받은 것을 보니 잘 받아온 것 같습니다. . result.text[:500] . &#39; n [[ &#39;날짜 &#39;, &#39;시가 &#39;, &#39;고가 &#39;, &#39;저가 &#39;, &#39;종가 &#39;, &#39;거래량 &#39;, &#39;외국인소진율 &#39;], n n t n t n t t n[&#34;20160104&#34;, 8130, 8150, 7920, 8140, 281440, 7.45], n t t n[&#34;20160105&#34;, 8040, 8250, 8000, 8190, 243179, 7.49], n t t n[&#34;20160106&#34;, 8200, 8590, 8110, 8550, 609906, 7.63], n t t n[&#34;20160107&#34;, 8470, 8690, 8190, 8380, 704752, 7.59], n t t n[&#34;20160108&#34;, 8210, 8900, 8130, 8770, 802330, 7.6], n t t n[&#34;20160111&#34;, 8870, 10100, 8800, 9380, 2844188, 7.25], n t t n[&#34;20160112&#34;, 9560, 9560, 8930, 9020, 834633, 7.05], n t t n[&#34;20160113&#34;, 9190, 9200, 8600, 8780, 885059, 6.89], n t t&#39; . result의 text 속성엔 위와 같이 요청한 데이터가 담겨서 옵니다. . replace 메소드와 strip메소드를 사용해서 아래와 같이 데이터를 정제해줍니다. . result.text.replace(&quot;&#39;&quot;,&#39;&quot;&#39;).strip() . &#39;[[&#34;날짜&#34;, &#34;시가&#34;, &#34;고가&#34;, &#34;저가&#34;, &#34;종가&#34;, &#34;거래량&#34;, &#34;외국인소진율&#34;], n n t n t n t t n[&#34;20160104&#34;, 8130, 8150, 7920, 8140, 281440, 7.45], n t t n[&#34;20160105&#34;, 8040, 8250, 8000, 8190, 243179, 7.49], n t t n[&#34;20160106&#34;, 8200, 8590, 8110, 8550, 609906, 7.63], n t t n[&#34;20160107&#34;, 8470, 8690, 8190, 8380, 704752, 7.59], n t t n[&#34;20160108&#34;, 8210, 8900, 8130, 8770, 802330, 7.6], n t t n[&#34;20160111&#34;, 8870, 10100, 8800, 9380, 2844188, 7.25], n t t n[&#34;20160112&#34;, 9560, 9560, 8930, 9020, 834633, 7.05], n t t n[&#34;20160113&#34;, 9190, 9200, 8600, 8780, 885059, 6.89], n t t n[&#34;20160114&#34;, 8610, 8710, 8420, 8710, 474061, 6.82], n t t n[&#34;20160115&#34;, 8720, 8840, 8450, 8450, 357135, 6.77], n t t n[&#34;20160118&#34;, 8320, 8760, 8300, 8630, 455032, 6.79], n t t n[&#34;20160119&#34;, 8620, 8830, 8520, 8800, 440366, 6.95], n t t n[&#34;20160120&#34;, 8770, 9060, 8400, 8660, 719046, 6.93], n t t n[&#34;20160121&#34;, 8790, 9010, 8630, 8720, 492932, 7.13], n t t n[&#34;20160122&#34;, 8750, 8890, 8490, 8670, 297561, 7.21], n t t n[&#34;20160125&#34;, 8750, 9140, 8670, 8980, 629032, 7.42], n t t n[&#34;20160126&#34;, 8970, 9340, 8910, 9090, 594779, 7.63], n t t n[&#34;20160127&#34;, 9200, 9210, 8660, 8730, 407195, 7.46], n t t n[&#34;20160128&#34;, 8680, 8850, 8540, 8800, 248182, 7.68], n t t n[&#34;20160129&#34;, 8840, 9290, 8750, 9080, 886006, 7.7], n t t n[&#34;20160201&#34;, 9480, 9730, 9340, 9410, 1576796, 7.72], n t t n[&#34;20160202&#34;, 9600, 9680, 9150, 9160, 880752, 7.69], n t t n[&#34;20160203&#34;, 9250, 9940, 9250, 9510, 1611939, 7.69], n t t n[&#34;20160204&#34;, 9630, 9690, 9170, 9210, 695375, 7.64], n t t n[&#34;20160205&#34;, 9240, 9520, 9160, 9510, 618357, 7.88], n t t n[&#34;20160211&#34;, 9070, 9330, 8570, 8610, 667543, 7.89], n t t n[&#34;20160212&#34;, 8310, 8600, 7700, 7710, 933129, 8.02], n t t n[&#34;20160215&#34;, 7950, 8040, 7730, 8030, 358474, 8.01], n t t n[&#34;20160216&#34;, 8030, 8250, 8030, 8040, 256999, 7.93], n t t n[&#34;20160217&#34;, 8170, 8200, 7760, 7860, 428503, 7.85], n t t n[&#34;20160218&#34;, 7980, 7990, 7800, 7870, 327298, 7.67], n t t n[&#34;20160219&#34;, 7880, 7890, 7740, 7820, 277191, 7.65], n t t n[&#34;20160222&#34;, 7820, 8100, 7770, 8080, 312241, 7.72], n t t n[&#34;20160223&#34;, 8120, 8130, 7840, 7850, 280833, 7.52], n t t n[&#34;20160224&#34;, 7850, 7950, 7780, 7810, 150099, 7.55], n t t n[&#34;20160225&#34;, 7860, 7960, 7700, 7760, 216439, 7.7], n t t n[&#34;20160226&#34;, 7870, 7910, 7730, 7900, 156421, 7.78], n t t n[&#34;20160229&#34;, 7900, 7900, 7650, 7700, 271304, 7.71], n t t n[&#34;20160302&#34;, 7750, 7880, 7720, 7880, 191527, 7.89], n t t n[&#34;20160303&#34;, 7920, 7920, 7800, 7890, 163751, 7.91], n t t n[&#34;20160304&#34;, 7880, 8000, 7840, 7900, 180393, 7.93], n t t n[&#34;20160307&#34;, 7950, 7990, 7790, 7810, 211122, 7.87], n t t n[&#34;20160308&#34;, 7800, 7890, 7740, 7810, 137794, 7.89], n t t n[&#34;20160309&#34;, 7820, 8030, 7800, 8000, 232477, 8.02], n t t n[&#34;20160310&#34;, 8000, 8070, 7940, 8000, 133772, 7.97], n t t n[&#34;20160311&#34;, 7980, 8050, 7920, 8010, 92895, 7.97], n t t n[&#34;20160314&#34;, 8050, 8060, 7960, 7980, 115755, 7.92], n t t n[&#34;20160315&#34;, 7980, 8020, 7830, 7830, 115457, 7.88], n t t n[&#34;20160316&#34;, 7830, 7930, 7810, 7860, 59946, 7.9], n t t n[&#34;20160317&#34;, 7910, 8010, 7830, 7950, 147930, 7.85], n t t n[&#34;20160318&#34;, 7950, 8260, 7930, 8260, 317926, 8.16], n t t n[&#34;20160321&#34;, 8300, 8310, 8130, 8200, 123793, 8.22], n t t n[&#34;20160322&#34;, 8200, 8640, 8190, 8480, 684981, 8.33], n t t n[&#34;20160323&#34;, 8500, 8500, 8300, 8380, 153350, 8.32], n t t n[&#34;20160324&#34;, 8350, 8440, 8160, 8300, 110144, 8.32], n t t n[&#34;20160325&#34;, 8260, 8500, 8260, 8370, 112864, 8.38], n t t n[&#34;20160328&#34;, 8440, 8470, 8270, 8310, 96593, 8.39], n t t n[&#34;20160329&#34;, 8310, 8390, 8210, 8360, 127357, 8.47], n t t n[&#34;20160330&#34;, 8370, 8470, 8320, 8340, 110804, 8.42], n t t n[&#34;20160331&#34;, 8370, 8430, 8000, 8110, 354740, 8.45], n t t n[&#34;20160401&#34;, 8150, 8450, 8150, 8390, 330275, 8.51], n t t n[&#34;20160404&#34;, 8530, 8970, 8510, 8900, 881142, 8.79], n t t n[&#34;20160405&#34;, 8940, 8950, 8700, 8900, 323475, 8.83], n t t n[&#34;20160406&#34;, 8900, 9240, 8800, 8950, 447833, 8.78], n t t n[&#34;20160407&#34;, 9060, 9260, 9000, 9120, 352344, 8.89], n t t n[&#34;20160408&#34;, 9050, 9240, 8910, 9060, 221392, 9.01], n t t n[&#34;20160411&#34;, 9120, 9390, 8840, 9160, 623213, 9.09], n t t n[&#34;20160412&#34;, 9080, 9160, 8810, 8810, 410229, 9.1], n t t n[&#34;20160414&#34;, 8830, 8950, 8620, 8840, 218109, 9.25], n t t n[&#34;20160415&#34;, 8870, 9090, 8760, 9020, 202064, 9.32], n t t n[&#34;20160418&#34;, 9070, 9100, 8850, 8910, 158796, 9.29], n t t n[&#34;20160419&#34;, 8940, 9050, 8870, 9010, 139229, 9.32], n t t n[&#34;20160420&#34;, 9070, 9210, 8930, 8960, 232922, 9.21], n t t n[&#34;20160421&#34;, 9020, 9050, 8890, 8930, 136782, 9.24], n t t n[&#34;20160422&#34;, 9000, 9040, 8890, 8930, 98225, 9.09], n t t n[&#34;20160425&#34;, 8950, 9630, 8950, 9630, 907794, 9.13], n t t n[&#34;20160426&#34;, 9600, 9900, 9500, 9530, 797621, 8.9], n t t n[&#34;20160427&#34;, 9440, 9550, 9130, 9160, 440243, 8.66], n t t n[&#34;20160428&#34;, 9270, 9310, 9090, 9130, 219534, 8.58], n t t n[&#34;20160429&#34;, 9110, 9330, 9050, 9330, 210588, 8.53], n t t n[&#34;20160502&#34;, 9360, 9380, 9090, 9090, 254265, 8.36], n t t n[&#34;20160503&#34;, 9090, 9300, 9000, 9050, 132892, 8.37], n t t n[&#34;20160504&#34;, 8970, 9030, 8660, 8700, 362666, 8.4], n t t n[&#34;20160509&#34;, 8700, 9240, 8700, 9120, 271636, 8.49], n t t n[&#34;20160510&#34;, 9130, 9640, 9120, 9310, 605125, 8.56], n t t n[&#34;20160511&#34;, 9400, 9660, 9370, 9430, 422692, 8.58], n t t n[&#34;20160512&#34;, 9440, 9770, 9350, 9700, 371307, 8.71], n t t n[&#34;20160513&#34;, 9710, 9740, 9460, 9490, 202275, 8.69], n t t n[&#34;20160516&#34;, 9470, 10150, 9470, 10100, 980264, 8.7], n t t n[&#34;20160517&#34;, 10250, 10300, 9920, 10050, 533964, 8.64], n t t n[&#34;20160518&#34;, 10100, 11700, 10050, 10650, 2574162, 8.51], n t t n[&#34;20160519&#34;, 10550, 11100, 10100, 10750, 1410629, 8.51], n t t n[&#34;20160520&#34;, 10800, 11400, 10650, 10800, 753916, 8.5], n t t n[&#34;20160523&#34;, 11100, 11400, 10850, 11100, 678196, 8.49], n t t n[&#34;20160524&#34;, 11200, 11950, 10950, 11550, 1199632, 8.22], n t t n[&#34;20160525&#34;, 11600, 11750, 10050, 10200, 1549992, 8.11], n t t n[&#34;20160526&#34;, 10300, 10450, 10000, 10400, 522664, 8.38], n t t n[&#34;20160527&#34;, 10450, 10500, 10150, 10250, 247091, 8.48], n t t n[&#34;20160530&#34;, 10300, 10500, 9980, 10400, 254573, 8.59], n t t n[&#34;20160531&#34;, 10450, 10550, 10200, 10500, 242286, 8.78], n t t n[&#34;20160601&#34;, 10500, 10600, 10350, 10400, 322649, 8.83], n t t n[&#34;20160602&#34;, 10500, 11000, 10400, 10900, 449419, 9.05], n t t n[&#34;20160603&#34;, 11000, 11000, 10600, 10650, 337465, 8.94], n t t n[&#34;20160607&#34;, 10700, 10750, 10450, 10700, 181624, 9.09], n t t n[&#34;20160608&#34;, 10800, 11100, 10700, 10900, 427260, 8.9], n t t n[&#34;20160609&#34;, 10850, 11300, 10400, 10600, 549073, 8.87], n t t n[&#34;20160610&#34;, 10700, 10750, 10400, 10650, 308504, 9.02], n t t n[&#34;20160613&#34;, 10700, 10750, 9980, 10150, 573462, 9.01], n t t n[&#34;20160614&#34;, 10050, 10550, 9900, 10350, 330124, 9.3], n t t n[&#34;20160615&#34;, 10350, 10550, 10250, 10400, 144971, 9.4], n t t n[&#34;20160616&#34;, 10400, 11250, 10300, 10500, 991321, 9.34], n t t n[&#34;20160617&#34;, 10750, 11050, 10000, 10150, 910307, 8.98], n t t n[&#34;20160620&#34;, 10300, 10300, 10000, 10200, 255159, 8.95], n t t n[&#34;20160621&#34;, 10250, 10250, 9990, 10100, 278627, 9.06], n t t n[&#34;20160622&#34;, 10150, 10150, 9780, 9830, 315346, 8.99], n t t n[&#34;20160623&#34;, 9710, 9870, 9510, 9730, 293348, 9.14], n t t n[&#34;20160624&#34;, 9840, 9910, 8700, 9080, 621895, 9.3], n t t n[&#34;20160627&#34;, 8750, 9480, 8750, 9400, 334886, 9.57], n t t n[&#34;20160628&#34;, 9210, 9770, 9210, 9760, 282254, 9.64], n t t n[&#34;20160629&#34;, 9850, 10100, 9700, 9750, 352292, 9.42], n t t n[&#34;20160630&#34;, 9850, 10400, 9760, 10100, 466039, 9.42], n t t n[&#34;20160701&#34;, 10200, 10200, 9960, 9960, 208228, 9.44], n t t n[&#34;20160704&#34;, 10000, 10400, 9900, 10400, 275210, 9.42], n t t n[&#34;20160705&#34;, 10400, 10450, 10200, 10350, 156010, 9.32], n t t n[&#34;20160706&#34;, 10250, 10400, 9990, 10100, 198457, 9.35], n t t n[&#34;20160707&#34;, 10300, 11100, 10200, 10850, 1346083, 9.69], n t t n[&#34;20160708&#34;, 11000, 11050, 10500, 10600, 409317, 9.54], n t t n[&#34;20160711&#34;, 10750, 10850, 10550, 10650, 245693, 9.49], n t t n[&#34;20160712&#34;, 10650, 10750, 10350, 10550, 213586, 9.5], n t t n[&#34;20160713&#34;, 10750, 10850, 10400, 10450, 227423, 9.5], n t t n[&#34;20160714&#34;, 10500, 10500, 10150, 10300, 153051, 9.45], n t t n[&#34;20160715&#34;, 10400, 10400, 10000, 10100, 240215, 9.44], n t t n[&#34;20160718&#34;, 10150, 10200, 9970, 10100, 169020, 9.47], n t t n[&#34;20160719&#34;, 10150, 10200, 10000, 10050, 110304, 9.43], n t t n[&#34;20160720&#34;, 10050, 10200, 9970, 9970, 216117, 9.41], n t t n[&#34;20160721&#34;, 10050, 10100, 9850, 9870, 224318, 9.42], n t t n[&#34;20160722&#34;, 9950, 10100, 9850, 9990, 156090, 9.42], n t t n[&#34;20160725&#34;, 10050, 10250, 9940, 9990, 185413, 9.3], n t t n[&#34;20160726&#34;, 10050, 10200, 9980, 10200, 139615, 9.28], n t t n[&#34;20160727&#34;, 10200, 10250, 9950, 9970, 174237, 9.13], n t t n[&#34;20160728&#34;, 10000, 10050, 9890, 9920, 132099, 9.1], n t t n[&#34;20160729&#34;, 9950, 10100, 9880, 9920, 93682, 9.09], n t t n[&#34;20160801&#34;, 9990, 9990, 9560, 9590, 236931, 8.93], n t t n[&#34;20160802&#34;, 9520, 9800, 9520, 9610, 95520, 8.98], n t t n[&#34;20160803&#34;, 9560, 9660, 9020, 9570, 124763, 9.04], n t t n[&#34;20160804&#34;, 9650, 9650, 9500, 9510, 76492, 9.08], n t t n[&#34;20160805&#34;, 9520, 9610, 9390, 9420, 165691, 9.07], n t t n[&#34;20160808&#34;, 9490, 9490, 9310, 9320, 101265, 9.11], n t t n[&#34;20160809&#34;, 9330, 9700, 9300, 9600, 163792, 9.12], n t t n[&#34;20160810&#34;, 9650, 9850, 9500, 9820, 104217, 9.15], n t t n[&#34;20160811&#34;, 9800, 9910, 9660, 9660, 93000, 9.1], n t t n[&#34;20160812&#34;, 9710, 9780, 9620, 9680, 100669, 9.11], n t t n[&#34;20160816&#34;, 9630, 9780, 9560, 9560, 101567, 9.06], n t t n[&#34;20160817&#34;, 9570, 9630, 9260, 9410, 83871, 9.08], n t t n[&#34;20160818&#34;, 9360, 9520, 9300, 9330, 86979, 9.05], n t t n[&#34;20160819&#34;, 9400, 9400, 9220, 9290, 90527, 9.02], n t t n[&#34;20160822&#34;, 9300, 9470, 9090, 9090, 107968, 9.0], n t t n[&#34;20160823&#34;, 9250, 9360, 9090, 9260, 97880, 9.02], n t t n[&#34;20160824&#34;, 9280, 9310, 9110, 9130, 52746, 8.96], n t t n[&#34;20160825&#34;, 9150, 9220, 9010, 9070, 138100, 8.87], n t t n[&#34;20160826&#34;, 9030, 9120, 8940, 9000, 112520, 8.84], n t t n[&#34;20160829&#34;, 9200, 9600, 9070, 9150, 275947, 8.68], n t t n[&#34;20160830&#34;, 9240, 9320, 9070, 9090, 130098, 8.57], n t t n[&#34;20160831&#34;, 9100, 9150, 8670, 8830, 344837, 8.54], n t t n[&#34;20160901&#34;, 8800, 8950, 8750, 8830, 81059, 8.5], n t t n[&#34;20160902&#34;, 8900, 9150, 8800, 9010, 157090, 8.54], n t t n[&#34;20160905&#34;, 9030, 9530, 9030, 9530, 367185, 8.74], n t t n[&#34;20160906&#34;, 9470, 9500, 9340, 9370, 152344, 8.7], n t t n[&#34;20160907&#34;, 9350, 9420, 9180, 9230, 131412, 8.67], n t t n[&#34;20160908&#34;, 9180, 9340, 9080, 9230, 150905, 8.56], n t t n[&#34;20160909&#34;, 9160, 9480, 9120, 9170, 250696, 8.55], n t t n[&#34;20160912&#34;, 8950, 9060, 8850, 8920, 175069, 8.5], n t t n[&#34;20160913&#34;, 8950, 9020, 8850, 8900, 59138, 8.47], n t t n[&#34;20160919&#34;, 8900, 8920, 8700, 8820, 102520, 8.46], n t t n[&#34;20160920&#34;, 8760, 9210, 8760, 9200, 134779, 8.54], n t t n[&#34;20160921&#34;, 9200, 9200, 9000, 9070, 65481, 8.49], n t t n[&#34;20160922&#34;, 9100, 9160, 8980, 9150, 91236, 8.5], n t t n[&#34;20160923&#34;, 9150, 9490, 9110, 9400, 205561, 8.6], n t t n[&#34;20160926&#34;, 9360, 9570, 9190, 9250, 183706, 8.59], n t t n[&#34;20160927&#34;, 9250, 9460, 9240, 9430, 100238, 8.63], n t t n[&#34;20160928&#34;, 9430, 9430, 9300, 9390, 69984, 8.61], n t t n[&#34;20160929&#34;, 9360, 9440, 9350, 9350, 41651, 8.59], n t t n[&#34;20160930&#34;, 9320, 9380, 8960, 8980, 169214, 8.49], n t t n[&#34;20161004&#34;, 8900, 9060, 8730, 8930, 182860, 8.49], n t t n[&#34;20161005&#34;, 9040, 9040, 8850, 8880, 69602, 8.46], n t t n[&#34;20161006&#34;, 8920, 8920, 8790, 8870, 101800, 8.49], n t t n[&#34;20161007&#34;, 8920, 8930, 8600, 8600, 165036, 8.41], n t t n[&#34;20161010&#34;, 8630, 8780, 8630, 8710, 60178, 8.45], n t t n[&#34;20161011&#34;, 8740, 8770, 8560, 8660, 123203, 8.38], n t t n[&#34;20161012&#34;, 8630, 8730, 8600, 8670, 40138, 8.34], n t t n[&#34;20161013&#34;, 8670, 8780, 8550, 8560, 66359, 8.3], n t t n[&#34;20161014&#34;, 8550, 8630, 8530, 8600, 37196, 8.27], n t t n[&#34;20161017&#34;, 8630, 8630, 8400, 8400, 93901, 8.23], n t t n[&#34;20161018&#34;, 8370, 8440, 8320, 8410, 66735, 8.21], n t t n[&#34;20161019&#34;, 8440, 8700, 8360, 8590, 115902, 8.22], n t t n[&#34;20161020&#34;, 8650, 8650, 8410, 8460, 69382, 8.14], n t t n[&#34;20161021&#34;, 8460, 8480, 8400, 8420, 31170, 8.13], n t t n[&#34;20161024&#34;, 8380, 8550, 8360, 8390, 47490, 8.11], n t t n[&#34;20161025&#34;, 8390, 8450, 8100, 8100, 80944, 8.08], n t t n[&#34;20161026&#34;, 8110, 8280, 7930, 8060, 75784, 8.09], n t t n[&#34;20161027&#34;, 8070, 8270, 8070, 8240, 49274, 8.11], n t t n[&#34;20161028&#34;, 8240, 8260, 7950, 7980, 103493, 8.02], n t t n[&#34;20161031&#34;, 7940, 7970, 7680, 7780, 81252, 7.99], n t t n[&#34;20161101&#34;, 7640, 7930, 7640, 7770, 86241, 8.02], n t t n[&#34;20161102&#34;, 7770, 7780, 7380, 7380, 79911, 8.01], n t t n[&#34;20161103&#34;, 7300, 7680, 7290, 7540, 59346, 8.09], n t t n[&#34;20161104&#34;, 7510, 7630, 7410, 7580, 40348, 8.08], n t t n[&#34;20161107&#34;, 7580, 7670, 7520, 7600, 71489, 8.0], n t t n[&#34;20161108&#34;, 7600, 7740, 7510, 7680, 30006, 8.0], n t t n[&#34;20161109&#34;, 7630, 7750, 7060, 7280, 218802, 8.01], n t t n[&#34;20161110&#34;, 7500, 7750, 7430, 7740, 185896, 8.12], n t t n[&#34;20161111&#34;, 7710, 7980, 7650, 7900, 64657, 8.12], n t t n[&#34;20161114&#34;, 7900, 7990, 7860, 7920, 54980, 8.08], n t t n[&#34;20161115&#34;, 7990, 8080, 7770, 7810, 66070, 8.03], n t t n[&#34;20161116&#34;, 7980, 8090, 7880, 8090, 70633, 8.05], n t t n[&#34;20161117&#34;, 8090, 8130, 8000, 8130, 36337, 8.03], n t t n[&#34;20161118&#34;, 8190, 8250, 8040, 8140, 64014, 8.0], n t t n[&#34;20161121&#34;, 8150, 8150, 7850, 7990, 52494, 7.99], n t t n[&#34;20161122&#34;, 8000, 8050, 7940, 7980, 41245, 7.98], n t t n[&#34;20161123&#34;, 7950, 8040, 7910, 7930, 58658, 7.97], n t t n[&#34;20161124&#34;, 7880, 7970, 7830, 7840, 51800, 7.94], n t t n[&#34;20161125&#34;, 7840, 8010, 7840, 7980, 23612, 7.95], n t t n[&#34;20161128&#34;, 8030, 8030, 7780, 7890, 43457, 7.9], n t t n[&#34;20161129&#34;, 7830, 7910, 7740, 7810, 49669, 7.85], n t t n[&#34;20161130&#34;, 7820, 7880, 7700, 7720, 38626, 7.88], n t t n[&#34;20161201&#34;, 7720, 7760, 7600, 7680, 75616, 7.87], n t t n[&#34;20161202&#34;, 7620, 7760, 7600, 7610, 81503, 7.8], n t t n[&#34;20161205&#34;, 7610, 7680, 7510, 7560, 53176, 7.82], n t t n[&#34;20161206&#34;, 7550, 7670, 7510, 7590, 46157, 7.79], n t t n[&#34;20161207&#34;, 7560, 7640, 7390, 7470, 65348, 7.76], n t t n[&#34;20161208&#34;, 7620, 7620, 7450, 7480, 37690, 7.76], n t t n[&#34;20161209&#34;, 7530, 7580, 7460, 7540, 39179, 7.72], n t t n[&#34;20161212&#34;, 7570, 7810, 7540, 7740, 54245, 7.73], n t t n[&#34;20161213&#34;, 7780, 7890, 7710, 7820, 125368, 7.69], n t t n[&#34;20161214&#34;, 7880, 7970, 7730, 7900, 93743, 7.69], n t t n[&#34;20161215&#34;, 7810, 7940, 7810, 7940, 54720, 7.68], n t t n[&#34;20161216&#34;, 7940, 8040, 7820, 8000, 60489, 7.68], n t t n[&#34;20161219&#34;, 8000, 8020, 7880, 7980, 29259, 7.66], n t t n[&#34;20161220&#34;, 7980, 8000, 7830, 7920, 25678, 7.65], n t t n[&#34;20161221&#34;, 7880, 7960, 7830, 7900, 34333, 7.65], n t t n[&#34;20161222&#34;, 7900, 7910, 7740, 7800, 49362, 7.59], n t t n[&#34;20161223&#34;, 7800, 7940, 7760, 7830, 31948, 7.46], n t t n[&#34;20161226&#34;, 7840, 7950, 7840, 7930, 41707, 7.44], n t t n[&#34;20161227&#34;, 7930, 7980, 7880, 7900, 78257, 7.43], n t t n[&#34;20161228&#34;, 7950, 8140, 7850, 8130, 99508, 7.44], n t t n[&#34;20161229&#34;, 8000, 8130, 7990, 8120, 48540, 7.46], n t t n[&#34;20170102&#34;, 8160, 8170, 8060, 8110, 60571, 7.41], n t t n[&#34;20170103&#34;, 8110, 8170, 8090, 8110, 32249, 7.49], n t t n[&#34;20170104&#34;, 8120, 8190, 8070, 8150, 74448, 7.49], n t t n[&#34;20170105&#34;, 8170, 8250, 8140, 8210, 52130, 7.5], n t t n[&#34;20170106&#34;, 8300, 8300, 8140, 8160, 48364, 7.46], n t t n[&#34;20170109&#34;, 8210, 8210, 8010, 8080, 69546, 7.41], n t t n[&#34;20170110&#34;, 8010, 8080, 7930, 7950, 96568, 7.25], n t t n[&#34;20170111&#34;, 7950, 8040, 7910, 7910, 53542, 7.18], n t t n[&#34;20170112&#34;, 7890, 7930, 7790, 7850, 75632, 7.16], n t t n[&#34;20170113&#34;, 7810, 7970, 7810, 7900, 52826, 7.16], n t t n[&#34;20170116&#34;, 7910, 8060, 7810, 7850, 41817, 7.12], n t t n[&#34;20170117&#34;, 7890, 8170, 7810, 8170, 216637, 7.03], n t t n[&#34;20170118&#34;, 8200, 8210, 7990, 8070, 139534, 6.96], n t t n[&#34;20170119&#34;, 8050, 8080, 7920, 7920, 86675, 6.88], n t t n[&#34;20170120&#34;, 7970, 8020, 7890, 7950, 44345, 6.87], n t t n[&#34;20170123&#34;, 8020, 8400, 7990, 8350, 288406, 6.83], n t t n[&#34;20170124&#34;, 8370, 8450, 8260, 8330, 159507, 6.72], n t t n[&#34;20170125&#34;, 8320, 8350, 8150, 8180, 162192, 6.58], n t t n[&#34;20170126&#34;, 8180, 8440, 8170, 8340, 131628, 6.62], n t t n[&#34;20170131&#34;, 8400, 8400, 8150, 8210, 86947, 6.56], n t t n[&#34;20170201&#34;, 8200, 8310, 8130, 8260, 77835, 6.58], n t t n[&#34;20170202&#34;, 8260, 8320, 8100, 8120, 96992, 6.54], n t t n[&#34;20170203&#34;, 8120, 8200, 8090, 8170, 62798, 6.51], n t t n[&#34;20170206&#34;, 8260, 8700, 8260, 8610, 344191, 6.63], n t t n[&#34;20170207&#34;, 8650, 8660, 8470, 8480, 178167, 6.64], n t t n[&#34;20170208&#34;, 8500, 8640, 8400, 8440, 152251, 6.62], n t t n[&#34;20170209&#34;, 8400, 8460, 8290, 8460, 81039, 6.62], n t t n[&#34;20170210&#34;, 8460, 8500, 8300, 8360, 107829, 6.59], n t t n[&#34;20170213&#34;, 8330, 8440, 8280, 8300, 137746, 6.65], n t t n[&#34;20170214&#34;, 8320, 8420, 8300, 8350, 81747, 6.68], n t t n[&#34;20170215&#34;, 8340, 8650, 8330, 8550, 162442, 6.76], n t t n[&#34;20170216&#34;, 8570, 8580, 8420, 8420, 77437, 6.71], n t t n[&#34;20170217&#34;, 8420, 8890, 8380, 8620, 321248, 6.69], n t t n[&#34;20170220&#34;, 8690, 8700, 8460, 8510, 147880, 6.66], n t t n[&#34;20170221&#34;, 8520, 8650, 8490, 8530, 93793, 6.68], n t t n[&#34;20170222&#34;, 8560, 8710, 8550, 8650, 115668, 6.8], n t t n[&#34;20170223&#34;, 8690, 8710, 8500, 8580, 97152, 6.8], n t t n[&#34;20170224&#34;, 8620, 8820, 8600, 8690, 120593, 6.93], n t t n[&#34;20170227&#34;, 8730, 8980, 8730, 8930, 210604, 7.05], n t t n[&#34;20170228&#34;, 9000, 9140, 8750, 8900, 316121, 6.95], n t t n[&#34;20170302&#34;, 8900, 8930, 8800, 8880, 127835, 7.02], n t t n[&#34;20170303&#34;, 8850, 8870, 8690, 8710, 127950, 7.13], n t t n[&#34;20170306&#34;, 8710, 9200, 8670, 9160, 448429, 7.52], n t t n[&#34;20170307&#34;, 9160, 9160, 8960, 9020, 174775, 7.47], n t t n[&#34;20170308&#34;, 9020, 9150, 9000, 9040, 183502, 7.53], n t t n[&#34;20170309&#34;, 9060, 9100, 9000, 9000, 184408, 7.59], n t t n[&#34;20170310&#34;, 9000, 9070, 8850, 9040, 270497, 7.63], n t t n[&#34;20170313&#34;, 9050, 9080, 8940, 8960, 63329, 7.61], n t t n[&#34;20170314&#34;, 8960, 9000, 8800, 8820, 106237, 7.58], n t t n[&#34;20170315&#34;, 8810, 8980, 8810, 8960, 126305, 7.7], n t t n[&#34;20170316&#34;, 8970, 9060, 8900, 9060, 92390, 7.79], n t t n[&#34;20170317&#34;, 9070, 9100, 8880, 8970, 92558, 7.8], n t t n[&#34;20170320&#34;, 8970, 8970, 8870, 8900, 54443, 7.82], n t t n[&#34;20170321&#34;, 8960, 9010, 8850, 8870, 173728, 7.83], n t t n[&#34;20170322&#34;, 8880, 9120, 8880, 9010, 109599, 7.91], n t t n[&#34;20170323&#34;, 8990, 9070, 8900, 8970, 74038, 7.89], n t t n[&#34;20170324&#34;, 8960, 8960, 8690, 8750, 162931, 7.92], n t t n[&#34;20170327&#34;, 8770, 9070, 8750, 9060, 208086, 8.2], n t t n[&#34;20170328&#34;, 9030, 9180, 9000, 9130, 174253, 8.36], n t t n[&#34;20170329&#34;, 9130, 9190, 9020, 9170, 109368, 8.36], n t t n[&#34;20170330&#34;, 9190, 9190, 9070, 9160, 75552, 8.35], n t t n[&#34;20170331&#34;, 9160, 9530, 9070, 9090, 442169, 8.56], n t t n[&#34;20170403&#34;, 9130, 9380, 9130, 9370, 183606, 8.72], n t t n[&#34;20170404&#34;, 9410, 9420, 9200, 9280, 107813, 8.66], n t t n[&#34;20170405&#34;, 9280, 9440, 9280, 9350, 138059, 8.75], n t t n[&#34;20170406&#34;, 9350, 9350, 9170, 9230, 77091, 8.76], n t t n[&#34;20170407&#34;, 9260, 9500, 9260, 9400, 139263, 8.98], n t t n[&#34;20170410&#34;, 9440, 9480, 9100, 9220, 174553, 8.95], n t t n[&#34;20170411&#34;, 9240, 9450, 9090, 9450, 155082, 8.96], n t t n[&#34;20170412&#34;, 9430, 9550, 9340, 9380, 106840, 8.94], n t t n[&#34;20170413&#34;, 9380, 9500, 9360, 9420, 90904, 8.99], n t t n[&#34;20170414&#34;, 9430, 9500, 9220, 9230, 84913, 8.96], n t t n[&#34;20170417&#34;, 9290, 9400, 9230, 9390, 48955, 8.97], n t t n[&#34;20170418&#34;, 9340, 9470, 9330, 9470, 78044, 9.03], n t t n[&#34;20170419&#34;, 9470, 9490, 9400, 9470, 87039, 9.16], n t t n[&#34;20170420&#34;, 9480, 9510, 9400, 9430, 73407, 9.2], n t t n[&#34;20170421&#34;, 9430, 9470, 9300, 9370, 66448, 9.22], n t t n[&#34;20170424&#34;, 9410, 9410, 9130, 9170, 101982, 9.23], n t t n[&#34;20170425&#34;, 9180, 9260, 9180, 9240, 47317, 9.26], n t t n[&#34;20170426&#34;, 9250, 9300, 9180, 9210, 62915, 9.26], n t t n[&#34;20170427&#34;, 9200, 9270, 9160, 9220, 54630, 9.27], n t t n[&#34;20170428&#34;, 9280, 9280, 9180, 9200, 59476, 9.33], n t t n[&#34;20170502&#34;, 9210, 9260, 9120, 9120, 57006, 9.35], n t t n[&#34;20170504&#34;, 9090, 9300, 9090, 9290, 96510, 9.48], n t t n[&#34;20170508&#34;, 9300, 9490, 9300, 9440, 189911, 9.63], n t t n[&#34;20170510&#34;, 9440, 9460, 9200, 9210, 120807, 9.6], n t t n[&#34;20170511&#34;, 9250, 9320, 9190, 9300, 76777, 9.69], n t t n[&#34;20170512&#34;, 9300, 9300, 9120, 9150, 74236, 9.63], n t t n[&#34;20170515&#34;, 9240, 9240, 9000, 9050, 111684, 9.67], n t t n[&#34;20170516&#34;, 9100, 9100, 8730, 8800, 161039, 9.63], n t t n[&#34;20170517&#34;, 8800, 8920, 8740, 8790, 76934, 9.63], n t t n[&#34;20170518&#34;, 8770, 8770, 8600, 8650, 117171, 9.64], n t t n[&#34;20170519&#34;, 8620, 8820, 8620, 8790, 60575, 9.65], n t t n[&#34;20170522&#34;, 8790, 8790, 8680, 8690, 71350, 9.63], n t t n[&#34;20170523&#34;, 8740, 8740, 8660, 8700, 63183, 9.6], n t t n[&#34;20170524&#34;, 8780, 8880, 8780, 8830, 56837, 9.6], n t t n[&#34;20170525&#34;, 8820, 8850, 8790, 8810, 69759, 9.58], n t t n[&#34;20170526&#34;, 8810, 8820, 8700, 8780, 83211, 9.55], n t t n[&#34;20170529&#34;, 8810, 8830, 8750, 8760, 53923, 9.53], n t t n[&#34;20170530&#34;, 8760, 8910, 8760, 8850, 70891, 9.53], n t t n[&#34;20170531&#34;, 8830, 9090, 8830, 8990, 150658, 9.55], n t t n[&#34;20170601&#34;, 9010, 9040, 8840, 8940, 89610, 9.45], n t t n[&#34;20170602&#34;, 8940, 9020, 8870, 9000, 90581, 9.42], n t t n[&#34;20170605&#34;, 8990, 9240, 8970, 9140, 177175, 9.51], n t t n[&#34;20170607&#34;, 9150, 9160, 8990, 9090, 108208, 9.43], n t t n[&#34;20170608&#34;, 9080, 9210, 8970, 9130, 255999, 9.36], n t t n[&#34;20170609&#34;, 9140, 9180, 9000, 9050, 124202, 9.33], n t t n[&#34;20170612&#34;, 9020, 9080, 8870, 8870, 72203, 9.3], n t t n[&#34;20170613&#34;, 8930, 9090, 8900, 9040, 138036, 9.32], n t t n[&#34;20170614&#34;, 9080, 9080, 8960, 8960, 97462, 9.28], n t t n[&#34;20170615&#34;, 8970, 8980, 8770, 8810, 173542, 9.15], n t t n[&#34;20170616&#34;, 8840, 8840, 8590, 8680, 158720, 9.05], n t t n[&#34;20170619&#34;, 8690, 8720, 8630, 8680, 63186, 9.02], n t t n[&#34;20170620&#34;, 8700, 8700, 8360, 8450, 213907, 8.81], n t t n[&#34;20170621&#34;, 8450, 8600, 8410, 8600, 95160, 8.84], n t t n[&#34;20170622&#34;, 8600, 8630, 8520, 8580, 70449, 8.77], n t t n[&#34;20170623&#34;, 8550, 8660, 8540, 8630, 110692, 8.76], n t t n[&#34;20170626&#34;, 8610, 8630, 8520, 8590, 38738, 8.74], n t t n[&#34;20170627&#34;, 8590, 8590, 8470, 8490, 64198, 8.72], n t t n[&#34;20170628&#34;, 8480, 8480, 8350, 8350, 65937, 8.7], n t t n[&#34;20170629&#34;, 8340, 8440, 8330, 8410, 47668, 8.71], n t t n[&#34;20170630&#34;, 8450, 8540, 8410, 8470, 48558, 8.73], n t t n[&#34;20170703&#34;, 8470, 8510, 8410, 8440, 25073, 8.72], n t t n[&#34;20170704&#34;, 8450, 8490, 8370, 8430, 44152, 8.7], n t t n[&#34;20170705&#34;, 8430, 8440, 8300, 8340, 62841, 8.68], n t t n[&#34;20170706&#34;, 8340, 8420, 8300, 8400, 48957, 8.66], n t t n[&#34;20170707&#34;, 8370, 8370, 8190, 8270, 91431, 8.58], n t t n[&#34;20170710&#34;, 8320, 8320, 8100, 8100, 83141, 8.54], n t t n[&#34;20170711&#34;, 8090, 8180, 8040, 8180, 78992, 8.56], n t t n[&#34;20170712&#34;, 8150, 8180, 8030, 8140, 72867, 8.48], n t t n[&#34;20170713&#34;, 8120, 8120, 7950, 8040, 117249, 8.4], n t t n[&#34;20170714&#34;, 8040, 8040, 7850, 7850, 89967, 8.31], n t t n[&#34;20170717&#34;, 7860, 8040, 7850, 8000, 103717, 8.37], n t t n[&#34;20170718&#34;, 8020, 8050, 7940, 7970, 54824, 8.31], n t t n[&#34;20170719&#34;, 7970, 8030, 7930, 8030, 59664, 8.29], n t t n[&#34;20170720&#34;, 8030, 8430, 7970, 8430, 250698, 8.26], n t t n[&#34;20170721&#34;, 8420, 8500, 8350, 8430, 178865, 8.16], n t t n[&#34;20170724&#34;, 8470, 8470, 8220, 8240, 123207, 8.07], n t t n[&#34;20170725&#34;, 8280, 8430, 8220, 8280, 95716, 8.03], n t t n[&#34;20170726&#34;, 8340, 8410, 8290, 8300, 70591, 8.01], n t t n[&#34;20170727&#34;, 8320, 8350, 8260, 8350, 37499, 7.99], n t t n[&#34;20170728&#34;, 8360, 8360, 8160, 8180, 89526, 7.96], n t t n[&#34;20170731&#34;, 8100, 8270, 8100, 8180, 40980, 7.95], n t t n[&#34;20170801&#34;, 8180, 8260, 8110, 8200, 59629, 7.93], n t t n[&#34;20170802&#34;, 8200, 8470, 8200, 8380, 145402, 7.98], n t t n[&#34;20170803&#34;, 8450, 8460, 8250, 8300, 81858, 7.9], n t t n[&#34;20170804&#34;, 8380, 8380, 8210, 8330, 74052, 7.85], n t t n[&#34;20170807&#34;, 8330, 8410, 8320, 8380, 35043, 7.84], n t t n[&#34;20170808&#34;, 8380, 8380, 8290, 8330, 28242, 7.82], n t t n[&#34;20170809&#34;, 8330, 8330, 8150, 8150, 93844, 7.76], n t t n[&#34;20170810&#34;, 8150, 8280, 8120, 8200, 54760, 7.77], n t t n[&#34;20170811&#34;, 8150, 8250, 7980, 8130, 128133, 7.73], n t t n[&#34;20170814&#34;, 8240, 8540, 8200, 8500, 193156, 7.85], n t t n[&#34;20170816&#34;, 8560, 8620, 8470, 8580, 121847, 7.87], n t t n[&#34;20170817&#34;, 8550, 8660, 8530, 8610, 83360, 7.87], n t t n[&#34;20170818&#34;, 8530, 8720, 8530, 8710, 115578, 7.84], n t t n[&#34;20170821&#34;, 8740, 8920, 8670, 8730, 105319, 7.83], n t t n[&#34;20170822&#34;, 8840, 8920, 8740, 8920, 151222, 7.79], n t t n[&#34;20170823&#34;, 8940, 9080, 8910, 8990, 331724, 7.61], n t t n[&#34;20170824&#34;, 8980, 8980, 8740, 8820, 173195, 7.58], n t t n[&#34;20170825&#34;, 8880, 8880, 8770, 8840, 62896, 7.61], n t t n[&#34;20170828&#34;, 8840, 8980, 8840, 8910, 91634, 7.69], n t t n[&#34;20170829&#34;, 8920, 9020, 8750, 8960, 138307, 7.69], n t t n[&#34;20170830&#34;, 9000, 9010, 8900, 9000, 47839, 7.75], n t t n[&#34;20170831&#34;, 9000, 9100, 9000, 9050, 122738, 7.84], n t t n[&#34;20170901&#34;, 9100, 9100, 9020, 9040, 66270, 7.86], n t t n[&#34;20170904&#34;, 8850, 8990, 8770, 8800, 111207, 7.92], n t t n[&#34;20170905&#34;, 8810, 8850, 8670, 8710, 108670, 7.96], n t t n[&#34;20170906&#34;, 8670, 8840, 8590, 8600, 129728, 8.02], n t t n[&#34;20170907&#34;, 8710, 8880, 8710, 8870, 87084, 8.1], n t t n[&#34;20170908&#34;, 8890, 8900, 8710, 8710, 87628, 8.06], n t t n[&#34;20170911&#34;, 8760, 9090, 8730, 8940, 141043, 8.03], n t t n[&#34;20170912&#34;, 8950, 8990, 8850, 8910, 66985, 8.0], n t t n[&#34;20170913&#34;, 8920, 9100, 8920, 9020, 119189, 8.06], n t t n[&#34;20170914&#34;, 9030, 9070, 8810, 9070, 173649, 8.22], n t t n[&#34;20170915&#34;, 9060, 9070, 8930, 9020, 32080, 8.2], n t t n[&#34;20170918&#34;, 9020, 9020, 8850, 8850, 53061, 8.2], n t t n[&#34;20170919&#34;, 8840, 8980, 8770, 8850, 42998, 8.22], n t t n[&#34;20170920&#34;, 8870, 9080, 8840, 8950, 98136, 8.26], n t t n[&#34;20170921&#34;, 8950, 9000, 8850, 8870, 69115, 8.22], n t t n[&#34;20170922&#34;, 8840, 8950, 8560, 8640, 135843, 8.22], n t t n[&#34;20170925&#34;, 8560, 8790, 8460, 8460, 86723, 8.19], n t t n[&#34;20170926&#34;, 8460, 8500, 8310, 8400, 46210, 8.2], n t t n[&#34;20170927&#34;, 8430, 8590, 8420, 8550, 39309, 8.22], n t t n[&#34;20170928&#34;, 8580, 8650, 8550, 8550, 19868, 8.21], n t t n[&#34;20170929&#34;, 8600, 8620, 8550, 8560, 15105, 8.2], n t t n[&#34;20171010&#34;, 8570, 8940, 8570, 8900, 130263, 8.16], n t t n[&#34;20171011&#34;, 8890, 9000, 8760, 8940, 92727, 8.19], n t t n[&#34;20171012&#34;, 8940, 8990, 8840, 8930, 60083, 8.17], n t t n[&#34;20171013&#34;, 8930, 9080, 8860, 8970, 113322, 8.17], n t t n[&#34;20171016&#34;, 9040, 9050, 8850, 8980, 88278, 8.2], n t t n[&#34;20171017&#34;, 9000, 9360, 9000, 9260, 461848, 8.48], n t t n[&#34;20171018&#34;, 9260, 9340, 9100, 9190, 123834, 8.49], n t t n[&#34;20171019&#34;, 9270, 9290, 9070, 9070, 114497, 8.48], n t t n[&#34;20171020&#34;, 9010, 9110, 9000, 9050, 50993, 8.5], n t t n[&#34;20171023&#34;, 9110, 9340, 9100, 9270, 188819, 8.5], n t t n[&#34;20171024&#34;, 9260, 9300, 9160, 9270, 115701, 8.53], n t t n[&#34;20171025&#34;, 9270, 9280, 9170, 9210, 65649, 8.52], n t t n[&#34;20171026&#34;, 9240, 9240, 9030, 9050, 128044, 8.58], n t t n[&#34;20171027&#34;, 9110, 9170, 9040, 9120, 73998, 8.62], n t t n[&#34;20171030&#34;, 9160, 9180, 9060, 9080, 83490, 8.71], n t t n[&#34;20171031&#34;, 9080, 9080, 8830, 8840, 181161, 8.63], n t t n[&#34;20171101&#34;, 8850, 9170, 8840, 9170, 115335, 8.69], n t t n[&#34;20171102&#34;, 9170, 9180, 9030, 9100, 68630, 8.67], n t t n[&#34;20171103&#34;, 9080, 9270, 9070, 9240, 117255, 8.79], n t t n[&#34;20171106&#34;, 9300, 9500, 9270, 9490, 369895, 9.16], n t t n[&#34;20171107&#34;, 9500, 9530, 9260, 9340, 166998, 9.1], n t t n[&#34;20171108&#34;, 9310, 9700, 9310, 9540, 520859, 9.38], n t t n[&#34;20171109&#34;, 9770, 9900, 9630, 9830, 540600, 9.36], n t t n[&#34;20171110&#34;, 9820, 10250, 9790, 10150, 402076, 9.47], n t t n[&#34;20171113&#34;, 10250, 10650, 10150, 10600, 460926, 9.57], n t t n[&#34;20171114&#34;, 10800, 11200, 10700, 11100, 732027, 9.9], n t t n[&#34;20171115&#34;, 10950, 11250, 10650, 10800, 726653, 10.34], n t t n[&#34;20171116&#34;, 10850, 11200, 10350, 10800, 379162, 10.58], n t t n[&#34;20171117&#34;, 10800, 11450, 10700, 11250, 595862, 10.76], n t t n[&#34;20171120&#34;, 11200, 11550, 11100, 11500, 397187, 10.73], n t t n[&#34;20171121&#34;, 11400, 11500, 11200, 11250, 256717, 10.8], n t t n[&#34;20171122&#34;, 11300, 11400, 10850, 11100, 348354, 11.09], n t t n[&#34;20171123&#34;, 11050, 11350, 10800, 11150, 282817, 11.37], n t t n[&#34;20171124&#34;, 11200, 11250, 10800, 10900, 296010, 11.44], n t t n[&#34;20171127&#34;, 10900, 11100, 10650, 11000, 205062, 11.5], n t t n[&#34;20171128&#34;, 10950, 11050, 10700, 10750, 146968, 11.49], n t t n[&#34;20171129&#34;, 10800, 10950, 10600, 10750, 196725, 11.44], n t t n[&#34;20171130&#34;, 10700, 10800, 10300, 10300, 336283, 11.57], n t t n[&#34;20171201&#34;, 10450, 10650, 10350, 10550, 216633, 11.77], n t t n[&#34;20171204&#34;, 10600, 10750, 10250, 10350, 228353, 11.7], n t t n[&#34;20171205&#34;, 10300, 10400, 10100, 10300, 184698, 11.8], n t t n[&#34;20171206&#34;, 10300, 10500, 10100, 10250, 155578, 11.66], n t t n[&#34;20171207&#34;, 10200, 10250, 9770, 9800, 329528, 11.64], n t t n[&#34;20171208&#34;, 9850, 9930, 9740, 9760, 124909, 11.73], n t t n[&#34;20171211&#34;, 9760, 9930, 9740, 9930, 86535, 11.75], n t t n[&#34;20171212&#34;, 9980, 10050, 9780, 9830, 137399, 11.68], n t t n[&#34;20171213&#34;, 9830, 9960, 9750, 9830, 171879, 11.61], n t t n[&#34;20171214&#34;, 9910, 9910, 9800, 9850, 153404, 11.65], n t t n[&#34;20171215&#34;, 9830, 10100, 9820, 9990, 304468, 11.71], n t t n[&#34;20171218&#34;, 10000, 10050, 9690, 9710, 254871, 11.47], n t t n[&#34;20171219&#34;, 9710, 9710, 9310, 9400, 264554, 11.6], n t t n[&#34;20171220&#34;, 9360, 9620, 9340, 9490, 165736, 11.81], n t t n[&#34;20171221&#34;, 9560, 9560, 9290, 9320, 125390, 11.83], n t t n[&#34;20171222&#34;, 9370, 9370, 9090, 9260, 226274, 12.02], n t t n[&#34;20171226&#34;, 9350, 9580, 9280, 9370, 215351, 11.99], n t t n[&#34;20171227&#34;, 9430, 9470, 9300, 9420, 104946, 12.0], n t t n[&#34;20171228&#34;, 9580, 9750, 9510, 9750, 155890, 11.77], n t t n[&#34;20180102&#34;, 9750, 9900, 9700, 9870, 120676, 11.68], n t t n[&#34;20180103&#34;, 9900, 10250, 9820, 10000, 268220, 11.67], n t t n[&#34;20180104&#34;, 10050, 10050, 9680, 9750, 161342, 11.52], n t t n[&#34;20180105&#34;, 9750, 9980, 9750, 9910, 116604, 11.53], n t t n[&#34;20180108&#34;, 10000, 10150, 9940, 9950, 158326, 11.49], n t t n[&#34;20180109&#34;, 9950, 10050, 9690, 9770, 184742, 11.52], n t t n[&#34;20180110&#34;, 9800, 9910, 9770, 9840, 89778, 11.52], n t t n[&#34;20180111&#34;, 9840, 10350, 9840, 10300, 317974, 11.66], n t t n[&#34;20180112&#34;, 10350, 10450, 10100, 10250, 311381, 11.47], n t t n[&#34;20180115&#34;, 10300, 10450, 10100, 10350, 269179, 11.4], n t t n[&#34;20180116&#34;, 10350, 10350, 10000, 10050, 216119, 11.39], n t t n[&#34;20180117&#34;, 10100, 10100, 9830, 9900, 224108, 11.48], n t t n[&#34;20180118&#34;, 9990, 10250, 9910, 10100, 234830, 11.48], n t t n[&#34;20180119&#34;, 10150, 10850, 10150, 10550, 496024, 11.47], n t t n[&#34;20180122&#34;, 10450, 10800, 10350, 10500, 241650, 11.36], n t t n[&#34;20180123&#34;, 10500, 10700, 10350, 10500, 157733, 11.32], n t t n[&#34;20180124&#34;, 10650, 10650, 10400, 10500, 187411, 11.39], n t t n[&#34;20180125&#34;, 10500, 10600, 10350, 10500, 179397, 11.42], n t t n[&#34;20180126&#34;, 10450, 10900, 10400, 10750, 251422, 11.43], n t t n[&#34;20180129&#34;, 10700, 10750, 10500, 10550, 245952, 11.43], n t t n[&#34;20180130&#34;, 10500, 11000, 10300, 10800, 526654, 11.49], n t t n[&#34;20180131&#34;, 10850, 11300, 10650, 10800, 485972, 11.3], n t t n[&#34;20180201&#34;, 10850, 10900, 10450, 10450, 298372, 11.25], n t t n[&#34;20180202&#34;, 10450, 10500, 10000, 10300, 294961, 11.23], n t t n[&#34;20180205&#34;, 9900, 10650, 9900, 10050, 313533, 11.23], n t t n[&#34;20180206&#34;, 9620, 10000, 9530, 10000, 305434, 11.33], n t t n[&#34;20180207&#34;, 10150, 10750, 10000, 10050, 588716, 11.36], n t t n[&#34;20180208&#34;, 10150, 10300, 10000, 10250, 180650, 11.34], n t t n[&#34;20180209&#34;, 9850, 10200, 9850, 10050, 166414, 11.36], n t t n[&#34;20180212&#34;, 10300, 11050, 10300, 10900, 673670, 11.29], n t t n[&#34;20180213&#34;, 11100, 11200, 10400, 10450, 343493, 11.06], n t t n[&#34;20180214&#34;, 10450, 10600, 10150, 10500, 197459, 11.03], n t t n[&#34;20180219&#34;, 10550, 10800, 10500, 10650, 197794, 11.07], n t t n[&#34;20180220&#34;, 10750, 13050, 10650, 12050, 4543722, 11.14], n t t n[&#34;20180221&#34;, 12050, 12650, 11900, 12150, 1091997, 11.34], n t t n[&#34;20180222&#34;, 12250, 13300, 12000, 12500, 1805812, 11.16], n t t n[&#34;20180223&#34;, 12400, 12650, 12150, 12500, 475850, 11.22], n t t n[&#34;20180226&#34;, 12750, 12850, 12050, 12100, 487602, 11.24], n t t n[&#34;20180227&#34;, 12200, 12350, 11800, 12050, 406559, 11.44], n t t n[&#34;20180228&#34;, 11950, 12300, 11400, 11400, 570361, 11.57], n t t n[&#34;20180302&#34;, 11450, 11750, 11400, 11550, 301574, 11.69], n t t n[&#34;20180305&#34;, 11650, 11750, 11100, 11250, 365544, 11.7], n t t n[&#34;20180306&#34;, 11450, 11600, 11300, 11350, 162679, 11.61], n t t n[&#34;20180307&#34;, 11500, 11950, 11350, 11400, 507488, 11.38], n t t n[&#34;20180308&#34;, 11500, 11700, 11400, 11500, 336931, 11.28], n t t n[&#34;20180309&#34;, 11600, 11800, 11400, 11650, 335997, 11.36], n t t n[&#34;20180312&#34;, 11900, 12400, 11800, 12150, 568919, 11.39], n t t n[&#34;20180313&#34;, 12250, 12300, 12000, 12050, 194821, 11.31], n t t n[&#34;20180314&#34;, 11950, 12300, 11850, 12250, 270990, 11.35], n t t n[&#34;20180315&#34;, 12250, 12950, 12100, 12800, 730921, 11.35], n t t n[&#34;20180316&#34;, 12850, 12850, 12400, 12500, 268782, 11.36], n t t n[&#34;20180319&#34;, 12500, 12550, 12100, 12100, 247683, 11.42], n t t n[&#34;20180320&#34;, 11950, 12600, 11950, 12300, 281262, 11.56], n t t n[&#34;20180321&#34;, 12450, 12550, 12250, 12350, 177979, 11.54], n t t n[&#34;20180322&#34;, 12350, 12950, 12150, 12250, 461295, 11.44], n t t n[&#34;20180323&#34;, 11750, 12000, 11350, 11550, 528504, 11.5], n t t n[&#34;20180326&#34;, 11300, 11650, 11050, 11600, 496562, 11.78], n t t n[&#34;20180327&#34;, 11800, 11850, 11550, 11550, 149019, 11.81], n t t n[&#34;20180328&#34;, 11350, 11800, 11350, 11600, 175221, 11.86], n t t n[&#34;20180329&#34;, 11700, 12150, 11650, 11900, 192037, 11.81], n t t n[&#34;20180330&#34;, 12050, 12400, 11900, 12250, 192869, 11.7], n t t n[&#34;20180402&#34;, 12350, 12350, 11850, 12050, 201636, 11.57], n t t n[&#34;20180403&#34;, 11950, 12400, 11950, 12350, 169250, 11.61], n t t n[&#34;20180404&#34;, 12550, 12600, 12150, 12250, 260021, 11.43], n t t n[&#34;20180405&#34;, 12450, 12650, 12250, 12300, 246051, 11.28], n t t n[&#34;20180406&#34;, 12250, 12350, 11950, 12000, 246931, 11.22], n t t n[&#34;20180409&#34;, 11950, 12550, 11800, 12550, 309843, 11.44], n t t n[&#34;20180410&#34;, 12550, 12950, 12350, 12850, 493495, 11.36], n t t n[&#34;20180411&#34;, 12950, 13400, 12750, 13150, 633981, 11.06], n t t n[&#34;20180412&#34;, 13250, 13250, 12450, 12700, 417912, 10.87], n t t n[&#34;20180413&#34;, 12600, 13100, 12400, 12950, 398688, 10.74], n t t n[&#34;20180416&#34;, 13000, 13050, 12750, 12900, 153975, 10.74], n t t n[&#34;20180417&#34;, 13000, 13000, 12550, 12550, 177401, 10.74], n t t n[&#34;20180418&#34;, 12650, 12700, 12100, 12350, 371564, 10.77], n t t n[&#34;20180419&#34;, 12300, 12300, 11850, 12150, 315272, 10.78], n t t n[&#34;20180420&#34;, 12150, 12550, 12000, 12300, 140028, 10.84], n t t n[&#34;20180423&#34;, 12300, 12500, 11800, 12250, 305907, 10.64], n t t n[&#34;20180424&#34;, 12250, 12500, 11950, 12000, 149286, 10.57], n t t n[&#34;20180425&#34;, 11850, 12100, 11800, 11850, 144095, 10.61], n t t n[&#34;20180426&#34;, 12000, 12150, 11800, 11800, 105708, 10.61], n t t n[&#34;20180427&#34;, 11850, 11950, 11350, 11650, 240082, 10.7], n t t n[&#34;20180430&#34;, 11650, 11650, 11200, 11550, 243943, 10.77], n t t n[&#34;20180502&#34;, 11300, 11700, 11100, 11450, 282457, 10.92], n t t n[&#34;20180503&#34;, 11400, 11600, 11300, 11400, 128166, 10.72], n t t n[&#34;20180504&#34;, 11350, 11650, 11100, 11450, 176792, 10.69], n t t n[&#34;20180508&#34;, 11550, 11550, 11150, 11150, 156726, 10.51], n t t n[&#34;20180509&#34;, 11150, 11250, 10700, 11200, 227226, 10.67], n t t n[&#34;20180510&#34;, 11200, 11300, 10900, 10950, 89191, 10.64], n t t n[&#34;20180511&#34;, 10950, 11200, 10950, 11000, 104215, 10.67], n t t n[&#34;20180514&#34;, 11000, 11100, 10550, 10900, 189786, 10.65], n t t n[&#34;20180515&#34;, 11000, 11350, 10900, 11250, 156107, 10.57], n t t n[&#34;20180516&#34;, 11200, 11450, 11050, 11300, 123985, 10.43], n t t n[&#34;20180517&#34;, 11350, 11750, 11250, 11600, 222032, 10.27], n t t n[&#34;20180518&#34;, 11550, 11700, 11450, 11550, 87395, 10.13], n t t n[&#34;20180521&#34;, 11500, 11650, 11450, 11550, 51571, 10.09], n t t n[&#34;20180523&#34;, 11550, 11550, 11200, 11400, 63532, 10.1], n t t n[&#34;20180524&#34;, 11350, 11500, 11300, 11300, 61476, 10.06], n t t n[&#34;20180525&#34;, 11200, 11350, 11100, 11250, 271659, 10.08], n t t n[&#34;20180528&#34;, 11250, 11600, 11250, 11550, 60366, 10.09], n t t n[&#34;20180529&#34;, 11500, 11650, 11400, 11450, 56965, 10.07], n t t n[&#34;20180530&#34;, 11350, 11450, 11200, 11400, 100702, 10.0], n t t n[&#34;20180531&#34;, 11400, 11600, 11250, 11450, 70704, 10.03], n t t n[&#34;20180601&#34;, 11450, 11700, 11350, 11550, 79703, 10.05], n t t n[&#34;20180604&#34;, 11550, 12050, 11300, 11750, 215783, 9.92], n t t n[&#34;20180605&#34;, 11800, 11950, 11600, 11700, 66094, 9.8], n t t n[&#34;20180607&#34;, 11700, 11750, 11450, 11650, 65640, 9.72], n t t n[&#34;20180608&#34;, 11650, 11700, 11400, 11500, 67197, 9.66], n t t n[&#34;20180611&#34;, 11500, 11650, 11400, 11500, 35991, 9.66], n t t n[&#34;20180612&#34;, 11500, 11750, 11500, 11650, 58240, 9.67], n t t n[&#34;20180614&#34;, 11650, 12000, 11550, 12000, 165452, 9.64], n t t n[&#34;20180615&#34;, 12000, 12050, 11750, 11800, 71864, 9.59], n t t n[&#34;20180618&#34;, 11800, 11900, 11200, 11450, 155385, 9.51], n t t n[&#34;20180619&#34;, 11350, 11450, 10750, 10850, 215289, 9.43], n t t n[&#34;20180620&#34;, 10800, 11250, 10800, 11200, 61718, 9.43], n t t n[&#34;20180621&#34;, 11200, 11350, 11000, 11000, 54794, 9.4], n t t n[&#34;20180622&#34;, 11000, 11150, 10850, 11150, 60641, 9.37], n t t n[&#34;20180625&#34;, 11000, 11200, 10850, 10900, 176086, 9.37], n t t n[&#34;20180626&#34;, 10850, 11100, 10650, 11000, 61908, 9.46], n t t n[&#34;20180627&#34;, 11100, 11100, 10550, 10750, 98881, 9.56], n t t n[&#34;20180628&#34;, 10750, 10850, 10250, 10300, 109892, 9.61], n t t n[&#34;20180629&#34;, 10350, 10450, 9990, 10200, 67505, 9.67], n t t n[&#34;20180702&#34;, 10200, 10250, 9900, 9940, 146002, 9.73], n t t n[&#34;20180703&#34;, 9940, 10350, 9940, 10000, 114950, 9.81], n t t n[&#34;20180704&#34;, 9980, 10450, 9980, 10450, 45931, 9.84], n t t n[&#34;20180705&#34;, 10450, 10600, 10150, 10400, 32153, 9.84], n t t n[&#34;20180706&#34;, 10450, 10750, 10150, 10450, 40958, 9.83], n t t n[&#34;20180709&#34;, 10900, 11300, 10400, 10450, 144702, 9.72], n t t n[&#34;20180710&#34;, 10500, 10750, 10450, 10500, 47448, 9.77], n t t n[&#34;20180711&#34;, 10500, 10600, 10350, 10500, 27316, 9.78], n t t n[&#34;20180712&#34;, 10450, 11000, 10400, 10800, 42613, 9.76], n t t n[&#34;20180713&#34;, 10850, 11050, 10700, 10950, 31649, 9.76], n t t n[&#34;20180716&#34;, 10950, 11000, 10700, 10800, 19437, 9.73], n t t n[&#34;20180717&#34;, 10850, 11200, 10700, 11050, 127186, 9.68], n t t n[&#34;20180718&#34;, 11100, 11250, 10900, 10950, 56270, 9.65], n t t n[&#34;20180719&#34;, 11000, 11000, 10600, 10600, 96686, 9.63], n t t n[&#34;20180720&#34;, 10650, 10750, 10500, 10500, 22912, 9.63], n t t n[&#34;20180723&#34;, 10550, 10600, 10350, 10350, 55756, 9.65], n t t n[&#34;20180724&#34;, 10300, 10800, 10250, 10700, 65258, 9.65], n t t n[&#34;20180725&#34;, 10550, 10700, 10150, 10200, 64850, 9.57], n t t n[&#34;20180726&#34;, 10200, 10600, 10200, 10500, 41959, 9.56], n t t n[&#34;20180727&#34;, 10550, 10700, 10400, 10650, 33164, 9.55], n t t n[&#34;20180730&#34;, 10650, 11150, 10450, 10950, 68997, 9.49], n t t n[&#34;20180731&#34;, 10900, 11150, 10800, 11050, 78102, 9.44], n t t n[&#34;20180801&#34;, 11050, 11500, 10950, 11400, 91051, 9.45], n t t n[&#34;20180802&#34;, 11450, 11700, 11200, 11450, 104941, 9.43], n t t n[&#34;20180803&#34;, 11500, 11850, 11400, 11650, 62460, 9.4], n t t n[&#34;20180806&#34;, 11550, 11750, 11400, 11650, 40690, 9.41], n t t n[&#34;20180807&#34;, 11550, 11700, 11500, 11650, 29281, 9.43], n t t n[&#34;20180808&#34;, 11650, 11800, 11550, 11550, 60600, 9.46], n t t n[&#34;20180809&#34;, 11550, 11650, 11450, 11600, 23495, 9.49], n t t n[&#34;20180810&#34;, 11600, 11600, 11400, 11500, 27420, 9.49], n t t n[&#34;20180813&#34;, 11500, 11650, 11250, 11300, 40595, 9.53], n t t n[&#34;20180814&#34;, 11300, 11400, 11200, 11300, 45698, 9.59], n t t n[&#34;20180816&#34;, 11050, 11100, 10300, 10600, 247778, 9.63], n t t n[&#34;20180817&#34;, 10700, 11350, 10650, 10900, 116104, 9.67], n t t n[&#34;20180820&#34;, 11050, 11400, 10700, 11350, 96590, 9.68], n t t n[&#34;20180821&#34;, 11300, 11450, 11250, 11450, 40869, 9.66], n t t n[&#34;20180822&#34;, 11400, 12150, 11150, 11450, 150405, 9.56], n t t n[&#34;20180823&#34;, 11450, 11700, 11300, 11700, 68284, 9.52], n t t n[&#34;20180824&#34;, 11750, 11800, 11350, 11400, 86605, 9.54], n t t n[&#34;20180827&#34;, 11400, 11600, 11400, 11450, 41732, 9.55], n t t n[&#34;20180828&#34;, 11450, 11700, 11400, 11650, 62377, 9.58], n t t n[&#34;20180829&#34;, 11700, 11700, 11300, 11600, 54304, 9.57], n t t n[&#34;20180830&#34;, 11500, 11650, 11350, 11600, 93124, 9.63], n t t n[&#34;20180831&#34;, 11500, 11800, 11500, 11600, 71357, 9.69], n t t n[&#34;20180903&#34;, 11700, 11700, 11450, 11550, 34494, 9.67], n t t n[&#34;20180904&#34;, 11500, 11600, 11400, 11450, 41102, 9.65], n t t n[&#34;20180905&#34;, 11450, 11550, 11350, 11400, 42718, 9.65], n t t n[&#34;20180906&#34;, 11350, 11600, 11250, 11500, 55464, 9.64], n t t n[&#34;20180907&#34;, 11450, 11600, 11350, 11450, 37238, 9.62], n t t n[&#34;20180910&#34;, 11450, 11550, 11350, 11400, 36395, 9.61], n t t n[&#34;20180911&#34;, 11450, 11550, 11200, 11550, 41874, 9.6], n t t n[&#34;20180912&#34;, 11600, 11700, 11350, 11600, 80433, 9.55], n t t n[&#34;20180913&#34;, 11550, 11700, 11400, 11500, 73528, 9.5], n t t n[&#34;20180914&#34;, 11500, 11550, 11200, 11350, 102381, 9.48], n t t n[&#34;20180917&#34;, 11300, 11300, 11000, 11050, 135163, 9.3], n t t n[&#34;20180918&#34;, 10950, 11100, 10800, 10950, 92846, 9.21], n t t n[&#34;20180919&#34;, 11050, 11150, 10650, 10750, 92323, 9.26], n t t n[&#34;20180920&#34;, 10800, 11100, 10750, 11050, 96411, 9.26], n t t n[&#34;20180921&#34;, 11100, 11100, 10900, 11100, 27217, 9.27], n t t n[&#34;20180927&#34;, 11100, 11300, 11000, 11250, 59632, 9.3], n t t n[&#34;20180928&#34;, 11300, 11500, 11150, 11350, 81785, 9.31], n t t n[&#34;20181001&#34;, 11250, 11500, 11100, 11150, 42239, 9.3], n t t n[&#34;20181002&#34;, 11250, 11250, 10850, 10900, 84470, 9.3], n t t n[&#34;20181004&#34;, 10950, 11100, 10800, 10950, 42691, 9.29], n t t n[&#34;20181005&#34;, 10800, 10900, 10350, 10600, 113619, 9.22], n t t n[&#34;20181008&#34;, 10600, 10650, 10100, 10100, 168229, 9.29], n t t n[&#34;20181010&#34;, 10250, 10250, 9800, 9900, 183815, 9.19], n t t n[&#34;20181011&#34;, 9500, 9770, 9230, 9250, 181929, 9.19], n t t n[&#34;20181012&#34;, 9060, 9660, 9060, 9600, 95130, 9.19], n t t n[&#34;20181015&#34;, 9600, 9600, 9360, 9370, 44621, 9.13], n t t n[&#34;20181016&#34;, 9350, 9750, 9350, 9600, 53432, 9.09], n t t n[&#34;20181017&#34;, 9650, 10050, 9650, 9890, 81996, 9.0], n t t n[&#34;20181018&#34;, 9820, 9990, 9650, 9680, 47239, 8.94], n t t n[&#34;20181019&#34;, 9570, 9820, 9550, 9780, 34401, 8.9], n t t n[&#34;20181022&#34;, 9780, 9910, 9550, 9580, 110564, 8.84], n t t n[&#34;20181023&#34;, 9580, 9630, 8990, 9080, 105290, 8.86], n t t n[&#34;20181024&#34;, 9120, 9260, 8800, 8800, 80856, 8.84], n t t n[&#34;20181025&#34;, 8600, 8780, 8460, 8580, 146074, 8.91], n t t n[&#34;20181026&#34;, 8730, 8790, 8210, 8370, 151816, 8.89], n t t n[&#34;20181029&#34;, 8270, 8550, 7770, 7900, 103108, 8.87], n t t n[&#34;20181030&#34;, 7500, 8320, 7500, 8230, 295343, 8.98], n t t n[&#34;20181031&#34;, 8250, 9500, 8160, 9050, 546775, 8.71], n t t n[&#34;20181101&#34;, 9000, 9410, 9000, 9210, 117366, 8.66], n t t n[&#34;20181102&#34;, 9340, 9460, 9130, 9310, 113927, 8.64], n t t n[&#34;20181105&#34;, 9290, 9940, 9200, 9510, 172127, 8.51], n t t n[&#34;20181106&#34;, 9520, 9760, 9420, 9640, 81099, 8.51], n t t n[&#34;20181107&#34;, 9640, 9900, 9500, 9570, 144059, 8.53], n t t n[&#34;20181108&#34;, 9690, 9990, 9560, 9740, 131886, 8.52], n t t n[&#34;20181109&#34;, 9740, 10050, 9450, 9980, 133154, 8.48], n t t n[&#34;20181112&#34;, 9890, 9890, 9390, 9420, 155423, 8.34], n t t n[&#34;20181113&#34;, 9300, 9680, 9270, 9470, 68533, 8.38], n t t n[&#34;20181114&#34;, 9460, 9840, 9460, 9560, 161809, 8.47], n t t n[&#34;20181115&#34;, 9510, 9610, 9150, 9390, 179719, 8.5], n t t n[&#34;20181116&#34;, 9380, 9730, 9380, 9600, 51007, 8.56], n t t n[&#34;20181119&#34;, 9570, 9670, 9560, 9660, 23946, 8.58], n t t n[&#34;20181120&#34;, 9510, 9670, 9500, 9500, 17839, 8.58], n t t n[&#34;20181121&#34;, 9480, 9580, 9230, 9480, 137363, 8.61], n t t n[&#34;20181122&#34;, 9520, 9760, 9090, 9590, 181019, 8.54], n t t n[&#34;20181123&#34;, 9590, 9630, 9410, 9480, 40882, 8.5], n t t n[&#34;20181126&#34;, 9580, 9580, 9390, 9400, 41585, 8.5], n t t n[&#34;20181127&#34;, 9450, 9510, 9410, 9430, 21198, 8.5], n t t n[&#34;20181128&#34;, 9480, 9490, 9150, 9320, 138261, 8.5], n t t n[&#34;20181129&#34;, 9410, 9410, 9260, 9360, 47101, 8.47], n t t n[&#34;20181130&#34;, 9370, 9380, 9100, 9110, 123667, 8.5], n t t n[&#34;20181203&#34;, 9230, 9430, 9230, 9370, 68336, 8.53], n t t n[&#34;20181204&#34;, 9430, 9490, 9320, 9400, 28642, 8.53], n t t n[&#34;20181205&#34;, 9310, 9430, 9310, 9420, 21831, 8.53], n t t n[&#34;20181206&#34;, 9480, 9610, 9280, 9430, 112702, 8.53], n t t n[&#34;20181207&#34;, 9490, 9890, 9440, 9690, 127703, 8.52], n t t n[&#34;20181210&#34;, 9590, 9710, 9520, 9660, 58399, 8.5], n t t n[&#34;20181211&#34;, 9660, 9760, 9320, 9320, 82378, 8.41], n t t n[&#34;20181212&#34;, 9330, 9580, 9330, 9570, 23962, 8.43], n t t n[&#34;20181213&#34;, 9530, 9700, 9450, 9600, 71270, 8.38], n t t n[&#34;20181214&#34;, 9730, 9730, 9250, 9430, 168582, 8.36], n t t n[&#34;20181217&#34;, 9430, 9670, 9300, 9500, 131559, 8.33], n t t n[&#34;20181218&#34;, 9500, 9500, 9090, 9350, 142335, 8.26], n t t n[&#34;20181219&#34;, 9350, 9400, 9100, 9180, 59515, 8.24], n t t n[&#34;20181220&#34;, 9180, 9180, 8870, 8950, 186214, 8.25], n t t n[&#34;20181221&#34;, 8950, 9130, 8950, 9030, 61176, 8.23], n t t n[&#34;20181224&#34;, 8910, 9340, 8800, 8830, 135709, 8.23], n t t n[&#34;20181226&#34;, 8600, 9160, 8590, 8730, 325058, 8.23], n t t n[&#34;20181227&#34;, 8810, 9120, 8730, 9040, 65184, 8.28], n t t n[&#34;20181228&#34;, 9080, 9170, 9030, 9100, 26013, 8.29], n t t n[&#34;20190102&#34;, 9190, 9430, 9080, 9340, 88403, 8.27], n t t n[&#34;20190103&#34;, 9340, 9390, 9130, 9170, 54992, 8.18], n t t n[&#34;20190104&#34;, 9100, 9700, 9100, 9530, 115336, 8.16], n t t n[&#34;20190107&#34;, 9660, 9800, 9510, 9530, 107912, 8.11], n t t n[&#34;20190108&#34;, 9610, 9640, 9320, 9520, 108681, 8.11], n t t n[&#34;20190109&#34;, 9520, 9620, 9380, 9450, 82564, 8.13], n t t n[&#34;20190110&#34;, 9430, 9530, 9350, 9450, 53611, 8.1], n t t n[&#34;20190111&#34;, 9400, 9520, 9390, 9400, 22406, 8.09], n t t n[&#34;20190114&#34;, 9470, 9470, 9360, 9400, 17930, 8.09], n t t n[&#34;20190115&#34;, 9380, 9450, 9350, 9400, 20020, 8.07], n t t n[&#34;20190116&#34;, 9440, 9600, 9400, 9510, 72822, 8.01], n t t n[&#34;20190117&#34;, 9520, 9670, 9450, 9640, 69704, 7.99], n t t n[&#34;20190118&#34;, 9640, 9710, 9600, 9640, 35072, 7.97], n t t n[&#34;20190121&#34;, 9660, 9820, 9650, 9750, 65445, 7.93], n t t n[&#34;20190122&#34;, 9750, 9860, 9660, 9800, 42143, 7.93], n t t n[&#34;20190123&#34;, 9840, 9880, 9740, 9820, 31492, 7.91], n t t n[&#34;20190124&#34;, 9820, 10000, 9710, 10000, 109714, 7.9], n t t n[&#34;20190125&#34;, 10000, 10100, 9920, 9980, 24176, 7.89], n t t n[&#34;20190128&#34;, 9970, 10100, 9860, 9930, 52888, 7.89], n t t n[&#34;20190129&#34;, 9930, 10000, 9690, 9780, 55670, 7.93], n t t n[&#34;20190130&#34;, 9730, 9820, 9580, 9630, 140428, 7.97], n t t n[&#34;20190131&#34;, 9720, 9840, 9580, 9840, 52731, 7.99], n t t n[&#34;20190201&#34;, 9830, 9890, 9770, 9840, 20257, 8.01], n t t n[&#34;20190207&#34;, 9810, 10100, 9720, 10000, 138074, 8.0], n t t n[&#34;20190208&#34;, 9930, 10000, 9900, 9990, 22134, 8.0], n t t n[&#34;20190211&#34;, 10000, 10300, 9950, 9990, 196429, 7.99], n t t n[&#34;20190212&#34;, 10000, 10100, 9920, 10050, 50533, 7.95], n t t n[&#34;20190213&#34;, 10050, 10150, 9960, 10050, 23716, 7.96], n t t n[&#34;20190214&#34;, 10050, 10050, 9890, 9940, 102144, 7.95], n t t n[&#34;20190215&#34;, 9950, 10000, 9710, 9880, 92273, 7.97], n t t n[&#34;20190218&#34;, 9880, 9960, 9850, 9850, 18197, 7.96], n t t n[&#34;20190219&#34;, 9830, 9930, 9820, 9860, 34871, 7.97], n t t n[&#34;20190220&#34;, 9890, 9900, 9750, 9800, 50337, 7.99], n t t n[&#34;20190221&#34;, 9880, 9910, 9730, 9830, 44725, 7.95], n t t n[&#34;20190222&#34;, 9750, 9910, 9700, 9750, 40910, 7.94], n t t n[&#34;20190225&#34;, 9800, 9820, 9680, 9680, 34874, 7.94], n t t n[&#34;20190226&#34;, 9750, 10000, 9680, 9850, 69962, 7.93], n t t n[&#34;20190227&#34;, 9850, 9990, 9850, 9930, 84808, 7.91], n t t n[&#34;20190228&#34;, 9930, 9980, 9720, 9800, 127433, 7.87], n t t n[&#34;20190304&#34;, 9810, 10050, 9800, 10000, 86244, 7.9], n t t n[&#34;20190305&#34;, 10050, 10150, 9970, 10100, 65110, 7.91], n t t n[&#34;20190306&#34;, 10150, 10250, 10050, 10100, 47791, 7.89], n t t n[&#34;20190307&#34;, 10050, 10100, 9910, 9990, 65924, 7.85], n t t n[&#34;20190308&#34;, 9990, 10000, 9860, 9930, 34591, 7.84], n t t n[&#34;20190311&#34;, 9930, 9960, 9700, 9850, 89651, 7.82], n t t n[&#34;20190312&#34;, 9900, 9960, 9870, 9930, 50658, 7.88], n t t n[&#34;20190313&#34;, 9950, 10150, 9930, 10050, 74778, 8.0], n t t n[&#34;20190314&#34;, 10050, 10100, 9990, 10100, 73439, 8.03], n t t n[&#34;20190315&#34;, 10150, 10150, 9970, 9970, 59500, 8.04], n t t n[&#34;20190318&#34;, 10000, 10050, 9900, 9990, 37952, 8.03], n t t n[&#34;20190319&#34;, 9990, 10100, 9970, 9980, 78213, 8.04], n t t n[&#34;20190320&#34;, 10050, 10050, 9860, 10000, 27138, 8.02], n t t n[&#34;20190321&#34;, 10050, 10050, 9790, 9820, 62361, 8.0], n t t n[&#34;20190322&#34;, 9820, 9920, 9570, 9680, 187248, 7.97], n t t n[&#34;20190325&#34;, 9600, 9720, 9510, 9650, 100403, 7.93], n t t n[&#34;20190326&#34;, 9630, 9680, 9560, 9560, 33185, 7.91], n t t n[&#34;20190327&#34;, 9600, 9600, 9440, 9470, 23278, 7.97], n t t n[&#34;20190328&#34;, 9540, 9540, 9070, 9070, 164879, 7.9], n t t n[&#34;20190329&#34;, 9140, 9440, 9040, 9400, 76176, 7.86], n t t n[&#34;20190401&#34;, 9390, 9480, 9310, 9420, 31132, 7.85], n t t n[&#34;20190402&#34;, 9460, 9460, 9320, 9390, 51002, 7.79], n t t n[&#34;20190403&#34;, 9480, 9540, 9400, 9510, 28078, 7.78], n t t n[&#34;20190404&#34;, 9560, 9590, 9450, 9560, 38609, 7.77], n t t n[&#34;20190405&#34;, 9540, 9570, 9400, 9440, 58908, 7.7], n t t n[&#34;20190408&#34;, 9570, 9570, 9410, 9450, 23721, 7.72], n t t n[&#34;20190409&#34;, 9480, 9500, 9350, 9390, 35620, 7.71], n t t n[&#34;20190410&#34;, 9390, 9420, 9250, 9390, 68686, 7.68], n t t n[&#34;20190411&#34;, 9440, 9590, 9340, 9490, 81137, 7.68], n t t n[&#34;20190412&#34;, 9490, 9660, 9370, 9600, 60392, 7.68], n t t n[&#34;20190415&#34;, 9570, 9890, 9480, 9690, 104176, 7.62], n t t n[&#34;20190416&#34;, 9690, 9770, 9610, 9710, 31108, 7.6], n t t n[&#34;20190417&#34;, 9740, 9840, 9640, 9740, 37533, 7.58], n t t n[&#34;20190418&#34;, 9740, 9890, 9650, 9650, 51023, 7.55], n t t n[&#34;20190419&#34;, 9740, 9790, 9690, 9720, 26028, 7.56], n t t n[&#34;20190422&#34;, 9740, 9920, 9680, 9900, 95700, 7.54], n t t n[&#34;20190423&#34;, 9880, 9960, 9820, 9920, 55550, 7.49], n t t n[&#34;20190424&#34;, 9950, 9970, 9800, 9930, 62744, 7.47], n t t n[&#34;20190425&#34;, 9940, 9960, 9700, 9860, 48468, 7.43], n t t n[&#34;20190426&#34;, 9890, 9960, 9770, 9880, 37701, 7.46], n t t n[&#34;20190429&#34;, 9840, 9970, 9830, 9850, 46248, 7.45], n t t n[&#34;20190430&#34;, 9810, 9950, 9780, 9780, 38878, 7.43], n t t n[&#34;20190502&#34;, 9770, 9890, 9770, 9870, 25235, 7.44], n t t n[&#34;20190503&#34;, 9870, 9920, 9790, 9860, 57649, 7.42], n t t n[&#34;20190507&#34;, 9760, 9880, 9710, 9860, 18445, 7.41], n t t n[&#34;20190508&#34;, 9860, 9930, 9780, 9810, 18894, 7.4], n t t n[&#34;20190509&#34;, 9880, 9880, 9730, 9820, 40324, 7.35], n t t n[&#34;20190510&#34;, 9820, 9820, 9700, 9800, 32923, 7.34], n t t n[&#34;20190513&#34;, 9840, 9840, 9610, 9610, 29504, 7.31], n t t n[&#34;20190514&#34;, 9600, 9650, 9460, 9640, 52369, 7.31], n t t n[&#34;20190515&#34;, 9640, 9680, 9550, 9570, 34418, 7.27], n t t n[&#34;20190516&#34;, 9570, 9670, 9370, 9390, 51140, 7.26], n t t n[&#34;20190517&#34;, 9370, 9550, 9130, 9260, 48789, 7.22], n t t n[&#34;20190520&#34;, 9250, 9450, 9100, 9350, 229321, 7.13], n t t n[&#34;20190521&#34;, 9350, 9640, 9350, 9620, 86187, 7.12], n t t n[&#34;20190522&#34;, 9620, 9620, 9430, 9430, 31172, 7.11], n t t n[&#34;20190523&#34;, 9600, 9600, 9390, 9440, 55059, 7.06], n t t n[&#34;20190524&#34;, 9520, 9520, 9330, 9380, 14563, 7.04], n t t n[&#34;20190527&#34;, 9380, 9410, 9190, 9320, 28918, 7.0], n t t n[&#34;20190528&#34;, 9300, 9400, 9260, 9300, 31046, 6.97], n t t n[&#34;20190529&#34;, 9370, 9430, 9210, 9310, 42534, 6.93], n t t n[&#34;20190530&#34;, 9260, 9500, 9200, 9300, 43638, 6.84], n t t n[&#34;20190531&#34;, 9350, 9600, 9070, 9200, 138861, 6.82], n t t n[&#34;20190603&#34;, 9200, 9250, 9030, 9230, 66893, 6.81], n t t n[&#34;20190604&#34;, 9220, 9400, 9100, 9380, 131427, 6.76], n t t n[&#34;20190605&#34;, 9380, 9570, 9330, 9490, 57571, 6.75], n t t n[&#34;20190607&#34;, 9450, 9490, 9160, 9220, 145811, 6.78], n t t n[&#34;20190610&#34;, 9220, 9300, 9180, 9220, 37730, 6.73], n t t n[&#34;20190611&#34;, 9270, 9550, 9210, 9520, 77472, 6.75], n t t n[&#34;20190612&#34;, 9470, 9780, 9410, 9600, 136781, 6.73], n t t n[&#34;20190613&#34;, 9600, 9920, 9530, 9680, 173213, 6.75], n t t n[&#34;20190614&#34;, 9680, 9870, 9680, 9730, 56319, 6.79], n t t n[&#34;20190617&#34;, 9790, 9830, 9650, 9790, 41302, 6.77], n t t n[&#34;20190618&#34;, 9790, 10150, 9680, 9900, 152876, 6.65], n t t n[&#34;20190619&#34;, 9900, 9960, 9730, 9900, 66758, 6.64], n t t n[&#34;20190620&#34;, 9930, 9950, 9810, 9820, 20025, 6.63], n t t n[&#34;20190621&#34;, 9780, 9990, 9780, 9870, 95774, 6.61], n t t n[&#34;20190624&#34;, 9910, 9920, 9820, 9880, 19246, 6.59], n t t n[&#34;20190625&#34;, 9810, 9890, 9730, 9860, 25531, 6.55], n t t n[&#34;20190626&#34;, 9860, 9900, 9790, 9840, 12594, 6.53], n t t n[&#34;20190627&#34;, 9890, 9890, 9700, 9760, 23986, 6.51], n t t n[&#34;20190628&#34;, 9800, 9870, 9610, 9680, 18528, 6.5], n t t n[&#34;20190701&#34;, 9680, 9840, 9680, 9710, 31341, 6.5], n t t n[&#34;20190702&#34;, 9810, 9810, 9700, 9750, 22238, 6.51], n t t n[&#34;20190703&#34;, 9810, 9810, 9670, 9750, 25770, 6.5], n t t n[&#34;20190704&#34;, 9730, 9780, 9540, 9740, 45464, 6.47], n t t n[&#34;20190705&#34;, 9750, 9780, 9490, 9650, 81317, 6.5], n t t n[&#34;20190708&#34;, 9650, 9650, 9210, 9210, 41576, 6.48], n t t n[&#34;20190709&#34;, 9350, 9470, 8980, 9310, 179334, 6.44], n t t n[&#34;20190710&#34;, 9360, 9520, 9270, 9380, 33290, 6.42], n t t n[&#34;20190711&#34;, 9480, 9580, 9380, 9500, 22715, 6.41], n t t n[&#34;20190712&#34;, 9530, 9740, 9390, 9530, 62868, 6.36], n t t n[&#34;20190715&#34;, 9620, 9960, 9400, 9800, 193916, 6.31], n t t n[&#34;20190716&#34;, 9800, 9890, 9660, 9760, 71302, 6.24], n t t n[&#34;20190717&#34;, 9750, 9780, 9560, 9750, 30731, 6.22], n t t n[&#34;20190718&#34;, 9730, 9730, 9610, 9690, 50738, 6.21], n t t n[&#34;20190719&#34;, 9700, 9810, 9660, 9770, 18237, 6.2], n t t n[&#34;20190722&#34;, 9770, 9800, 9600, 9600, 11527, 6.18], n t t n[&#34;20190723&#34;, 9600, 9690, 9490, 9500, 14635, 6.17], n t t n[&#34;20190724&#34;, 9500, 9600, 9440, 9470, 11037, 6.16], n t t n[&#34;20190725&#34;, 9500, 9550, 9210, 9360, 17499, 6.13], n t t n[&#34;20190726&#34;, 9450, 9450, 9220, 9390, 14088, 6.1], n t t n[&#34;20190729&#34;, 9450, 9450, 8940, 9080, 68584, 6.07], n t t n[&#34;20190730&#34;, 9030, 9150, 8820, 8920, 95596, 6.07], n t t n[&#34;20190731&#34;, 8930, 8980, 8700, 8920, 53537, 6.07], n t t n[&#34;20190801&#34;, 8920, 9180, 8780, 8900, 36073, 6.04], n t t n[&#34;20190802&#34;, 8710, 8930, 8590, 8840, 82479, 6.05], n t t n[&#34;20190805&#34;, 8840, 8840, 7990, 8000, 135515, 6.05], n t t n[&#34;20190806&#34;, 7640, 8400, 7640, 7940, 303102, 6.01], n t t n[&#34;20190807&#34;, 7970, 8650, 7500, 8600, 309525, 5.86], n t t n[&#34;20190808&#34;, 8540, 9010, 8400, 9000, 89314, 5.84], n t t n[&#34;20190809&#34;, 9020, 9030, 8770, 8970, 103254, 5.86], n t t n[&#34;20190812&#34;, 9000, 9000, 8840, 8920, 7387, 5.86], n t t n[&#34;20190813&#34;, 8800, 9360, 8790, 9160, 60457, 5.86], n t t n[&#34;20190814&#34;, 9150, 9500, 9110, 9400, 235291, 5.81], n t t n[&#34;20190816&#34;, 9250, 9250, 8800, 9020, 108224, 5.82], n t t n[&#34;20190819&#34;, 9020, 9060, 8470, 8720, 480449, 5.72], n t t n[&#34;20190820&#34;, 8670, 8800, 8660, 8760, 73877, 5.71], n t t n[&#34;20190821&#34;, 8770, 9140, 8720, 9100, 109741, 5.65], n t t n[&#34;20190822&#34;, 9050, 9300, 8720, 9150, 147728, 5.56], n t t n[&#34;20190823&#34;, 9020, 9080, 8850, 8860, 69108, 5.49], n t t n[&#34;20190826&#34;, 8810, 8940, 8650, 8930, 30632, 5.51], n t t n[&#34;20190827&#34;, 8940, 9000, 8410, 8730, 403719, 5.08], n t t n[&#34;20190828&#34;, 8770, 8770, 8240, 8390, 252754, 4.93], n t t n[&#34;20190829&#34;, 8390, 8440, 8230, 8230, 155522, 4.86], n t t n[&#34;20190830&#34;, 8300, 8340, 8180, 8230, 110183, 4.79], n t t n[&#34;20190902&#34;, 8200, 8290, 7990, 8070, 93231, 4.84], n t t n[&#34;20190903&#34;, 8060, 8150, 7960, 7970, 144638, 4.85], n t t n[&#34;20190904&#34;, 7970, 8090, 7970, 8050, 42358, 4.81], n t t n[&#34;20190905&#34;, 8070, 8110, 8020, 8080, 77480, 4.77], n t t n[&#34;20190906&#34;, 8120, 8120, 7930, 8000, 189109, 4.72], n t t n[&#34;20190909&#34;, 8000, 8190, 7980, 8000, 57940, 4.72], n t t n[&#34;20190910&#34;, 8070, 8080, 7990, 8000, 13627, 4.72], n t t n[&#34;20190911&#34;, 8060, 8180, 7880, 7900, 395212, 4.81], n t t n[&#34;20190916&#34;, 7970, 8150, 7860, 8110, 68468, 4.86], n t t n[&#34;20190917&#34;, 8130, 8430, 8090, 8280, 229106, 5.04], n t t n[&#34;20190918&#34;, 8370, 8370, 8220, 8300, 32102, 5.03], n t t n[&#34;20190919&#34;, 8300, 8350, 8090, 8180, 113570, 5.03], n t t n[&#34;20190920&#34;, 8200, 8240, 8100, 8210, 29858, 5.05], n t t n[&#34;20190923&#34;, 8200, 8680, 8050, 8340, 138928, 4.94], n t t n[&#34;20190924&#34;, 8290, 8390, 8230, 8230, 30980, 4.93], n t t n[&#34;20190925&#34;, 8230, 8470, 8110, 8240, 126818, 4.92], n t t n[&#34;20190926&#34;, 8280, 8280, 7980, 8120, 70346, 4.88], n t t n[&#34;20190927&#34;, 8100, 8130, 7880, 7980, 144544, 4.89], n t t n[&#34;20190930&#34;, 7980, 8040, 7900, 7910, 26914, 4.88], n t t n[&#34;20191001&#34;, 7880, 8180, 7860, 8100, 103313, 4.92], n t t n[&#34;20191002&#34;, 8120, 8120, 7680, 7680, 291165, 4.9], n t t n[&#34;20191004&#34;, 7690, 7830, 7670, 7800, 37463, 4.9], n t t n[&#34;20191007&#34;, 7800, 7850, 7760, 7790, 15138, 4.9], n t t n[&#34;20191008&#34;, 7790, 7840, 7770, 7830, 19135, 4.91], n t t n[&#34;20191010&#34;, 7880, 7880, 7650, 7840, 31612, 4.96], n t t n[&#34;20191011&#34;, 7850, 7900, 7820, 7850, 28408, 4.99], n t t n[&#34;20191014&#34;, 7920, 7930, 7850, 7900, 20512, 4.98], n t t n[&#34;20191015&#34;, 7890, 7930, 7680, 7800, 145252, 5.02], n t t n[&#34;20191016&#34;, 7870, 8150, 7850, 8000, 241290, 5.12], n t t n[&#34;20191017&#34;, 8010, 8120, 7920, 7940, 11065, 5.12], n t t n[&#34;20191018&#34;, 7960, 7980, 7830, 7830, 33879, 5.08], n t t n[&#34;20191021&#34;, 7830, 7870, 7790, 7800, 42419, 5.06], n t t n[&#34;20191022&#34;, 7770, 7940, 7750, 7860, 212452, 5.09], n t t n[&#34;20191023&#34;, 7940, 7940, 7740, 7860, 213459, 5.12], n t t n[&#34;20191024&#34;, 7860, 8200, 7750, 7980, 109246, 5.1], n t t n[&#34;20191025&#34;, 7980, 8130, 7900, 8000, 38500, 5.11], n t t n[&#34;20191028&#34;, 8000, 8440, 8000, 8120, 227009, 4.98], n t t n[&#34;20191029&#34;, 8080, 8110, 7780, 7860, 180160, 4.91], n t t n[&#34;20191030&#34;, 7910, 7910, 7500, 7730, 283033, 5.0], n t t n[&#34;20191031&#34;, 7780, 7920, 7600, 7830, 349763, 5.0], n t t n[&#34;20191101&#34;, 7800, 7840, 7710, 7770, 99335, 4.98], n t t n[&#34;20191104&#34;, 7750, 7880, 7750, 7850, 55732, 5.03], n t t n[&#34;20191105&#34;, 7850, 8080, 7840, 8070, 172022, 5.23], n t t n[&#34;20191106&#34;, 8020, 8520, 8020, 8370, 546262, 5.07], n t t n[&#34;20191107&#34;, 8310, 8460, 8190, 8210, 86701, 5.17], n t t n[&#34;20191108&#34;, 8220, 8460, 8150, 8370, 160729, 5.11], n t t n[&#34;20191111&#34;, 8390, 8390, 8190, 8270, 56589, 5.04], n t t n[&#34;20191112&#34;, 8270, 8440, 8230, 8250, 37138, 4.99], n t t n[&#34;20191113&#34;, 8220, 8280, 8180, 8190, 39144, 4.96], n t t n[&#34;20191114&#34;, 8220, 8380, 8160, 8310, 84060, 4.91], n t t n[&#34;20191115&#34;, 8260, 8370, 8010, 8150, 47324, 4.88], n t t n[&#34;20191118&#34;, 8170, 8260, 8110, 8200, 30185, 4.87], n t t n[&#34;20191119&#34;, 8200, 8380, 8080, 8340, 152795, 4.85], n t t n[&#34;20191120&#34;, 8290, 8350, 8110, 8270, 270370, 4.75], n t t n[&#34;20191121&#34;, 8270, 8330, 8100, 8270, 170686, 4.71], n t t n[&#34;20191122&#34;, 8320, 8330, 8100, 8100, 57998, 4.69], n t t n[&#34;20191125&#34;, 8190, 8240, 8110, 8190, 32694, 4.69], n t t n[&#34;20191126&#34;, 8240, 8250, 8100, 8230, 85259, 4.71], n t t n[&#34;20191127&#34;, 8230, 8240, 8090, 8210, 120870, 4.77], n t t n[&#34;20191128&#34;, 8200, 8320, 8190, 8260, 115148, 4.79], n t t n[&#34;20191129&#34;, 8260, 8290, 8140, 8190, 34505, 4.66], n t t n[&#34;20191202&#34;, 8190, 8230, 8100, 8110, 31702, 4.66], n t t n[&#34;20191203&#34;, 8100, 8120, 7970, 8040, 34777, 4.65], n t t n[&#34;20191204&#34;, 8070, 8080, 7950, 8010, 29710, 4.64], n t t n[&#34;20191205&#34;, 8010, 8250, 7750, 7760, 378075, 4.69], n t t n[&#34;20191206&#34;, 7800, 7850, 7680, 7810, 108137, 4.67], n t t n[&#34;20191209&#34;, 7810, 7860, 7500, 7580, 280557, 4.66], n t t n[&#34;20191210&#34;, 7620, 7660, 7540, 7600, 62018, 4.67], n t t n[&#34;20191211&#34;, 7620, 7850, 7550, 7700, 99110, 4.7], n t t n[&#34;20191212&#34;, 7750, 7910, 7700, 7800, 105534, 4.66], n t t n[&#34;20191213&#34;, 7800, 8180, 7710, 7860, 689432, 4.85], n t t n[&#34;20191216&#34;, 7860, 8250, 7860, 8010, 93201, 4.84], n t t n[&#34;20191217&#34;, 8100, 8260, 7960, 8170, 235843, 4.99], n t t n[&#34;20191218&#34;, 8230, 8230, 7980, 8050, 334713, 5.17], n t t n[&#34;20191219&#34;, 8030, 8150, 8000, 8130, 63093, 5.22], n t t n[&#34;20191220&#34;, 8140, 8210, 8090, 8140, 50591, 5.23], n t t n[&#34;20191223&#34;, 8120, 8240, 8000, 8150, 115949, 5.36], n t t n[&#34;20191224&#34;, 8140, 8180, 7990, 8150, 150877, 5.47], n t t n[&#34;20191226&#34;, 8160, 8160, 8000, 8000, 318154, 5.68], n t t n[&#34;20191227&#34;, 8010, 8150, 7980, 8140, 57641, 5.72], n t t n[&#34;20191230&#34;, 8110, 8420, 8110, 8310, 198584, 6.04], n t t n[&#34;20200102&#34;, 8340, 8400, 8290, 8400, 111305, 6.05], n t t n[&#34;20200103&#34;, 8400, 8450, 8290, 8360, 96437, 6.09], n t t n[&#34;20200106&#34;, 8290, 8330, 8120, 8180, 73230, 6.08], n t t n[&#34;20200107&#34;, 8200, 8280, 8090, 8160, 117904, 6.13], n t t n[&#34;20200108&#34;, 8170, 8170, 7830, 7930, 263246, 6.12], n t t n[&#34;20200109&#34;, 8020, 8060, 7900, 7900, 50346, 6.08], n t t n[&#34;20200110&#34;, 7970, 8140, 7880, 8100, 77059, 6.07], n t t n[&#34;20200113&#34;, 8140, 8250, 8070, 8220, 91646, 6.03], n t t n[&#34;20200114&#34;, 8240, 8240, 8070, 8140, 100901, 6.03], n t t n[&#34;20200115&#34;, 8160, 8170, 8000, 8090, 72255, 6.06], n t t n[&#34;20200116&#34;, 8140, 8140, 8030, 8030, 80537, 6.1], n t t n[&#34;20200117&#34;, 8040, 8130, 8030, 8110, 47098, 6.1], n t t n[&#34;20200120&#34;, 8140, 8340, 8100, 8240, 205611, 6.12], n t t n[&#34;20200121&#34;, 8250, 8280, 8080, 8130, 161270, 6.13], n t t n[&#34;20200122&#34;, 8160, 8470, 8160, 8290, 251732, 6.37], n t t n[&#34;20200123&#34;, 8390, 8410, 8290, 8350, 90264, 6.35], n t t n[&#34;20200128&#34;, 8400, 8960, 8290, 8590, 915591, 6.07], n t t n[&#34;20200129&#34;, 8640, 8690, 8110, 8150, 370187, 5.76], n t t n[&#34;20200130&#34;, 8180, 8260, 7990, 8000, 239447, 5.75], n t t n[&#34;20200131&#34;, 8030, 8110, 7880, 7910, 255775, 5.78], n t t n[&#34;20200203&#34;, 7790, 7840, 7370, 7530, 394550, 5.65], n t t n[&#34;20200204&#34;, 7550, 7640, 7530, 7600, 87815, 5.58], n t t n[&#34;20200205&#34;, 7640, 7770, 7630, 7680, 120557, 5.6], n t t n[&#34;20200206&#34;, 7750, 7890, 7690, 7750, 116790, 5.63], n t t n[&#34;20200207&#34;, 7750, 7790, 7580, 7670, 182278, 5.78], n t t n[&#34;20200210&#34;, 7680, 7690, 7490, 7620, 120898, 5.7], n t t n[&#34;20200211&#34;, 7650, 7680, 7570, 7650, 66183, 5.72], n t t n[&#34;20200212&#34;, 7650, 7690, 7540, 7630, 95648, 5.65], n t t n[&#34;20200213&#34;, 7660, 7670, 7540, 7580, 98128, 5.65], n t t n[&#34;20200214&#34;, 7600, 7700, 7550, 7590, 100164, 5.57], n t t n[&#34;20200217&#34;, 7600, 7610, 7510, 7550, 53442, 5.57], n t t n[&#34;20200218&#34;, 7520, 7670, 7380, 7390, 256144, 5.53], n t t n[&#34;20200219&#34;, 7420, 7480, 7390, 7450, 43486, 5.5], n t t n[&#34;20200220&#34;, 7520, 7540, 7260, 7330, 139692, 5.49], n t t n[&#34;20200221&#34;, 7330, 7350, 7090, 7180, 218425, 5.58], n t t n[&#34;20200224&#34;, 6960, 7090, 6750, 6800, 220736, 5.57], n t t n[&#34;20200225&#34;, 6800, 7040, 6590, 7020, 149025, 5.57], n t t n[&#34;20200226&#34;, 6980, 7170, 6840, 7030, 129260, 5.56], n t t n[&#34;20200227&#34;, 7090, 7340, 6990, 7030, 205047, 5.52], n t t n[&#34;20200228&#34;, 6860, 7030, 6670, 6830, 185609, 5.46], n t t n[&#34;20200302&#34;, 6740, 6900, 6710, 6830, 72101, 5.47], n t t n[&#34;20200303&#34;, 6950, 7040, 6860, 6970, 122598, 5.56], n t t n[&#34;20200304&#34;, 6960, 7100, 6920, 7030, 105803, 5.61], n t t n[&#34;20200305&#34;, 7090, 7300, 7070, 7230, 101494, 5.65], n t t n[&#34;20200306&#34;, 7190, 7330, 7120, 7250, 119515, 5.6], n t t n[&#34;20200309&#34;, 7230, 7230, 7030, 7060, 85174, 5.57], n t t n[&#34;20200310&#34;, 6920, 7070, 6870, 6940, 172889, 5.51], n t t n[&#34;20200311&#34;, 6980, 7110, 6730, 6770, 156412, 5.49], n t t n[&#34;20200312&#34;, 6760, 6760, 6300, 6420, 398356, 5.44], n t t n[&#34;20200313&#34;, 5960, 6150, 5560, 5870, 357670, 5.35], n t t n[&#34;20200316&#34;, 5870, 6050, 5700, 5710, 359328, 5.46], n t t n[&#34;20200317&#34;, 5250, 5730, 5250, 5570, 148121, 5.52], n t t n[&#34;20200318&#34;, 5630, 5780, 5420, 5420, 155883, 5.54], n t t n[&#34;20200319&#34;, 5630, 6450, 4800, 4985, 504037, 5.44], n t t n[&#34;20200320&#34;, 5010, 5290, 5000, 5190, 206771, 5.49], n t t n[&#34;20200323&#34;, 4900, 5170, 4850, 5020, 134269, 5.56], n t t n[&#34;20200324&#34;, 5170, 5570, 5100, 5330, 202739, 5.55], n t t n[&#34;20200325&#34;, 5510, 5670, 5440, 5630, 175934, 5.71], n t t n[&#34;20200326&#34;, 5720, 6050, 5540, 5900, 282680, 5.75], n t t n[&#34;20200327&#34;, 6300, 6300, 5900, 6130, 193283, 5.75], n t t n[&#34;20200330&#34;, 6130, 6350, 6000, 6350, 190170, 5.84], n t t n[&#34;20200331&#34;, 6400, 6650, 6360, 6550, 143491, 5.83], n t t n[&#34;20200401&#34;, 6610, 6780, 6500, 6580, 163650, 5.84], n t t n[&#34;20200402&#34;, 6520, 6680, 6420, 6610, 146475, 5.89], n t t n[&#34;20200403&#34;, 6680, 6930, 6580, 6830, 161419, 6.01], n t t n[&#34;20200406&#34;, 6930, 7220, 6930, 7190, 218593, 6.15], n t t n[&#34;20200407&#34;, 7300, 7450, 6930, 7430, 179098, 6.18], n t t n[&#34;20200408&#34;, 7430, 7700, 7310, 7650, 201101, 6.2], n t t n[&#34;20200409&#34;, 7750, 7750, 7530, 7650, 202716, 6.19], n t t n[&#34;20200410&#34;, 7700, 7860, 7310, 7730, 238244, 6.19], n t t n[&#34;20200413&#34;, 7730, 7750, 7490, 7490, 128951, 6.13], n t t n[&#34;20200414&#34;, 7590, 7590, 7420, 7500, 145355, 6.24], n t t n[&#34;20200416&#34;, 7420, 7630, 7330, 7630, 119442, 6.32], n t t n[&#34;20200417&#34;, 7650, 7730, 7540, 7720, 139320, 6.3], n t t n[&#34;20200420&#34;, 8700, 8700, 7850, 8050, 800932, 6.2], n t t n[&#34;20200421&#34;, 8240, 10450, 8220, 10450, 11147681, 5.48], n t t n[&#34;20200422&#34;, 11950, 12450, 10650, 10750, 12377730, 5.0], n t t n[&#34;20200423&#34;, 10800, 11500, 10600, 10900, 2543063, 4.98], n t t n[&#34;20200424&#34;, 10800, 10800, 10150, 10150, 697323, 4.96], n t t n[&#34;20200427&#34;, 10100, 10550, 10100, 10450, 487432, 4.91], n t t n[&#34;20200428&#34;, 10950, 12350, 10950, 11900, 1450030, 4.87], n t t n[&#34;20200429&#34;, 11650, 11650, 10900, 11250, 1007942, 4.93], n t t n[&#34;20200504&#34;, 10650, 10800, 10250, 10600, 710210, 5.18], n t t n[&#34;20200506&#34;, 10850, 10850, 10200, 10600, 771765, 4.93], n t t n[&#34;20200507&#34;, 10300, 10650, 10250, 10400, 534871, 5.19], n t t n[&#34;20200508&#34;, 10750, 11100, 10350, 10500, 996717, 5.0], n t t n[&#34;20200511&#34;, 10600, 10600, 10150, 10300, 469424, 5.11], n t t n[&#34;20200512&#34;, 10350, 10650, 10150, 10600, 617130, 5.08], n t t n[&#34;20200513&#34;, 10650, 10650, 10300, 10350, 380200, 5.02], n t t n[&#34;20200514&#34;, 10250, 10900, 10100, 10700, 1355080, 5.04], n t t n[&#34;20200515&#34;, 10850, 10950, 10450, 10550, 680362, 4.86], n t t n[&#34;20200518&#34;, 10550, 10800, 10200, 10700, 622190, 5.02], n t t n[&#34;20200519&#34;, 10500, 10500, 9910, 10050, 758107, 5.09], n t t n[&#34;20200520&#34;, 10100, 10200, 9990, 10050, 424923, 5.19], n t t n[&#34;20200521&#34;, 10050, 10150, 9710, 9710, 618577, 5.47], n t t n[&#34;20200522&#34;, 9760, 10250, 9760, 10250, 740048, 5.92], n t t n[&#34;20200525&#34;, 10250, 10450, 10000, 10300, 453897, 5.69], n t t n[&#34;20200526&#34;, 10400, 10400, 10050, 10150, 250626, 5.61], n t t n[&#34;20200527&#34;, 10650, 12450, 10600, 11300, 6795894, 5.2], n t t n[&#34;20200528&#34;, 11350, 11350, 10450, 10800, 1106251, 5.27], n t t n[&#34;20200529&#34;, 10900, 12150, 10850, 11550, 4470917, 5.28], n t t n[&#34;20200601&#34;, 11750, 11900, 11450, 11600, 1547269, 5.24], n t t n[&#34;20200602&#34;, 11600, 11800, 10950, 11750, 1171851, 5.27], n t t n[&#34;20200603&#34;, 12100, 12100, 11450, 11600, 1193172, 5.21], n t t n[&#34;20200604&#34;, 11850, 11850, 11250, 11350, 975698, 5.31], n t t n[&#34;20200605&#34;, 12200, 14750, 11900, 14750, 17523169, 5.75], n t t n[&#34;20200608&#34;, 14950, 16750, 14550, 15400, 15837277, 4.97], n t t n[&#34;20200609&#34;, 15400, 17500, 14850, 16100, 13134543, 5.08], n t t n[&#34;20200610&#34;, 16000, 16750, 15750, 16100, 2860356, 5.07], n t t n[&#34;20200611&#34;, 16050, 16100, 15150, 15200, 2015446, 5.26], n t t n[&#34;20200612&#34;, 14650, 15800, 14500, 15350, 1597695, 5.35], n t t n[&#34;20200615&#34;, 15600, 16450, 14100, 14650, 2388709, 5.37], n t t n[&#34;20200616&#34;, 15200, 16950, 14950, 15950, 3693462, 5.29], n t t n[&#34;20200617&#34;, 16900, 17300, 15400, 15750, 2449757, 5.1], n t t n[&#34;20200618&#34;, 15850, 16050, 15450, 15950, 1104813, 4.93], n t t n[&#34;20200619&#34;, 16050, 16050, 15050, 15700, 857391, 4.72], n t t n[&#34;20200622&#34;, 15550, 16200, 15500, 16000, 793242, 4.71], n t t n[&#34;20200623&#34;, 16100, 18100, 15700, 16850, 4128400, 4.83], n t t n[&#34;20200624&#34;, 17050, 17950, 16700, 16900, 2410334, 4.62], n t t n[&#34;20200625&#34;, 16700, 17850, 16500, 17500, 2015059, 4.85], n t t n[&#34;20200626&#34;, 17750, 17800, 16250, 17000, 1218309, 4.32], n t t n[&#34;20200629&#34;, 16800, 17300, 16450, 16700, 699133, 4.24], n t t n[&#34;20200630&#34;, 17150, 17150, 15600, 16250, 1096630, 4.16], n t t n[&#34;20200701&#34;, 16100, 16600, 15150, 15200, 947834, 4.34], n t t n[&#34;20200702&#34;, 15200, 15550, 14400, 15150, 1211802, 5.02], n t t n[&#34;20200703&#34;, 15050, 15250, 14750, 15050, 536560, 5.06], n t t n[&#34;20200706&#34;, 15050, 15300, 14700, 15150, 530324, 4.88], n t t n[&#34;20200707&#34;, 15150, 15250, 14850, 15100, 481223, 4.57], n t t n[&#34;20200708&#34;, 15050, 15550, 14900, 15200, 667288, 4.42], n t t n[&#34;20200709&#34;, 15450, 16050, 15300, 16050, 1315716, 4.19], n t t n[&#34;20200710&#34;, 16350, 16700, 15900, 16350, 1179178, 3.99], n t t n[&#34;20200713&#34;, 16500, 16600, 15950, 16000, 553258, 3.97], n t t n[&#34;20200714&#34;, 16000, 17000, 16000, 16800, 1670969, 3.97], n t t n[&#34;20200715&#34;, 16700, 16700, 15700, 15700, 1236148, 3.88], n t t n[&#34;20200716&#34;, 15750, 15850, 15050, 15200, 728717, 3.9], n t t n[&#34;20200717&#34;, 15050, 16050, 15050, 15550, 708937, 3.82], n t t n[&#34;20200720&#34;, 15750, 16400, 15650, 16300, 847845, 3.97], n t t n[&#34;20200721&#34;, 16150, 17350, 16100, 16750, 1892728, 3.75], n t t n[&#34;20200722&#34;, 16950, 21750, 16450, 21650, 10002002, 3.99], n t t n[&#34;20200723&#34;, 20650, 27000, 20050, 23700, 17433401, 4.08], n t t n[&#34;20200724&#34;, 24550, 25900, 23050, 23050, 4381522, 4.17], n t t n[&#34;20200727&#34;, 23050, 24300, 20750, 23000, 2495898, 4.26], n t t n[&#34;20200728&#34;, 23000, 24450, 23000, 23450, 1924723, 3.93], n t t n[&#34;20200729&#34;, 23500, 24650, 23450, 24050, 1828787, 3.92], n t t n[&#34;20200730&#34;, 24400, 24450, 22100, 23300, 1021338, 3.86], n t t n[&#34;20200731&#34;, 23450, 24850, 22950, 24400, 2271790, 4.09], n t t n[&#34;20200803&#34;, 24700, 25600, 23650, 24050, 1808460, 3.64], n t t n[&#34;20200804&#34;, 24300, 26000, 23450, 26000, 2884139, 4.27], n t t n[&#34;20200805&#34;, 25500, 31550, 25100, 28100, 11359853, 3.29], n t t n[&#34;20200806&#34;, 27750, 29650, 27600, 28350, 2129537, 3.45], n t t n[&#34;20200807&#34;, 29250, 29800, 28450, 29350, 1525232, 3.52], n t t n[&#34;20200810&#34;, 29650, 31300, 29150, 30300, 1476226, 3.54], n t t n[&#34;20200811&#34;, 30350, 34450, 23750, 24400, 11121453, 3.73], n t t n[&#34;20200812&#34;, 25300, 26700, 24250, 24950, 3375125, 3.7], n t t n[&#34;20200813&#34;, 24750, 25150, 21700, 22550, 3196774, 3.81], n t t n[&#34;20200814&#34;, 22400, 22400, 20700, 21200, 2252686, 4.95], n t t n[&#34;20200818&#34;, 21350, 25000, 21350, 22000, 4866233, 3.7], n t t n[&#34;20200819&#34;, 22800, 23900, 22250, 22900, 2644150, 3.66], n t t n[&#34;20200820&#34;, 23600, 24200, 22450, 22850, 1961789, 3.73], n t t n[&#34;20200821&#34;, 23000, 23200, 21500, 21650, 999529, 3.74], n t t n[&#34;20200824&#34;, 21700, 24050, 21100, 23100, 2414561, 3.84], n t t n[&#34;20200825&#34;, 22900, 23550, 22750, 23250, 1096492, 3.85], n t t n[&#34;20200826&#34;, 23300, 24200, 23000, 23750, 1572466, 3.73], n t t n[&#34;20200827&#34;, 23900, 24700, 23300, 23750, 1374591, 3.68], n t t n[&#34;20200828&#34;, 23750, 23900, 23000, 23050, 731100, 3.77], n t t n[&#34;20200831&#34;, 23200, 24000, 23000, 23450, 786605, 4.02], n t t n[&#34;20200901&#34;, 23500, 23900, 23350, 23650, 604784, 4.18], n t t n[&#34;20200902&#34;, 23850, 25250, 23450, 24950, 2551151, 4.1], n t t n[&#34;20200903&#34;, 25500, 26350, 25000, 25350, 1628037, 3.77], n t t n[&#34;20200904&#34;, 24150, 25450, 24100, 25200, 1072478, 3.77], n t t n[&#34;20200907&#34;, 25600, 25700, 25100, 25550, 813108, 3.8], n t t n[&#34;20200908&#34;, 25650, 28250, 22300, 23650, 5096054, 4.03], n t t n[&#34;20200909&#34;, 23900, 25150, 22750, 24550, 2313031, 3.81], n t t n[&#34;20200910&#34;, 25500, 26100, 24700, 25950, 1809380, 3.74], n t t n[&#34;20200911&#34;, 28200, 29600, 26150, 26750, 3734243, 3.69], n t t n[&#34;20200914&#34;, 26850, 26850, 25450, 26300, 809547, 3.75], n t t n[&#34;20200915&#34;, 26200, 26950, 25300, 25400, 911902, 3.7], n t t n[&#34;20200916&#34;, 24850, 25700, 24700, 25050, 681274, 3.8], n t t n[&#34;20200917&#34;, 24800, 25850, 24700, 25300, 722838, 3.77], n t t n[&#34;20200918&#34;, 25000, 25450, 24750, 25050, 624243, 3.75], n t t n[&#34;20200921&#34;, 25300, 26200, 24600, 25050, 811682, 3.62], n t t n[&#34;20200922&#34;, 24600, 25050, 23500, 24050, 788037, 3.63], n t t n[&#34;20200923&#34;, 24150, 24400, 22500, 23150, 633571, 3.66], n t t n[&#34;20200924&#34;, 22450, 22800, 20950, 21100, 1097445, 3.92], n t t n[&#34;20200925&#34;, 21100, 22750, 20600, 22100, 612823, 3.85], n t t n[&#34;20200928&#34;, 22100, 22500, 21500, 22000, 442765, 3.7], n t t n[&#34;20200929&#34;, 22000, 22050, 20700, 21850, 551123, 3.76], n t t n[&#34;20201005&#34;, 22200, 24350, 21700, 24100, 1094057, 3.55], n t t n[&#34;20201006&#34;, 24100, 24350, 23350, 23750, 463912, 3.4], n t t n[&#34;20201007&#34;, 23500, 23550, 23100, 23500, 322425, 3.27], n t t n[&#34;20201008&#34;, 23600, 24850, 23600, 24150, 716608, 3.26], n t t n[&#34;20201012&#34;, 24500, 24950, 22950, 24000, 540131, 3.13], n t t n[&#34;20201013&#34;, 24200, 24550, 23900, 24300, 426678, 3.12], n t t n[&#34;20201014&#34;, 23900, 24200, 23600, 23850, 420287, 3.16], n t t n[&#34;20201015&#34;, 23850, 23850, 22450, 22750, 508031, 3.1], n t t n[&#34;20201016&#34;, 22750, 22950, 20750, 21550, 912125, 3.21], n t t n[&#34;20201019&#34;, 21350, 22900, 20750, 21300, 590550, 3.18], n t t n[&#34;20201020&#34;, 20850, 21600, 19850, 20300, 745574, 3.46], n t t n[&#34;20201021&#34;, 20200, 20500, 19850, 20100, 402212, 3.65], n t t n[&#34;20201022&#34;, 19850, 20050, 19000, 19200, 620779, 4.1], n t t n[&#34;20201023&#34;, 19500, 20600, 19350, 19800, 567794, 4.06], n t t n[&#34;20201026&#34;, 19500, 19750, 17000, 17100, 1338181, 4.49], n t t n[&#34;20201027&#34;, 16850, 18150, 15900, 17500, 854079, 5.28], n t t n[&#34;20201028&#34;, 17250, 19400, 17250, 18950, 757160, 5.52], n t t n[&#34;20201029&#34;, 18400, 19300, 18300, 19150, 424896, 5.46], n t t n[&#34;20201030&#34;, 19500, 19500, 17700, 17900, 459297, 5.12], n t t n[&#34;20201102&#34;, 17850, 18600, 17650, 18150, 297779, 5.35], n t t n[&#34;20201103&#34;, 18150, 18700, 17450, 18400, 357550, 5.69], n t t n[&#34;20201104&#34;, 18750, 18750, 17950, 18250, 285222, 5.89], n t t n[&#34;20201105&#34;, 18300, 18900, 18250, 18750, 273054, 6.2], n t t n[&#34;20201106&#34;, 19000, 19250, 18000, 18300, 379887, 5.99], n t t n[&#34;20201109&#34;, 18550, 19100, 18300, 18950, 321139, 6.38], n t t n[&#34;20201110&#34;, 19950, 20000, 18000, 18100, 912306, 5.52], n t t n[&#34;20201111&#34;, 18250, 18350, 16700, 17700, 690470, 5.54], n t t n[&#34;20201112&#34;, 17500, 18250, 17500, 17950, 339516, 5.83], n t t n[&#34;20201113&#34;, 17950, 18800, 17750, 18450, 561160, 6.04], n t t n[&#34;20201116&#34;, 18950, 19300, 18550, 18750, 728497, 5.85], n t t n[&#34;20201117&#34;, 18850, 18850, 17800, 17950, 648331, 5.29], n t t n[&#34;20201118&#34;, 17800, 18350, 17550, 18050, 481113, 5.38], n t t n[&#34;20201119&#34;, 18200, 18850, 18000, 18600, 730789, 6.02], n t t n[&#34;20201120&#34;, 18700, 21350, 18400, 19300, 2497361, 5.8], n t t n[&#34;20201123&#34;, 19400, 19400, 18600, 18600, 829449, 5.3], n t t n[&#34;20201124&#34;, 22150, 24150, 21000, 21000, 18110748, 3.56], n t t n[&#34;20201125&#34;, 21500, 21500, 19700, 19850, 3089628, 3.41], n t t n[&#34;20201126&#34;, 19900, 20250, 19300, 19750, 1114933, 3.36], n t t n[&#34;20201127&#34;, 19800, 20050, 19500, 19500, 635582, 3.3], n t t n[&#34;20201130&#34;, 19550, 19650, 19050, 19150, 854958, 3.25], n t t n[&#34;20201201&#34;, 19150, 19900, 19050, 19800, 956640, 3.19], n t t n[&#34;20201202&#34;, 20050, 20350, 19400, 19750, 875174, 3.12], n t t n[&#34;20201203&#34;, 19750, 19900, 19000, 19150, 701206, 3.08], n t t n[&#34;20201204&#34;, 19500, 19600, 18950, 19000, 543537, 3.08], n t t n[&#34;20201207&#34;, 19050, 19450, 18850, 19050, 535318, 3.16], n t t n[&#34;20201208&#34;, 19200, 19600, 18600, 18600, 687202, 2.94], n t t n[&#34;20201209&#34;, 18800, 18850, 17900, 18000, 831661, 2.96], n t t n[&#34;20201210&#34;, 17750, 18200, 17650, 18000, 386252, 3.02], n t t n[&#34;20201211&#34;, 18050, 18950, 17850, 18450, 1046707, 3.09], n t t n[&#34;20201214&#34;, 18650, 19850, 18450, 19550, 1828211, 3.02], n t t n[&#34;20201215&#34;, 19700, 19800, 19050, 19700, 812174, 2.96], n t t n[&#34;20201216&#34;, 19600, 19600, 18800, 19200, 612248, 2.79], n t t n[&#34;20201217&#34;, 19000, 19100, 18300, 18900, 618956, 2.8], n t t n[&#34;20201218&#34;, 19200, 21900, 19150, 20450, 7400180, 2.67], n t t n[&#34;20201221&#34;, 22000, 24350, 21900, 23450, 10000048, 2.59], n t t n[&#34;20201222&#34;, 23950, 25500, 22000, 23900, 7361077, 2.57], n t t n[&#34;20201223&#34;, 23500, 24300, 20450, 20450, 2745447, 2.66], n t t n[&#34;20201224&#34;, 20300, 20500, 19500, 20000, 1222707, 3.06], n t t n[&#34;20201228&#34;, 20000, 20050, 18600, 18650, 1215378, 3.11], n t t n[&#34;20201229&#34;, 18750, 19400, 18750, 19150, 471430, 3.11], n t t n[&#34;20201230&#34;, 19100, 19800, 18800, 19650, 623218, 3.14], n t t n[&#34;20210104&#34;, 19750, 19900, 18850, 19100, 719076, 3.03], n t t n[&#34;20210105&#34;, 19000, 19500, 18650, 19400, 660993, 3.06], n t t n[&#34;20210106&#34;, 19350, 19850, 19000, 19700, 692445, 3.4], n t t n[&#34;20210107&#34;, 20000, 20100, 19500, 19700, 612986, 3.33], n t t n[&#34;20210108&#34;, 19850, 19900, 18950, 19100, 620141, 3.06], n t t n[&#34;20210111&#34;, 19000, 19100, 17850, 18100, 868959, 3.08], n t t n[&#34;20210112&#34;, 18000, 18500, 18000, 18100, 377078, 3.02], n t t n[&#34;20210113&#34;, 18000, 18350, 17900, 18000, 325304, 2.9], n t t n[&#34;20210114&#34;, 18100, 18850, 18000, 18450, 395979, 2.95], n t t n[&#34;20210115&#34;, 18600, 18700, 18000, 18050, 334730, 2.95], n t t n[&#34;20210118&#34;, 18000, 18050, 17050, 17050, 509279, 2.97], n t t n[&#34;20210119&#34;, 16900, 17200, 15900, 16550, 716826, 3.31], n t t n[&#34;20210120&#34;, 16550, 17250, 16550, 16900, 281517, 3.31], n t t n[&#34;20210121&#34;, 17100, 17200, 16650, 16700, 292345, 3.24], n t t n[&#34;20210122&#34;, 16600, 16650, 16250, 16400, 288262, 3.35], n t t n[&#34;20210125&#34;, 16450, 17000, 16400, 17000, 279789, 3.57], n t t n[&#34;20210126&#34;, 17150, 17150, 16500, 16700, 271418, 3.47], n t t n[&#34;20210127&#34;, 16700, 16850, 16300, 16350, 231488, 3.43], n t t n[&#34;20210128&#34;, 16200, 16200, 15750, 15750, 321083, 3.42], n t t n[&#34;20210129&#34;, 15900, 16000, 14750, 15050, 415174, 3.8], n t t n[&#34;20210201&#34;, 14800, 15700, 14800, 15600, 217267, 3.96], n t t n[&#34;20210202&#34;, 15650, 16050, 15550, 15800, 170148, 3.89], n t t n[&#34;20210203&#34;, 15850, 15950, 15500, 15700, 147509, 3.87], n t t n[&#34;20210204&#34;, 15700, 15750, 15250, 15300, 143047, 3.83], n t t n[&#34;20210205&#34;, 15450, 15750, 15200, 15500, 186469, 3.9], n t t n[&#34;20210208&#34;, 15800, 15800, 15300, 15400, 159256, 3.79], n t t n[&#34;20210209&#34;, 15700, 15700, 15350, 15400, 153252, 3.76], n t t n[&#34;20210210&#34;, 15050, 15650, 15050, 15350, 192189, 3.78], n t t n[&#34;20210215&#34;, 15450, 15800, 15150, 15700, 202964, 3.87], n t t n[&#34;20210216&#34;, 16000, 16150, 15600, 15700, 333293, 3.64], n t t n[&#34;20210217&#34;, 16700, 16850, 15750, 15800, 638207, 3.17], n t t n[&#34;20210218&#34;, 15650, 16050, 15500, 15650, 215478, 3.11], n t t n[&#34;20210219&#34;, 15650, 15700, 15100, 15400, 218109, 3.01], n t t n[&#34;20210222&#34;, 15300, 15500, 15100, 15150, 171635, 3.0], n t t n[&#34;20210223&#34;, 15000, 15100, 14400, 14400, 354880, 2.89], n t t n[&#34;20210224&#34;, 14350, 14600, 13750, 13750, 271570, 2.95], n t t n[&#34;20210225&#34;, 13900, 14450, 13800, 14450, 244951, 3.21], n t t n[&#34;20210226&#34;, 13950, 14500, 13800, 14500, 242941, 3.26], n t t n[&#34;20210302&#34;, 14750, 14750, 14100, 14300, 199410, 3.15], n t t n[&#34;20210303&#34;, 14150, 14500, 14100, 14300, 127587, 3.13], n t t n[&#34;20210304&#34;, 14200, 14250, 13600, 13750, 225778, 3.13], n t t n[&#34;20210305&#34;, 13600, 13950, 13400, 13950, 187692, 3.26], n t t n[&#34;20210308&#34;, 15200, 17200, 14400, 14400, 3201857, 2.84], n t t n[&#34;20210309&#34;, 14900, 15300, 14050, 14750, 607724, 2.88], n t t n[&#34;20210310&#34;, 14800, 14850, 14200, 14200, 242598, 2.86], n t t n[&#34;20210311&#34;, 14100, 14400, 14000, 14000, 217350, 2.89], n t t n[&#34;20210312&#34;, 14150, 14300, 14000, 14300, 152528, 2.96], n t t n[&#34;20210315&#34;, 14250, 14600, 14100, 14350, 176810, 2.94], n t t n[&#34;20210316&#34;, 14350, 14850, 14350, 14700, 229193, 3.1], n t t n[&#34;20210317&#34;, 14900, 15150, 14700, 15000, 229342, 3.15], n t t n[&#34;20210318&#34;, 15100, 15300, 14900, 14900, 227040, 3.06], n t t n[&#34;20210319&#34;, 14700, 15000, 14600, 14900, 140595, 3.17], n t t n[&#34;20210322&#34;, 15050, 15050, 14600, 14650, 194753, 3.0], n t t n[&#34;20210323&#34;, 14750, 14850, 14550, 14650, 178875, 3.08], n t t n[&#34;20210324&#34;, 14550, 14650, 14300, 14300, 181575, 3.09], n t t n[&#34;20210325&#34;, 14350, 14400, 14100, 14200, 120178, 3.09], n t t n[&#34;20210326&#34;, 14250, 14350, 14100, 14200, 98410, 3.07], n t t n[&#34;20210329&#34;, 14300, 14350, 14050, 14200, 112767, 3.11], n t t n[&#34;20210330&#34;, 14300, 14400, 14100, 14150, 117105, 3.05], n t t n[&#34;20210331&#34;, 14150, 14250, 14000, 14250, 142773, 3.03], n t t n[&#34;20210401&#34;, 14200, 14300, 14050, 14050, 146791, 2.96], n t t n[&#34;20210402&#34;, 14300, 14750, 14250, 14450, 323722, 2.98], n t t n[&#34;20210405&#34;, 14600, 14600, 14250, 14350, 106239, 3.02], n t t n[&#34;20210406&#34;, 14350, 14400, 14100, 14100, 138948, 3.0], n t t n[&#34;20210407&#34;, 14200, 14450, 14100, 14400, 119400, 3.13], n t t n[&#34;20210408&#34;, 14350, 15150, 14350, 14950, 473907, 3.21], n t t n[&#34;20210409&#34;, 15000, 15150, 14700, 15050, 205650, 3.19], n t t n[&#34;20210412&#34;, 15150, 15550, 15050, 15500, 300002, 3.28], n t t n[&#34;20210413&#34;, 15500, 15550, 15250, 15300, 186169, 3.2], n t t n[&#34;20210414&#34;, 15250, 15950, 15100, 15800, 433602, 3.27], n t t n[&#34;20210415&#34;, 15850, 15900, 15450, 15600, 204149, 3.18], n t t n[&#34;20210416&#34;, 15600, 15650, 15350, 15400, 208989, 3.23], n t t n[&#34;20210419&#34;, 15300, 15600, 15200, 15450, 169881, 3.28], n t t n[&#34;20210420&#34;, 15650, 15650, 15350, 15450, 196856, 3.36], n t t n[&#34;20210421&#34;, 15450, 16200, 15400, 15600, 576694, 3.57], n t t n[&#34;20210422&#34;, 15750, 15800, 15400, 15500, 239113, 3.42], n t t n[&#34;20210423&#34;, 15500, 16050, 15250, 15600, 351254, 3.49], n t t n[&#34;20210426&#34;, 15750, 16350, 15700, 16250, 645319, 3.58], n t t n[&#34;20210427&#34;, 16300, 16500, 16050, 16100, 355521, 3.38], n t t n[&#34;20210428&#34;, 16300, 16300, 15650, 15800, 415415, 3.19], n t t n[&#34;20210429&#34;, 15750, 15800, 15250, 15350, 249142, 3.09], n t t n[&#34;20210430&#34;, 15400, 15600, 15200, 15350, 160945, 3.06], n t t n[&#34;20210503&#34;, 15200, 15450, 14950, 14950, 210516, 3.05], n t t n[&#34;20210504&#34;, 14950, 15050, 14200, 14550, 301529, 3.18], n t t n[&#34;20210506&#34;, 14600, 14800, 14500, 14600, 143915, 3.13], n t t n[&#34;20210507&#34;, 14700, 14900, 14550, 14850, 130208, 3.12], n t t n[&#34;20210510&#34;, 15050, 15400, 14850, 15400, 219045, 3.19], n t t n[&#34;20210511&#34;, 15350, 15400, 15050, 15300, 134726, 3.18], n t t n[&#34;20210512&#34;, 15300, 15400, 14750, 14900, 130243, 3.12], n t t n[&#34;20210513&#34;, 14550, 15000, 14450, 14550, 161273, 3.14], n t t n[&#34;20210514&#34;, 14750, 14750, 14600, 14700, 98572, 3.15], n t t n[&#34;20210517&#34;, 14800, 15050, 14700, 15000, 137869, 3.18], n t t n[&#34;20210518&#34;, 15000, 15150, 14850, 14950, 106804, 3.21], n t t n[&#34;20210520&#34;, 15200, 15200, 14700, 14800, 143099, 3.17], n t t n[&#34;20210521&#34;, 14850, 14900, 14650, 14650, 110068, 3.19], n t t n[&#34;20210524&#34;, 14800, 14800, 14300, 14400, 127994, 3.13], n t t n[&#34;20210525&#34;, 14400, 14500, 14250, 14450, 118947, 3.11], n t t n[&#34;20210526&#34;, 14350, 14600, 14300, 14500, 71634, 3.13], n t t n[&#34;20210527&#34;, 14500, 14650, 14400, 14600, 87897, 3.14], n t t n[&#34;20210528&#34;, 14650, 14750, 14450, 14500, 136282, 3.15], n t t n[&#34;20210531&#34;, 14550, 14700, 14500, 14650, 99060, 3.2], n t t n[&#34;20210601&#34;, 14700, 14700, 14450, 14600, 114966, 3.13], n t t n[&#34;20210602&#34;, 14700, 14700, 14450, 14500, 109559, 3.1], n t t n[&#34;20210603&#34;, 14550, 14650, 14450, 14600, 96158, 3.08], n t t n[&#34;20210604&#34;, 14600, 14800, 14550, 14700, 133900, 3.09], n t t n[&#34;20210607&#34;, 14800, 15550, 14750, 15150, 511140, 3.27], n t t n[&#34;20210608&#34;, 15350, 15500, 15000, 15050, 272839, 3.1], n t t n[&#34;20210609&#34;, 15000, 15100, 14700, 14700, 168044, 2.97], n t t n[&#34;20210610&#34;, 14600, 15100, 14600, 15100, 176342, 3.06], n t t n[&#34;20210611&#34;, 15150, 15200, 14750, 14900, 146819, 2.98], n t t n[&#34;20210614&#34;, 14900, 14950, 14650, 14650, 107560, 2.93], n t t n[&#34;20210615&#34;, 14800, 14800, 14550, 14600, 142514, 2.85], n t t n[&#34;20210616&#34;, 14600, 14650, 14500, 14550, 202631, 2.85], n t t n[&#34;20210617&#34;, 14600, 14600, 14400, 14450, 123852, 2.74], n t t n[&#34;20210618&#34;, 14500, 14550, 14350, 14450, 116161, 2.67], n t t n[&#34;20210621&#34;, 14400, 14600, 14350, 14500, 123450, 2.69], n t t n[&#34;20210622&#34;, 14500, 14550, 14400, 14450, 99679, 2.61], n t t n[&#34;20210623&#34;, 14450, 14700, 14400, 14550, 186682, 2.61], n t t n[&#34;20210624&#34;, 14700, 16150, 14700, 15400, 1999686, 2.78], n t t n[&#34;20210625&#34;, 15550, 15600, 15200, 15400, 378323, 2.94], n t t n[&#34;20210628&#34;, 15600, 15800, 15400, 15750, 404835, 2.97], n t t n[&#34;20210629&#34;, 15800, 16100, 15600, 15650, 413276, 2.91], n t t n[&#34;20210630&#34;, 15650, 15800, 15450, 15650, 153804, 2.96], n t t n[&#34;20210701&#34;, 15700, 15900, 15450, 15800, 268235, 3.14], n t t n[&#34;20210702&#34;, 15900, 15950, 15600, 15750, 266741, 3.19], n t t n[&#34;20210705&#34;, 15850, 16350, 15800, 16000, 698678, 2.9], n t t n[&#34;20210706&#34;, 16100, 16200, 15600, 15700, 284953, 2.75], n t t n[&#34;20210707&#34;, 15750, 16350, 15550, 16100, 645979, 3.01], n t t n[&#34;20210708&#34;, 16100, 16150, 15300, 15350, 483775, 2.88], n t t n[&#34;20210709&#34;, 15200, 15250, 14800, 14950, 342502, 2.96], n t t n[&#34;20210712&#34;, 15000, 15150, 14900, 15050, 160334, 2.88], n t t n[&#34;20210713&#34;, 15050, 15100, 14850, 14850, 184810, 2.85], n t t n[&#34;20210714&#34;, 14950, 14950, 14700, 14750, 118825, 2.77], n t t n[&#34;20210715&#34;, 15000, 15000, 14750, 14850, 149813, 2.75], n t t n[&#34;20210716&#34;, 14850, 14950, 14650, 14800, 169898, 2.7], n t t n[&#34;20210719&#34;, 14800, 15200, 14700, 15100, 206899, 2.65], n t t n[&#34;20210720&#34;, 14950, 15050, 14800, 14900, 107379, 2.63], n t t n[&#34;20210721&#34;, 15050, 15050, 14750, 14800, 94116, 2.57], n t t n[&#34;20210722&#34;, 14750, 14850, 14650, 14750, 109511, 2.57], n t t n[&#34;20210723&#34;, 14750, 14900, 14500, 14600, 107504, 2.57], n t t n[&#34;20210726&#34;, 14700, 14700, 14350, 14400, 192180, 2.54], n t t n[&#34;20210727&#34;, 14500, 14550, 14350, 14400, 96069, 2.55], n t t n[&#34;20210728&#34;, 14300, 14600, 14300, 14450, 94912, 2.56], n t t n[&#34;20210729&#34;, 14500, 14650, 14500, 14600, 83468, 2.56], n t t n[&#34;20210730&#34;, 14650, 14750, 14400, 14650, 115604, 2.55] n t t n t n n]&#39; . json의 loads 메소드를 사용해서 데이터 형태를 아래와 같이 바꿔줍니다. . json.loads(result.text.replace(&quot;&#39;&quot;,&#39;&quot;&#39;).strip())[:5] . [[&#39;날짜&#39;, &#39;시가&#39;, &#39;고가&#39;, &#39;저가&#39;, &#39;종가&#39;, &#39;거래량&#39;, &#39;외국인소진율&#39;], [&#39;20160104&#39;, 8130, 8150, 7920, 8140, 281440, 7.45], [&#39;20160105&#39;, 8040, 8250, 8000, 8190, 243179, 7.49], [&#39;20160106&#39;, 8200, 8590, 8110, 8550, 609906, 7.63], [&#39;20160107&#39;, 8470, 8690, 8190, 8380, 704752, 7.59]] . 여기서부턴 그냥 pandas 데이터 프레임 만드는 것과 동일합니다. 정리하면 코드는 아래와 같습니다. . import requests import json import pandas as pd corp_code = &#39;000020&#39; startTime = 20160104 endTime = 20210730 timeframe = &#39;day&#39; url = f&#39;https://api.finance.naver.com/ siseJson.naver?symbol={corp_code}&amp; requestType=1&amp; startTime={startTime}&amp; endTime={endTime}&amp; timeframe={timeframe}&#39; result = requests.post(url) data = result.text.replace(&quot;&#39;&quot;,&#39;&quot;&#39;).strip() data = json.loads(data) data = pd.DataFrame(data[1:], columns=data[0]) data[&#39;날짜&#39;] = pd.to_datetime(data[&#39;날짜&#39;]) data[&#39;종목코드&#39;] = corp_code . data . 날짜 시가 고가 저가 종가 거래량 외국인소진율 종목코드 . 0 2016-01-04 | 8130 | 8150 | 7920 | 8140 | 281440 | 7.45 | 000020 | . 1 2016-01-05 | 8040 | 8250 | 8000 | 8190 | 243179 | 7.49 | 000020 | . 2 2016-01-06 | 8200 | 8590 | 8110 | 8550 | 609906 | 7.63 | 000020 | . 3 2016-01-07 | 8470 | 8690 | 8190 | 8380 | 704752 | 7.59 | 000020 | . 4 2016-01-08 | 8210 | 8900 | 8130 | 8770 | 802330 | 7.60 | 000020 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 1367 2021-07-26 | 14700 | 14700 | 14350 | 14400 | 192180 | 2.54 | 000020 | . 1368 2021-07-27 | 14500 | 14550 | 14350 | 14400 | 96069 | 2.55 | 000020 | . 1369 2021-07-28 | 14300 | 14600 | 14300 | 14450 | 94912 | 2.56 | 000020 | . 1370 2021-07-29 | 14500 | 14650 | 14500 | 14600 | 83468 | 2.56 | 000020 | . 1371 2021-07-30 | 14650 | 14750 | 14400 | 14650 | 115604 | 2.55 | 000020 | . 1372 rows × 8 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 셀레니움 라이브러리를 이용하는 것이 아니라 api를 이용해 크롤링을 한다면 네이버 금융 api 이외에도 데이터 api가 공개되어있는 사이트들을 크롤링 할 수 있습니다. .",
            "url": "https://edypidy.github.io/studyblog/code/%ED%81%AC%EB%A1%A4%EB%A7%81/api/%EB%84%A4%EC%9D%B4%EB%B2%84/%EB%84%A4%EC%9D%B4%EB%B2%84%20%EA%B8%88%EC%9C%B5/2021/12/26/%EB%84%A4%EC%9D%B4%EB%B2%84_%EA%B8%88%EC%9C%B5_api%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EA%B8%88%EC%9C%B5%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%81%AC%EB%A1%A4%EB%A7%81.html",
            "relUrl": "/code/%ED%81%AC%EB%A1%A4%EB%A7%81/api/%EB%84%A4%EC%9D%B4%EB%B2%84/%EB%84%A4%EC%9D%B4%EB%B2%84%20%EA%B8%88%EC%9C%B5/2021/12/26/%EB%84%A4%EC%9D%B4%EB%B2%84_%EA%B8%88%EC%9C%B5_api%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EA%B8%88%EC%9C%B5%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%81%AC%EB%A1%A4%EB%A7%81.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "[SSUDA][Kaggle Review] Predict Future Sales",
            "content": "Intro . 본 글은 kaggle의 Predict Future Sales 대회의 데이터를 사용하였으며 아래의 코드를 대부분 필사 및 의역하였습니다. . Model stacking, feature engineering and EDA | Deep Learning for Time Series Forecasting | Time-series forecasting with LSTM autoencoders | . Predict future sales . 가게 품목들의 다음 달 총 판매량을 예측하는 대회입니다. . 주어진 데이터는 매일의 판매량 데이터입니다. test 데이터 내 모든 가게들의 물건별 총 판매량을 예측해야합니다. . test 데이터 내의 가게들과 물건들은 매달 아주 살짝 달라지는 것을 주의해야합니다. 달라진다고 하더라도 깨지지 않는 모델을 잘 만드는 것이 필요하겠습니다. . Data Description . ID - an Id that represents a (Shop, Item) tuple within the test set | shop_id - unique identifier of a shop | item_id - unique identifier of a product | item_category_id - unique identifier of item category | item_cnt_day - number of products sold. You are predicting a monthly amount of this measure | item_price - current price of an item | date - date in format dd/mm/yyyy | date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33 | item_name - name of item | shop_name - name of shop | item_category_name - name of item category | . Load Libraries . from google.colab import drive drive.mount(&#39;/content/drive&#39;) import warnings, os, random import datetime import numpy as np import pandas as pd from tqdm import tqdm import seaborn as sns import matplotlib.pyplot as plt !pip install catboost from catboost import Pool from catboost import CatBoostRegressor from xgboost import XGBRegressor from xgboost import plot_importance from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.preprocessing import StandardScaler, MinMaxScaler from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split import tensorflow as tf import tensorflow.keras.layers as L from tensorflow.keras import optimizers, Sequential, Model from tensorflow.keras.utils import plot_model from keras.layers.convolutional import Conv1D, MaxPooling1D from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten import plotly as py import plotly.graph_objs as go from plotly.offline import init_notebook_mode, iplot %matplotlib inline sns.set_palette(&#39;deep&#39;) sns.set_color_codes() sns.set(style=&#39;darkgrid&#39;) warnings.filterwarnings(action=&#39;ignore&#39;) pd.set_option(&#39;display.float_format&#39;, lambda x: &#39;%.2f&#39; % x) init_notebook_mode(connected=True) # 재현가능한 실험을 위해 Set seed 함수를 만듭니다. def seed_everything(seed=0): random.seed(seed) np.random.seed(seed) tf.random.set_seed(seed) os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) os.environ[&#39;TF_DETERMINISTIC_OPS&#39;] = &#39;1&#39; seed = 0 seed_everything(seed) ## Set Drive Path path=&quot;/content/drive/MyDrive/my_data/kaggle_study/Predict_Future_Sales/&quot; . . Load Data . sales = pd.read_csv(path + &#39;sales_train.csv&#39;, parse_dates=[&#39;date&#39;], dtype={&#39;date&#39;: &#39;str&#39;, &#39;date_block_num&#39;: &#39;int32&#39;, &#39;shop_id&#39;: &#39;int32&#39;, &#39;item_id&#39;: &#39;int32&#39;, &#39;item_price&#39;: &#39;float32&#39;, &#39;item_cnt_day&#39;: &#39;int32&#39;}) test = pd.read_csv(path + &#39;test.csv&#39;, dtype={&#39;ID&#39;: &#39;int32&#39;, &#39;shop_id&#39;: &#39;int32&#39;}) shops = pd.read_csv(path + &#39;shops.csv&#39;, dtype={&#39;shop_name&#39;: &#39;str&#39;, &#39;shop_id&#39;: &#39;int32&#39;}) items = pd.read_csv(path + &#39;items.csv&#39;, dtype={&#39;item_name&#39;: &#39;str&#39;, &#39;item_id&#39;: &#39;int32&#39;, &#39;item_category_id&#39;: &#39;int32&#39;}) item_categories = pd.read_csv(path + &#39;item_categories.csv&#39;, dtype={&#39;item_category_name&#39;: &#39;str&#39;, &#39;item_category_id&#39;: &#39;int32&#39;}) . . # Join data sets train = sales.join(items, on=&#39;item_id&#39;, rsuffix=&#39;_&#39;).join(shops, on=&#39;shop_id&#39;, rsuffix=&#39;_&#39;).join(item_categories, on=&#39;item_category_id&#39;, rsuffix=&#39;_&#39;) train.drop([&#39;item_id_&#39;, &#39;shop_id_&#39;, &#39;item_category_id_&#39;], axis=1, inplace=True) . . Peek . print(&#39;Train Shape : &#39;, train.shape) . . Train Shape : (2935849, 10) . train.head().T . . 0 1 2 3 4 . date 2013-02-01 00:00:00 | 2013-03-01 00:00:00 | 2013-05-01 00:00:00 | 2013-06-01 00:00:00 | 2013-01-15 00:00:00 | . date_block_num 0 | 0 | 0 | 0 | 0 | . shop_id 59 | 25 | 25 | 25 | 25 | . item_id 22154 | 2552 | 2552 | 2554 | 2555 | . item_price 999.00 | 899.00 | 899.00 | 1709.05 | 1099.00 | . item_cnt_day 1 | 1 | -1 | 1 | 1 | . item_name ЯВЛЕНИЕ 2012 (BD) | DEEP PURPLE The House Of Blue Light LP | DEEP PURPLE The House Of Blue Light LP | DEEP PURPLE Who Do You Think We Are LP | DEEP PURPLE 30 Very Best Of 2CD (Фирм.) | . item_category_id 37 | 58 | 58 | 58 | 56 | . shop_name Ярославль ТЦ &quot;Альтаир&quot; | Москва ТРК &quot;Атриум&quot; | Москва ТРК &quot;Атриум&quot; | Москва ТРК &quot;Атриум&quot; | Москва ТРК &quot;Атриум&quot; | . item_category_name Кино - Blu-Ray | Музыка - Винил | Музыка - Винил | Музыка - Винил | Музыка - CD фирменного производства | . train.describe() . . date_block_num shop_id item_id item_price item_cnt_day item_category_id . count 2935849.00 | 2935849.00 | 2935849.00 | 2935849.00 | 2935849.00 | 2935849.00 | . mean 14.57 | 33.00 | 10197.23 | 890.62 | 1.24 | 40.00 | . std 9.42 | 16.23 | 6324.30 | 1726.44 | 2.62 | 17.10 | . min 0.00 | 0.00 | 0.00 | -1.00 | -22.00 | 0.00 | . 25% 7.00 | 22.00 | 4476.00 | 249.00 | 1.00 | 28.00 | . 50% 14.00 | 31.00 | 9343.00 | 399.00 | 1.00 | 40.00 | . 75% 23.00 | 47.00 | 15684.00 | 999.00 | 1.00 | 55.00 | . max 33.00 | 59.00 | 22169.00 | 307980.00 | 2169.00 | 83.00 | . print(&#39;Min date from train set : %s&#39;%train[&#39;date&#39;].min().date()) print(&#39;Max date from train set : %s&#39;%train[&#39;date&#39;].max().date()) . . Min date from train set : 2013-01-01 Max date from train set : 2015-12-10 . Caution : Data leakages . Data leakage에 대해선 test set에 나와있는 &#39;shop_id&#39;. &#39;item_id&#39;만을 사용하겠습니다. . test_shop_ids = test[&#39;shop_id&#39;].unique() test_item_ids = test[&#39;item_id&#39;].unique() # test set에만 존재하는 shops lk_train = train[train[&#39;shop_id&#39;].isin(test_shop_ids)] # test set에만 존재하는 items lk_train = lk_train[lk_train[&#39;item_id&#39;].isin(test_item_ids)] . print(&#39;Data set size before leaking : &#39;, train.shape[0]) print(&#39;Data set size after leaking : &#39;, lk_train.shape[0]) . Data set size before leaking : 2935849 Data set size after leaking : 1224439 . Data cleaning . &#39;item_price &gt; 0&#39; 이어야합니다 . train = train.query(&#39;item_price &gt; 0&#39;) . Data preprocessing . text 피쳐들로는 아무것도 하지 않을 것이니 드롭해줍니다. | 문제는 다음 달의 총 판매량 예측을 하는 것입니다. 하지만 주어진 데이터는 일별 데이터입니다. 원치않는 컬럼은 제거하고 월 단위로 합쳐보겠습니다. | . train_monthly = lk_train[[&#39;date&#39;, &#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_category_id&#39;, &#39;item_id&#39;, &#39;item_price&#39;, &#39;item_cnt_day&#39;]] . train_monthly = train_monthly.sort_values(&#39;date&#39;).groupby([&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_category_id&#39;, &#39;item_id&#39;], as_index=False) train_monthly = train_monthly.agg({&#39;item_price&#39; : [&#39;sum&#39;, &#39;mean&#39;], &#39;item_cnt_day&#39; : [&#39;sum&#39;, &#39;mean&#39;, &#39;count&#39;]}) # agg함수 애용해야겠습니다.. # Rename Features train_monthly.columns = [&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_category_id&#39;, &#39;item_id&#39;, &#39;item_price&#39;, &#39;mean_item_price&#39;, &#39;item_cnt&#39;, &#39;mean_item_cnt&#39;, &#39;transactions&#39;] . 실제 데이터와 유사하게 하기위해서 로드된 데이터 세트로부터 결측값을 생성할 필요가 있습니다. 그러니 각 shop과 item마다 결측된 데이터를 생성해줍시다.(딱히 결측된 데이터를 갖고있는 것은 아니니 0으로 대체해봅니다.) . shop_ids = train_monthly[&#39;shop_id&#39;].unique() item_ids = train_monthly[&#39;item_id&#39;].unique() empty_df = [] for i in range(34): # 2013-Jan:0 ~ 2015-Dec:33 for shop in shop_ids: for item in item_ids: empty_df.append([i, shop, item]) empty_df = pd.DataFrame(empty_df, columns=[&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;]) . train_monthly = pd.merge(empty_df, train_monthly, on=[&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;], how=&#39;left&#39;) train_monthly.fillna(0, inplace=True) . 이렇게 함으로써 (shop, item) 쌍의 데이터 형태가 동일해집니다. 새로운 데이터 세트를 한번 봅시다. . train_monthly.head().T . 0 1 2 3 4 . date_block_num 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . shop_id 2.00 | 2.00 | 2.00 | 2.00 | 2.00 | . item_id 5572.00 | 5643.00 | 5583.00 | 7893.00 | 7894.00 | . item_category_id 2.00 | 2.00 | 5.00 | 6.00 | 6.00 | . item_price 10730.00 | 4775.21 | 1188.30 | 5970.00 | 1490.00 | . mean_item_price 1532.86 | 2387.60 | 594.15 | 1990.00 | 1490.00 | . item_cnt 9.00 | 0.00 | 2.00 | 3.00 | 1.00 | . mean_item_cnt 1.29 | 0.00 | 1.00 | 1.00 | 1.00 | . transactions 7.00 | 2.00 | 2.00 | 3.00 | 1.00 | . train_monthly.describe().T . count mean std min 25% 50% 75% max . date_block_num 6734448.00 | 16.50 | 9.81 | 0.00 | 8.00 | 16.50 | 25.00 | 33.00 | . shop_id 6734448.00 | 31.64 | 17.56 | 2.00 | 16.00 | 34.50 | 47.00 | 59.00 | . item_id 6734448.00 | 11041.89 | 6210.74 | 30.00 | 5385.25 | 11265.50 | 16068.25 | 22167.00 | . item_category_id 6734448.00 | 3.79 | 13.21 | 0.00 | 0.00 | 0.00 | 0.00 | 83.00 | . item_price 6734448.00 | 187.35 | 2171.80 | 0.00 | 0.00 | 0.00 | 0.00 | 515573.59 | . mean_item_price 6734448.00 | 81.13 | 531.16 | 0.00 | 0.00 | 0.00 | 0.00 | 42990.00 | . item_cnt 6734448.00 | 0.24 | 3.46 | -4.00 | 0.00 | 0.00 | 0.00 | 2253.00 | . mean_item_cnt 6734448.00 | 0.10 | 0.61 | -2.00 | 0.00 | 0.00 | 0.00 | 1000.00 | . transactions 6734448.00 | 0.18 | 0.90 | 0.00 | 0.00 | 0.00 | 0.00 | 31.00 | . train_monthly[&#39;year&#39;] = train_monthly[&#39;date_block_num&#39;].apply(lambda x: ((x//12)+2013)) train_monthly[&#39;month&#39;] = train_monthly[&#39;date_block_num&#39;].apply(lambda x: (x % 12)) . train_monthly . date_block_num shop_id item_id item_category_id item_price mean_item_price item_cnt mean_item_cnt transactions year month . 0 0 | 2 | 5572 | 2.00 | 10730.00 | 1532.86 | 9.00 | 1.29 | 7.00 | 2013 | 0 | . 1 0 | 2 | 5643 | 2.00 | 4775.21 | 2387.60 | 0.00 | 0.00 | 2.00 | 2013 | 0 | . 2 0 | 2 | 5583 | 5.00 | 1188.30 | 594.15 | 2.00 | 1.00 | 2.00 | 2013 | 0 | . 3 0 | 2 | 7893 | 6.00 | 5970.00 | 1990.00 | 3.00 | 1.00 | 3.00 | 2013 | 0 | . 4 0 | 2 | 7894 | 6.00 | 1490.00 | 1490.00 | 1.00 | 1.00 | 1.00 | 2013 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6734443 33 | 36 | 9103 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | . 6734444 33 | 36 | 9107 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | . 6734445 33 | 36 | 5704 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | . 6734446 33 | 36 | 12733 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | . 6734447 33 | 36 | 15925 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | . 6734448 rows × 11 columns . EDA . # Grouping data for EDA gp_month_mean = train_monthly.groupby([&#39;month&#39;], as_index=False)[&#39;item_cnt&#39;].mean() gp_month_sum = train_monthly.groupby([&#39;month&#39;], as_index=False)[&#39;item_cnt&#39;].sum() gp_category_mean = train_monthly.groupby([&#39;item_category_id&#39;], as_index=False)[&#39;item_cnt&#39;].mean() gp_category_sum = train_monthly.groupby([&#39;item_category_id&#39;], as_index=False)[&#39;item_cnt&#39;].sum() gp_shop_mean = train_monthly.groupby([&#39;shop_id&#39;], as_index=False).mean() gp_shop_sum = train_monthly.groupby([&#39;shop_id&#39;], as_index=False).sum() . . fig, axes = plt.subplots(2, 1, figsize=(22,10), sharex=True) # sharex라는 option이 있었군요.. fig.patch.set_facecolor(&#39;#eaeaf2&#39;) sns.lineplot(x=&#39;month&#39;, y=&#39;item_cnt&#39;, data=gp_month_mean, ax=axes[0]).set_title(&#39;Monthly mean&#39;, fontsize=20, fontweight=&#39;bold&#39;) sns.lineplot(x=&#39;month&#39;, y=&#39;item_cnt&#39;, data=gp_month_sum, ax=axes[1]).set_title(&#39;Monthly sum&#39;, fontsize=20, fontweight=&#39;bold&#39;) . . Text(0.5, 1.0, &#39;Monthly sum&#39;) . 위에서 볼 수 있듯이 연말로 갈수록 item sales count가(mean) 커지는 경향이 있습니다. 그렇다면 어떤 category의 item이 더 많이 팔릴까요? . fig, axes = plt.subplots(2, 1, figsize=(22,10), sharex=True) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) sns.barplot(x=&#39;item_category_id&#39;, y=&#39;item_cnt&#39;, data=gp_category_mean, ax=axes[0], palette=&#39;rocket&#39;).set_title(&#39;Category mean&#39;, fontsize=20, fontweight=&#39;bold&#39;) sns.barplot(x=&#39;item_category_id&#39;, y=&#39;item_cnt&#39;, data=gp_category_sum, ax=axes[1], palette=&#39;rocket&#39;).set_title(&#39;Category sum&#39;, fontsize=20, fontweight=&#39;bold&#39;) . . Text(0.5, 1.0, &#39;Category sum&#39;) . 평균적으로 많이 팔린 몇몇의 item이 보이고 그에 반해 총 판매량이 큰 item들이 보입니다. 아무래도 몇몇 item이 대부분의 sell count를 차지하는 것 같습니다. . 이번에는 어떤 가게가 많은 판매량을 갖는지 보겠습니다. . fig, axes = plt.subplots(2, 1, figsize=(22,10), sharex=True) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) sns.barplot(x=&#39;shop_id&#39;, y=&#39;item_cnt&#39;, data=gp_shop_mean, ax=axes[0], palette=&#39;rocket&#39;).set_title(&#39;Shop mean&#39;, fontsize=20, fontweight=&#39;bold&#39;) sns.barplot(x=&#39;shop_id&#39;, y=&#39;item_cnt&#39;, data=gp_shop_sum, ax=axes[1], palette=&#39;rocket&#39;).set_title(&#39;Shop sum&#39;, fontsize=20, fontweight=&#39;bold&#39;) . . Text(0.5, 1.0, &#39;Shop sum&#39;) . 대부분의 shop들은 비슷한 비율의 판매율을 보이고, 세 군데 정도의 shop에서 높은 판매율을 보이네요. 아마 shop의 크기를 암시적으로 의미할 것 같습니다. . Checking for outliers . sns.jointplot(x=&#39;item_cnt&#39;, y=&#39;item_price&#39;, data=train_monthly, height=8) . . &lt;seaborn.axisgrid.JointGrid at 0x7fbc3c235410&gt; . sns.jointplot(x=&#39;item_cnt&#39;, y=&#39;transactions&#39;, data=train_monthly, height=8) . . &lt;seaborn.axisgrid.JointGrid at 0x7fbbc40a4490&gt; . &#39;item_cnt&#39;의 분포를 보겠습니다. . fig = plt.figure(figsize=(22,6)) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) sns.boxplot(train_monthly[&#39;item_cnt&#39;]) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbbc3eebf90&gt; . Removing outliers . &#39;item_cnt &gt; 20 and &lt; 0&#39;와 &#39;item_price &gt;= 400_000&#39;을 outliers로 잡고 제거하겠습니다. . 이 글에선 outlier에 대한 기준이 딱히 밝혀지진 않았습니다. 다른 근거를 통해 outlier를 재설정하여 분석하는 것도 괜찮아 보입니다. . train_monthly = train_monthly.query(&#39;item_cnt &gt;= 0 and item_cnt &lt;= 20 and item_price &lt; 400000&#39;) . Creating the label . 예측해야할 레이블은 다음달의 총 판매량입니다. . train_monthly[&#39;item_cnt_month&#39;] = train_monthly.sort_values(&#39;date_block_num&#39;).groupby([&#39;shop_id&#39;, &#39;item_id&#39;])[&#39;item_cnt&#39;].shift(-1) # pd.DataFrame.shift(n) 메소드는 n칸씩 뒤로 밀어줍니다. -1칸이면 앞으로 당깁니다. 아래의 결과를 보면 더욱 명확해집니다. . train_monthly.query(&#39;shop_id == 2 and item_id == 5572&#39;).head(3) . date_block_num shop_id item_id item_category_id item_price mean_item_price item_cnt mean_item_cnt transactions year month item_cnt_month . 0 0 | 2 | 5572 | 2.00 | 10730.00 | 1532.86 | 9.00 | 1.29 | 7.00 | 2013 | 0 | 1.00 | . 198072 1 | 2 | 5572 | 2.00 | 1590.00 | 1590.00 | 1.00 | 1.00 | 1.00 | 2013 | 1 | 1.00 | . 396144 2 | 2 | 5572 | 2.00 | 1490.00 | 1490.00 | 1.00 | 1.00 | 1.00 | 2013 | 2 | 2.00 | . Feature Engineering . item의 가격이 구매 개수에 따라 달라집니다. 단위를 맞추어 item price 컬럼을 만들겠습니다. . train_monthly[&#39;item_price_unit&#39;] = train_monthly[&#39;item_price&#39;] // train_monthly[&#39;item_cnt&#39;] train_monthly[&#39;item_price_unit&#39;].fillna(0, inplace=True) . item을 판매하는 곳은 한 곳이 아닙니다. 여러 가게들의 가격 동향을 반영하기 위해 최고가와 최저가 컬럼을 추가하겠습니다. . gp_item_price = train_monthly.sort_values(&#39;date_block_num&#39;).groupby([&#39;item_id&#39;], as_index=False).agg({&#39;item_price&#39;:[np.min, np.max]}) gp_item_price.columns = [&#39;item_id&#39;, &#39;hist_min_item_price&#39;, &#39;hist_max_item_price&#39;] train_monthly = pd.merge(train_monthly, gp_item_price, how=&#39;left&#39;, on=&#39;item_id&#39;) . 얼마나 item들의 과거 최저가, 최고가로부터 가격에 변동이 있는지 반영하겠습니다. . train_monthly[&#39;price_increase&#39;] = train_monthly[&#39;item_price&#39;] - train_monthly[&#39;hist_min_item_price&#39;] train_monthly[&#39;price_decrease&#39;] = train_monthly[&#39;hist_max_item_price&#39;] - train_monthly[&#39;item_price&#39;] . Rolling window 기반의 피쳐들을 생성합니다.(window = 3 months) . Rolling window는 시계열 데이터를 다룰때 자주 사용되는 기법입니다. window=3이니 원래라면, 맨 처음 2달간의 데이터엔 NA값이 생성됩니다. min_periods=1을 설정해줌으로써 첫달부터 NA값이 생성되지 않습니다. . f_min = lambda x: x.rolling(window=3, min_periods=1).min() # Max value f_max = lambda x: x.rolling(window=3, min_periods=1).max() # Mean value f_mean = lambda x: x.rolling(window=3, min_periods=1).mean() # Standard value f_std = lambda x: x.rolling(window=3, min_periods=1).std() function_list = [f_min, f_max, f_mean, f_std] function_name = [&#39;min&#39;, &#39;max&#39;, &#39;mean&#39;, &#39;std&#39;] for i in range(len(function_list)): train_monthly[(&#39;item_cnt_%s&#39; % function_name[i])] = train_monthly.sort_values(&#39;date_block_num&#39;).groupby([&#39;shop_id&#39;, &#39;item_category_id&#39;, &#39;item_id&#39;])[&#39;item_cnt&#39;].apply(function_list[i]) # std feature의 empty값을 0으로 채워줍니다. train_monthly[&#39;item_cnt_std&#39;].fillna(0, inplace=True) . Lag based features . 오늘의 게으름이 당장 내일의 인생에만 반영되지는 않습니다. 먼 미래에도 반영될 수 있는 것이죠. . lag_list = [1, 2, 3] for lag in lag_list: ft_name = (&#39;item_cnt_shifted%s&#39; % lag) train_monthly[ft_name] = train_monthly.sort_values(&#39;date_block_num&#39;).groupby([&#39;shop_id&#39;, &#39;item_category_id&#39;, &#39;item_id&#39;])[&#39;item_cnt&#39;].shift(lag) # shift된 feature들의 결측값을 0으로 대체합니다. train_monthly[ft_name].fillna(0, inplace=True) . Item sales count trend &gt; . train_monthly[&#39;item_trend&#39;] = train_monthly[&#39;item_cnt&#39;] for lag in lag_list: ft_name = (&#39;item_cnt_shifted%s&#39; % lag) train_monthly[&#39;item_trend&#39;] -= train_monthly[ft_name] # 다 더한다음에 나누는건 이해 가는데 빼는건 뭔가 싶지만 일단 넘어가겠습니다.. train_monthly[&#39;item_trend&#39;] /= len(lag_list) + 1 . Dataset after feature engineering . train_monthly.head().T . 0 1 2 3 4 . date_block_num 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . shop_id 2.00 | 2.00 | 2.00 | 2.00 | 2.00 | . item_id 5572.00 | 5643.00 | 5583.00 | 7893.00 | 7894.00 | . item_category_id 2.00 | 2.00 | 5.00 | 6.00 | 6.00 | . item_price 10730.00 | 4775.21 | 1188.30 | 5970.00 | 1490.00 | . mean_item_price 1532.86 | 2387.60 | 594.15 | 1990.00 | 1490.00 | . item_cnt 9.00 | 0.00 | 2.00 | 3.00 | 1.00 | . mean_item_cnt 1.29 | 0.00 | 1.00 | 1.00 | 1.00 | . transactions 7.00 | 2.00 | 2.00 | 3.00 | 1.00 | . year 2013.00 | 2013.00 | 2013.00 | 2013.00 | 2013.00 | . month 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . item_cnt_month 1.00 | 0.00 | 1.00 | 2.00 | 2.00 | . item_price_unit 1192.00 | inf | 594.00 | 1990.00 | 1490.00 | . hist_min_item_price 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . hist_max_item_price 18979.50 | 35260.00 | 5592.00 | 27950.00 | 25880.00 | . price_increase 10730.00 | 4775.21 | 1188.30 | 5970.00 | 1490.00 | . price_decrease 8249.50 | 30484.79 | 4403.70 | 21980.00 | 24390.00 | . item_cnt_min 9.00 | 0.00 | 2.00 | 3.00 | 1.00 | . item_cnt_max 9.00 | 0.00 | 2.00 | 3.00 | 1.00 | . item_cnt_mean 9.00 | 0.00 | 2.00 | 3.00 | 1.00 | . item_cnt_std 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . item_cnt_shifted1 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . item_cnt_shifted2 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . item_cnt_shifted3 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . item_trend 2.25 | 0.00 | 0.50 | 0.75 | 0.25 | . train_monthly.describe().T . count mean std min 25% 50% 75% max . date_block_num 6728118.00 | 16.50 | 9.81 | 0.00 | 8.00 | 16.00 | 25.00 | 33.00 | . shop_id 6728118.00 | 31.64 | 17.56 | 2.00 | 16.00 | 35.00 | 47.00 | 59.00 | . item_id 6728118.00 | 11042.87 | 6209.70 | 30.00 | 5386.00 | 11266.00 | 16068.00 | 22167.00 | . item_category_id 6728118.00 | 3.75 | 13.16 | 0.00 | 0.00 | 0.00 | 0.00 | 83.00 | . item_price 6728118.00 | 169.77 | 1792.01 | 0.00 | 0.00 | 0.00 | 0.00 | 366860.00 | . mean_item_price 6728118.00 | 79.71 | 522.27 | 0.00 | 0.00 | 0.00 | 0.00 | 42990.00 | . item_cnt 6728118.00 | 0.19 | 0.91 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . mean_item_cnt 6728118.00 | 0.09 | 0.31 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . transactions 6728118.00 | 0.17 | 0.74 | 0.00 | 0.00 | 0.00 | 0.00 | 19.00 | . year 6728118.00 | 2013.94 | 0.80 | 2013.00 | 2013.00 | 2014.00 | 2015.00 | 2015.00 | . month 6728118.00 | 5.21 | 3.32 | 0.00 | 2.00 | 5.00 | 8.00 | 11.00 | . item_cnt_month 6530047.00 | 0.19 | 0.91 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_price_unit 6728118.00 | inf | nan | 0.00 | 0.00 | 0.00 | 0.00 | inf | . hist_min_item_price 6728118.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . hist_max_item_price 6728118.00 | 5561.39 | 11482.78 | 0.00 | 1043.00 | 2408.00 | 5592.00 | 366860.00 | . price_increase 6728118.00 | 169.77 | 1792.01 | 0.00 | 0.00 | 0.00 | 0.00 | 366860.00 | . price_decrease 6728118.00 | 5396.74 | 11022.27 | 0.00 | 999.00 | 2396.00 | 5490.00 | 366860.00 | . item_cnt_min 6728118.00 | 0.14 | 0.69 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_max 6728118.00 | 0.26 | 1.25 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_mean 6728118.00 | 0.20 | 0.92 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_std 6728118.00 | 0.07 | 0.44 | 0.00 | 0.00 | 0.00 | 0.00 | 13.44 | . item_cnt_shifted1 6728118.00 | 0.16 | 0.88 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_shifted2 6728118.00 | 0.14 | 0.85 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_shifted3 6728118.00 | 0.13 | 0.82 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_trend 6728118.00 | -0.06 | 0.44 | -12.50 | 0.00 | 0.00 | 0.00 | 5.00 | . &#51104;&#44624; &#51200;&#51109;&#54616;&#44592; . 여기까지 하고 데이터를 잠시 저장하겠습니다.(여기서부터 다시 실험해볼 수 있어요.) . . train_monthly = pd.read_csv(path + &#39;train_monthly.csv&#39;) . Train/Validation split . test set는 미래의 데이터입니다. 그러므로 같은 분포에서 시뮬레이션 되게끔 train/validation set을 나눌 필요가 있습니다. | train set은 3~28 까지의 date_block(달)이 될 것입니다. validation은 마지막 5 date block(29~32)로 하겠습니다. 그리고 test date block은 33이 됩니다. | 처음 3개월은 드롭합니다. 이미 window feature를 만드는데 사용되어서 이 기간동안의 windowed features는 유용하지 않습니다. | . train_set = train_monthly.query(&#39;date_block_num &gt;= 3 and date_block_num &lt; 28&#39;) validation_set = train_monthly.query(&#39;date_block_num &gt;=28 and date_block_num &lt; 33&#39;) test_set = train_monthly.query(&#39;date_block_num == 33&#39;).copy() train_set.dropna(subset=[&#39;item_cnt_month&#39;], inplace=True) validation_set.dropna(subset=[&#39;item_cnt_month&#39;], inplace=True) train_set.dropna(inplace=True) validation_set.dropna(inplace=True) # 이렇게 다 dropna할거면 한꺼번에 해도 되지 않나 싶긴하다.. 그래도 뜻이 명확하니 좋다. print(&#39;Train set records : &#39;, train_set.shape[0]) print(&#39;Validation set records : &#39;, validation_set.shape[0]) print(&#39;Test set records : &#39;, test_set.shape[0]) print(&#39;Train set records: %s (%.f%% of complete data)&#39; % (train_set.shape[0], ((train_set.shape[0]/train_monthly.shape[0])*100))) print(&#39;Validation set records: %s (%.f%% of complete data)&#39; % (validation_set.shape[0], ((validation_set.shape[0]/train_monthly.shape[0])*100))) . . Train set records : 4946785 Validation set records : 989412 Test set records : 197879 Train set records: 4946785 (74% of complete data) Validation set records: 989412 (15% of complete data) . Mean encoding . train/validation split 이후에 처리해야 하는 과정입니다. . gp_shop_mean = train_set.groupby([&#39;shop_id&#39;]).agg({&#39;item_cnt_month&#39; : [&#39;mean&#39;]}) gp_shop_mean.columns = [&#39;shop_mean&#39;] gp_shop_mean.reset_index(inplace=True) # Item mean encoding gp_item_mean = train_set.groupby([&#39;item_id&#39;]).agg({&#39;item_cnt_month&#39; : [&#39;mean&#39;]}) gp_item_mean.columns = [&#39;item_mean&#39;] gp_item_mean.reset_index(inplace=True) # Shop with item mean encoding gp_shop_item_mean = train_set.groupby([&#39;shop_id&#39;, &#39;item_id&#39;]).agg({&#39;item_cnt_month&#39; : [&#39;mean&#39;]}) gp_shop_item_mean.columns = [&#39;shop_item_mean&#39;] gp_shop_item_mean.reset_index(inplace=True) # Year mean encoding gp_year_mean = train_set.groupby([&#39;year&#39;]).agg({&#39;item_cnt_month&#39; : [&#39;mean&#39;]}) gp_year_mean.columns = [&#39;year_mean&#39;] gp_year_mean.reset_index(inplace=True) # Month mean encoding gp_month_mean = train_set.groupby([&#39;month&#39;]).agg({&#39;item_cnt_month&#39; : [&#39;mean&#39;]}) gp_month_mean.columns = [&#39;month_mean&#39;] gp_month_mean.reset_index(inplace=True) # Add meand encoding features to train set. train_set = pd.merge(train_set, gp_shop_mean, how=&#39;left&#39;, on=[&#39;shop_id&#39;]) train_set = pd.merge(train_set, gp_item_mean, how=&#39;left&#39;, on=[&#39;item_id&#39;]) train_set = pd.merge(train_set, gp_shop_item_mean, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;item_id&#39;]) train_set = pd.merge(train_set, gp_year_mean, how=&#39;left&#39;, on=[&#39;year&#39;]) train_set = pd.merge(train_set, gp_month_mean, how=&#39;left&#39;, on=[&#39;month&#39;]) # Add meand encoding features to validation set. validation_set = pd.merge(validation_set, gp_shop_mean, how=&#39;left&#39;, on=[&#39;shop_id&#39;]) validation_set = pd.merge(validation_set, gp_item_mean, how=&#39;left&#39;, on=[&#39;item_id&#39;]) validation_set = pd.merge(validation_set, gp_shop_item_mean, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;item_id&#39;]) validation_set = pd.merge(validation_set, gp_year_mean, how=&#39;left&#39;, on=[&#39;year&#39;]) validation_set = pd.merge(validation_set, gp_month_mean, how=&#39;left&#39;, on=[&#39;month&#39;]) . X_train = train_set.drop([&#39;item_cnt_month&#39;, &#39;date_block_num&#39;], axis=1) Y_train = train_set[&#39;item_cnt_month&#39;].astype(int) X_validation = validation_set.drop([&#39;item_cnt_month&#39;, &#39;date_block_num&#39;], axis=1) Y_validation = validation_set[&#39;item_cnt_month&#39;].astype(int) . int_features = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;year&#39;, &#39;month&#39;] X_train[int_features] = X_train[int_features].astype(&#39;int32&#39;) X_validation[int_features] = X_validation[int_features].astype(&#39;int32&#39;) . Build test set . 우리는 &#39;date_block_num&#39; 34에 대한 예측을 하려 합니다.그렇기에 우리의 test set은 block 33이고 예측은 block 34에 대한 예측이 됩니다. 즉, 우리는 block 33을 34번쨰 block을 예측하기 위해 써야합니다. . latests_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=[&#39;shop_id&#39;, &#39;item_id&#39;], keep=&#39;last&#39;) X_test = pd.merge(test, latests_records, how=&#39;left&#39;, on=[&#39;shop_id&#39;, &#39;item_id&#39;], suffixes=[&#39;&#39;, &#39;_&#39;]) X_test[&#39;year&#39;] = 2015 X_test[&#39;month&#39;] = 9 X_test.drop(&#39;item_cnt_month&#39;, axis=1, inplace=True) X_test[int_features] = X_test[int_features].astype(&#39;int32&#39;) X_test = X_test[X_train.columns] . X_test . Unnamed: 0 shop_id item_id item_category_id item_price mean_item_price item_cnt mean_item_cnt transactions year month item_price_unit hist_min_item_price hist_max_item_price price_increase price_decrease item_cnt_min item_cnt_max item_cnt_mean item_cnt_std item_cnt_shifted1 item_cnt_shifted2 item_cnt_shifted3 item_trend shop_mean item_mean shop_item_mean year_mean month_mean . 0 6490255.00 | 5 | 5037 | 19.00 | 749.50 | 749.50 | 1.00 | 1.00 | 1.00 | 2015 | 9 | 749.00 | 0.00 | 25990.00 | 749.50 | 25240.50 | 1.00 | 3.00 | 1.67 | 1.15 | 3.00 | 1.00 | 1.00 | -1.00 | 0.13 | 0.70 | 0.28 | 0.22 | 0.17 | . 1 nan | 5 | 5320 | nan | nan | nan | nan | nan | nan | 2015 | 9 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . 2 6491376.00 | 5 | 5233 | 19.00 | 2997.00 | 999.00 | 3.00 | 1.00 | 3.00 | 2015 | 9 | 999.00 | 0.00 | 7191.75 | 2997.00 | 4194.75 | 1.00 | 3.00 | 2.00 | 1.00 | 1.00 | 2.00 | 3.00 | -0.75 | 0.13 | 0.07 | 0.12 | 0.22 | 0.17 | . 3 6491871.00 | 5 | 5232 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | 0.00 | 0.00 | 4796.00 | 0.00 | 4796.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.13 | 0.00 | 0.00 | 0.22 | 0.17 | . 4 nan | 5 | 5268 | nan | nan | nan | nan | nan | nan | 2015 | 9 | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | nan | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 214195 6453087.00 | 45 | 18454 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | 0.00 | 0.00 | 2189.00 | 0.00 | 2189.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.40 | 0.32 | 0.22 | 0.17 | . 214196 6454424.00 | 45 | 16188 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | 0.00 | 0.00 | 1359.00 | 0.00 | 1359.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.00 | 0.00 | 0.22 | 0.17 | . 214197 6450315.00 | 45 | 15757 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | 0.00 | 0.00 | 796.00 | 0.00 | 796.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.23 | 0.24 | 0.22 | 0.17 | . 214198 6453171.00 | 45 | 19648 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | 0.00 | 0.00 | 297.00 | 0.00 | 297.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.03 | 0.00 | 0.22 | 0.17 | . 214199 6452245.00 | 45 | 969 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 2015 | 9 | 0.00 | 0.00 | 5490.00 | 0.00 | 5490.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.12 | 0.20 | 0.20 | 0.22 | 0.17 | . 214200 rows × 29 columns . sets = [X_train, X_validation, X_test] # This was taking too long. # Replace missing values with the median of each item. # for dataset in sets: # for item_id in dataset[&#39;item_id&#39;].unique(): # for column in dataset.columns: # item_median = dataset[(dataset[&#39;item_id&#39;] == item_id)][column].median() # dataset.loc[(dataset[column].isnull()) &amp; (dataset[&#39;item_id&#39;] == item_id), column] = item_median # Replace missing values with the median of each shop. for dataset in sets: for shop_id in dataset[&#39;shop_id&#39;].unique(): for column in dataset.columns: shop_median = dataset[(dataset[&#39;shop_id&#39;] == shop_id)][column].median() dataset.loc[(dataset[column].isnull()) &amp; (dataset[&#39;shop_id&#39;] == shop_id), column] = shop_median # Fill remaining missing values on test set with mean. X_test.fillna(X_test.mean(), inplace=True) . test set에는 &#39;item_category_id&#39; 컬럼이 없습니다. 그렇다고 만들기는 까다로우니 드롭하겠습니다. . X_train.drop([&#39;item_category_id&#39;], axis=1, inplace=True) X_validation.drop([&#39;item_category_id&#39;], axis=1, inplace=True) X_test.drop([&#39;item_category_id&#39;], axis=1, inplace=True) . X_test.head().T . 0 1 2 3 4 . Unnamed: 0 6490255.00 | 6490184.50 | 6491376.00 | 6491871.00 | 6490184.50 | . shop_id 5.00 | 5.00 | 5.00 | 5.00 | 5.00 | . item_id 5037.00 | 5320.00 | 5233.00 | 5232.00 | 5268.00 | . item_price 749.50 | 0.00 | 2997.00 | 0.00 | 0.00 | . mean_item_price 749.50 | 0.00 | 999.00 | 0.00 | 0.00 | . item_cnt 1.00 | 0.00 | 3.00 | 0.00 | 0.00 | . mean_item_cnt 1.00 | 0.00 | 1.00 | 0.00 | 0.00 | . transactions 1.00 | 0.00 | 3.00 | 0.00 | 0.00 | . year 2015.00 | 2015.00 | 2015.00 | 2015.00 | 2015.00 | . month 9.00 | 9.00 | 9.00 | 9.00 | 9.00 | . item_price_unit 749.00 | 0.00 | 999.00 | 0.00 | 0.00 | . hist_min_item_price 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . hist_max_item_price 25990.00 | 2428.81 | 7191.75 | 4796.00 | 2428.81 | . price_increase 749.50 | 0.00 | 2997.00 | 0.00 | 0.00 | . price_decrease 25240.50 | 2394.00 | 4194.75 | 4796.00 | 2394.00 | . item_cnt_min 1.00 | 0.00 | 1.00 | 0.00 | 0.00 | . item_cnt_max 3.00 | 0.00 | 3.00 | 0.00 | 0.00 | . item_cnt_mean 1.67 | 0.00 | 2.00 | 0.00 | 0.00 | . item_cnt_std 1.15 | 0.00 | 1.00 | 0.00 | 0.00 | . item_cnt_shifted1 3.00 | 0.00 | 1.00 | 0.00 | 0.00 | . item_cnt_shifted2 1.00 | 0.00 | 2.00 | 0.00 | 0.00 | . item_cnt_shifted3 1.00 | 0.00 | 3.00 | 0.00 | 0.00 | . item_trend -1.00 | 0.00 | -0.75 | 0.00 | 0.00 | . shop_mean 0.13 | 0.13 | 0.13 | 0.13 | 0.13 | . item_mean 0.70 | 0.05 | 0.07 | 0.00 | 0.05 | . shop_item_mean 0.28 | 0.00 | 0.12 | 0.00 | 0.00 | . year_mean 0.22 | 0.22 | 0.22 | 0.22 | 0.22 | . month_mean 0.17 | 0.17 | 0.17 | 0.17 | 0.17 | . X_test.describe().T . count mean std min 25% 50% 75% max . Unnamed: 0 214200.00 | 6430389.42 | 78049.11 | 651488.00 | 6381778.75 | 6431162.50 | 6480756.00 | 6530238.00 | . shop_id 214200.00 | 31.64 | 17.56 | 2.00 | 16.00 | 34.50 | 47.00 | 59.00 | . item_id 214200.00 | 11019.40 | 6252.64 | 30.00 | 5381.50 | 11203.00 | 16071.50 | 22167.00 | . item_price 214200.00 | 239.96 | 1852.61 | 0.00 | 0.00 | 0.00 | 0.00 | 166944.50 | . mean_item_price 214200.00 | 133.81 | 865.99 | 0.00 | 0.00 | 0.00 | 0.00 | 32990.00 | . item_cnt 214200.00 | 0.23 | 0.95 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . mean_item_cnt 214200.00 | 0.13 | 0.37 | 0.00 | 0.00 | 0.00 | 0.00 | 10.00 | . transactions 214200.00 | 0.20 | 0.73 | 0.00 | 0.00 | 0.00 | 0.00 | 15.00 | . year 214200.00 | 2015.00 | 0.00 | 2015.00 | 2015.00 | 2015.00 | 2015.00 | 2015.00 | . month 214200.00 | 9.00 | 0.00 | 9.00 | 9.00 | 9.00 | 9.00 | 9.00 | . item_price_unit 214200.00 | inf | nan | 0.00 | 0.00 | 0.00 | 0.00 | inf | . hist_min_item_price 214200.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . hist_max_item_price 214200.00 | 5396.39 | 11365.76 | 0.00 | 1194.00 | 2428.81 | 5196.00 | 366860.00 | . price_increase 214200.00 | 239.96 | 1852.61 | 0.00 | 0.00 | 0.00 | 0.00 | 166944.50 | . price_decrease 214200.00 | 5151.84 | 10778.85 | 0.00 | 1098.00 | 2394.00 | 5063.00 | 366860.00 | . item_cnt_min 214200.00 | 0.18 | 0.74 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_max 214200.00 | 0.33 | 1.27 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_mean 214200.00 | 0.25 | 0.95 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_std 214200.00 | 0.08 | 0.43 | 0.00 | 0.00 | 0.00 | 0.00 | 12.02 | . item_cnt_shifted1 214200.00 | 0.21 | 0.91 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_shifted2 214200.00 | 0.18 | 0.83 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_cnt_shifted3 214200.00 | 0.17 | 0.85 | 0.00 | 0.00 | 0.00 | 0.00 | 20.00 | . item_trend 214200.00 | -0.08 | 0.46 | -11.00 | 0.00 | 0.00 | 0.00 | 5.00 | . shop_mean 214200.00 | 0.19 | 0.13 | 0.00 | 0.12 | 0.17 | 0.20 | 0.68 | . item_mean 214200.00 | 0.18 | 0.41 | 0.00 | 0.01 | 0.05 | 0.18 | 6.97 | . shop_item_mean 214200.00 | 0.18 | 0.56 | 0.00 | 0.00 | 0.00 | 0.12 | 20.00 | . year_mean 214200.00 | 0.22 | 0.00 | 0.14 | 0.22 | 0.22 | 0.22 | 0.22 | . month_mean 214200.00 | 0.17 | 0.00 | 0.14 | 0.17 | 0.17 | 0.17 | 0.21 | . Modeling the data . Tree based models . int_features = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;year&#39;, &#39;month&#39;] X_train[int_features] = X_train[int_features].astype(&#39;int32&#39;) X_validation[int_features] = X_validation[int_features].astype(&#39;int32&#39;) . # pandas.Categorical 자료형을 사용하면 데이터 세트의 사전 처리속도를 200배 개선할 수 있다고 합니다. X_train[int_features] = X_train[int_features].astype(&#39;category&#39;) X_validation[int_features] = X_validation[int_features].astype(&#39;category&#39;) X_test[int_features] = X_test[int_features].astype(&#39;category&#39;) . def model_performance_sc_plot(predictions, labels, title): # Get min and max values of the predictions and labels. min_val = max(max(predictions), max(labels)) max_val = min(min(predictions), min(labels)) # Create dataframe with predicitons and labels. performance_df = pd.DataFrame({&quot;Label&quot;:labels}) performance_df[&quot;Prediction&quot;] = predictions # Plot data sns.jointplot(y=&quot;Label&quot;, x=&quot;Prediction&quot;, data=performance_df, kind=&quot;reg&quot;, height=7) plt.plot([min_val, max_val], [min_val, max_val], &#39;m--&#39;) plt.title(title, fontsize=9) plt.show() . XGBoost . xgb_features = [&#39;item_cnt&#39;,&#39;item_cnt_mean&#39;, &#39;item_cnt_std&#39;, &#39;item_cnt_shifted1&#39;, &#39;item_cnt_shifted2&#39;, &#39;item_cnt_shifted3&#39;, &#39;shop_mean&#39;, &#39;shop_item_mean&#39;, &#39;item_trend&#39;, &#39;mean_item_cnt&#39;] xgb_train = X_train[xgb_features] xgb_val = X_validation[xgb_features] xgb_test = X_test[xgb_features] . xgb_model = XGBRegressor(max_depth=8, n_estimators=500, min_child_weight=1000, colsample_bytree=0.7, subsample=0.7, eta=0.3, seed=0) xgb_model.fit(xgb_train, Y_train, eval_metric=&quot;rmse&quot;, eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], verbose=20, early_stopping_rounds=20) . [08:43:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. [08:43:20] WARNING: /workspace/src/learner.cc:686: Tree method is automatically selected to be &#39;approx&#39; for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to &#39;exact&#39;. [0] validation_0-rmse:0.942842 validation_1-rmse:0.92285 Multiple eval metrics have been passed: &#39;validation_1-rmse&#39; will be used for early stopping. Will train until validation_1-rmse hasn&#39;t improved in 20 rounds. [20] validation_0-rmse:0.684505 validation_1-rmse:0.790798 Stopping. Best iteration: [18] validation_0-rmse:0.687606 validation_1-rmse:0.788279 . XGBRegressor(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.7, eta=0.3, gamma=0, importance_type=&#39;gain&#39;, learning_rate=0.1, max_delta_step=0, max_depth=8, min_child_weight=1000, missing=None, n_estimators=500, n_jobs=1, nthread=None, objective=&#39;reg:linear&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=None, subsample=0.7, verbosity=1) . plt.rcParams[&quot;figure.figsize&quot;] = (15, 6) plot_importance(xgb_model) plt.show() . xgb_train_pred = xgb_model.predict(xgb_train) xgb_val_pred = xgb_model.predict(xgb_val) xgb_test_pred = xgb_model.predict(xgb_test) . print(&#39;Train rmse:&#39;, np.sqrt(mean_squared_error(Y_train, xgb_train_pred))) print(&#39;Validation rmse:&#39;, np.sqrt(mean_squared_error(Y_validation, xgb_val_pred))) . Train rmse: 0.6921620336737511 Validation rmse: 0.7898067075909768 . Random forest . rf_features = [&#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_cnt&#39;, &#39;transactions&#39;, &#39;year&#39;, &#39;item_cnt_mean&#39;, &#39;item_cnt_std&#39;, &#39;item_cnt_shifted1&#39;, &#39;shop_mean&#39;, &#39;item_mean&#39;, &#39;item_trend&#39;, &#39;mean_item_cnt&#39;] rf_train = X_train[rf_features] rf_val = X_validation[rf_features] rf_test = X_test[rf_features] . rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1) rf_model.fit(rf_train, Y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=7, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False) . rf_train_pred = rf_model.predict(rf_train) rf_val_pred = rf_model.predict(rf_val) rf_test_pred = rf_model.predict(rf_test) . print(&#39;Train rmse:&#39;, np.sqrt(mean_squared_error(Y_train, rf_train_pred))) print(&#39;Validation rmse:&#39;, np.sqrt(mean_squared_error(Y_validation, rf_val_pred))) . Train rmse: 0.6985868322226099 Validation rmse: 0.776123635046122 . Linear models . lr_features = [&#39;item_cnt&#39;, &#39;item_cnt_shifted1&#39;, &#39;item_trend&#39;, &#39;mean_item_cnt&#39;, &#39;shop_mean&#39;] lr_train = X_train[lr_features] lr_val = X_validation[lr_features] lr_test = X_test[lr_features] . lr_scaler = MinMaxScaler() lr_scaler.fit(lr_train) lr_train = lr_scaler.transform(lr_train) lr_val = lr_scaler.transform(lr_val) lr_test = lr_scaler.transform(lr_test) . lr_model = LinearRegression(n_jobs=-1) lr_model.fit(lr_train, Y_train) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False) . lr_train_pred = lr_model.predict(lr_train) lr_val_pred = lr_model.predict(lr_val) lr_test_pred = lr_model.predict(lr_test) . print(&#39;Train rmse:&#39;, np.sqrt(mean_squared_error(Y_train, lr_train_pred))) print(&#39;Validation rmse:&#39;, np.sqrt(mean_squared_error(Y_validation, lr_val_pred))) . Train rmse: 0.7347132326333323 Validation rmse: 0.7755311093536288 . KNN Regressor . knn_features = [&#39;item_cnt&#39;, &#39;item_cnt_mean&#39;, &#39;item_cnt_std&#39;, &#39;item_cnt_shifted1&#39;, &#39;item_cnt_shifted2&#39;, &#39;shop_mean&#39;, &#39;shop_item_mean&#39;, &#39;item_trend&#39;, &#39;mean_item_cnt&#39;] # Subsample train set (using the whole data was taking too long). X_train_sampled = X_train[:100000] Y_train_sampled = Y_train[:100000] knn_train = X_train_sampled[knn_features] knn_val = X_validation[knn_features] knn_test = X_test[knn_features] . knn_scaler = MinMaxScaler() knn_scaler.fit(knn_train) knn_train = knn_scaler.transform(knn_train) knn_val = knn_scaler.transform(knn_val) knn_test = knn_scaler.transform(knn_test) . knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1) knn_model.fit(knn_train, Y_train_sampled) . KNeighborsRegressor(algorithm=&#39;auto&#39;, leaf_size=13, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=-1, n_neighbors=9, p=2, weights=&#39;uniform&#39;) . knn_train_pred = knn_model.predict(knn_train) knn_val_pred = knn_model.predict(knn_val) knn_test_pred = knn_model.predict(knn_test) . print(&#39;Train rmse:&#39;, np.sqrt(mean_squared_error(Y_train_sampled, knn_train_pred))) print(&#39;Validation rmse:&#39;, np.sqrt(mean_squared_error(Y_validation, knn_val_pred))) . Train rmse: 0.48661554779725724 Validation rmse: 0.8003593727057509 . Create new datasets with the predictions from first level . 메타 모델 학습을 위해 1차 모델들의 예측값을 갖는 데이터 프레임을 만들겠습니다. 이 데이터 프레임은 X_validation을 1차 모델로 예측한 값들로 구성됩니다. . first_level = pd.DataFrame(xgb_val_pred, columns=[&#39;xgbm&#39;]) first_level[&#39;random_forest&#39;] = rf_val_pred first_level[&#39;linear_regression&#39;] = lr_val_pred first_level[&#39;knn&#39;] = knn_val_pred first_level[&#39;label&#39;] = Y_validation.values first_level.head(20) . xgbm random_forest linear_regression knn label . 0 0.70 | 0.62 | 0.60 | 1.00 | 0 | . 1 0.64 | 0.56 | 0.04 | 0.78 | 0 | . 2 0.10 | 0.21 | 0.04 | 0.00 | 0 | . 3 0.81 | 0.95 | 0.04 | 0.11 | 4 | . 4 1.59 | 1.57 | 1.46 | 0.67 | 1 | . 5 0.64 | 0.46 | 0.04 | 0.11 | 1 | . 6 0.43 | 0.21 | 0.04 | 0.11 | 0 | . 7 0.10 | 0.06 | 0.04 | 0.00 | 1 | . 8 0.38 | 0.21 | 0.04 | 0.11 | 0 | . 9 1.73 | 1.13 | 1.15 | 2.89 | 2 | . 10 0.53 | 0.27 | 0.04 | 0.56 | 0 | . 11 0.43 | 0.41 | 0.04 | 0.11 | 0 | . 12 0.64 | 0.67 | 0.04 | 0.11 | 0 | . 13 0.53 | 0.46 | 0.04 | 0.33 | 0 | . 14 0.13 | 0.06 | 0.04 | 0.00 | 0 | . 15 0.46 | 0.28 | 0.04 | 0.22 | 1 | . 16 0.28 | 0.20 | 0.04 | 0.11 | 0 | . 17 0.53 | 0.28 | 0.04 | 0.11 | 0 | . 18 0.57 | 0.57 | 0.60 | 0.89 | 0 | . 19 0.52 | 0.60 | 1.08 | 0.33 | 0 | . 이 데이터 프레임은 X_test를 1차 모델로 예측한 값들로 구성됩니다. . first_level_test = pd.DataFrame(xgb_test_pred, columns=[&#39;xgbm&#39;]) first_level_test[&#39;random_forest&#39;] = rf_test_pred first_level_test[&#39;linear_regression&#39;] = lr_test_pred first_level_test[&#39;knn&#39;] = knn_test_pred first_level_test.head() . xgbm random_forest linear_regression knn . 0 0.41 | 0.95 | 0.85 | 0.89 | . 1 0.08 | 0.06 | 0.06 | 0.00 | . 2 0.52 | 0.85 | 1.79 | 1.11 | . 3 0.08 | 0.00 | 0.06 | 0.00 | . 4 0.08 | 0.06 | 0.06 | 0.00 | . 메타모델은 LinearRegression으로 합니다. 메타모델의 절편 값이 0이고 모든 기울기 값이 동일할 때 이 앙상블 방법은 평균과 동일합니다. . meta_model = LinearRegression(n_jobs=-1) . first_level.drop(&#39;label&#39;, axis=1, inplace=True) meta_model.fit(first_level, Y_validation) . LinearRegression(copy_X=True, fit_intercept=True, n_jobs=-1, normalize=False) . ensemble_pred = meta_model.predict(first_level) final_predictions = meta_model.predict(first_level_test) . print(&#39;Train rmse:&#39;, np.sqrt(mean_squared_error(ensemble_pred, Y_validation))) . Train rmse: 0.7649936729638841 . prediction_df = pd.DataFrame(test[&#39;ID&#39;], columns=[&#39;ID&#39;]) prediction_df[&#39;item_cnt_month&#39;] = final_predictions.clip(0., 20.) prediction_df.to_csv(path + &#39;submission.csv&#39;, index=False) prediction_df.head(10) . ID item_cnt_month . 0 0 | 0.82 | . 1 1 | 0.08 | . 2 2 | 1.27 | . 3 3 | 0.06 | . 4 4 | 0.08 | . 5 5 | 0.94 | . 6 6 | 1.24 | . 7 7 | 0.21 | . 8 8 | 1.99 | . 9 9 | 0.06 | .",
            "url": "https://edypidy.github.io/studyblog/kaggle/jupyter/meta-model/time-series/xgboost/randomforest/linearregression/knn/2021/12/26/kaggle_study-_Predict_Future_Sales.html",
            "relUrl": "/kaggle/jupyter/meta-model/time-series/xgboost/randomforest/linearregression/knn/2021/12/26/kaggle_study-_Predict_Future_Sales.html",
            "date": " • Dec 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "[SSUDA][Kaggle Review]Heart Desease 데이터 셋 분석과 train_test split에 따른 Accuracy",
            "content": ". Data copyright . 이번에 분석해볼 데이터는 캐글의 Heart Desease 데이터입니다. 출처는 아래와 같습니다. . https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset . Intro . 개인적으로 진행중인 kaggle review에서의 이번주차 데이터 셋입니다. 캐글 코드를 매우 많이 참고한 점을 밝힙니다. 참고한 코드 주소는 아래에 참고한 플랏에서 밝혀놓았습니다. . 아래의 바닐라 모델 중 Accuracy가 좋은 것을 튜닝으로 개선시켜 볼 것입니다. . XGboost, AdaBoost, Lgbm, Logistic Regression, RandomForest, MLPClassifier . Data description . Age : Age of the patient . | Sex : Sex of the patient . | exang: exercise induced angina (1 = yes; 0 = no) . | ca: number of major vessels (0-3) . | cp : Chest Pain type chest pain type . Value 1: typical angina Value 2: atypical angina Value 3: non-anginal pain Value 4: asymptomatic . | trtbps : resting blood pressure (in mm Hg) . | chol : cholestoral in mg/dl fetched via BMI sensor . | fbs : (fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false) . | rest_ecg : resting electrocardiographic results . Value 0: normal Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV) Value 2: showing probable or definite left ventricular hypertrophy by Estes&#39; criteria . | thalach : maximum heart rate achieved . | target : 0= less chance of heart attack 1= more chance of heart attack . | . Preparation . Libraries &amp; Workspace setting . import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;) import scipy.stats as ss sns.set_palette(&#39;deep&#39;) sns.set_color_codes() sns.set_style(&#39;white&#39;) . . Load Data . df = pd.read_csv(&#39;https://raw.githubusercontent.com/edypidy/Datasets/main/Heart%20Attack%20Analysis%20%26%20Prediction%20Dataset/heart.csv&#39;) df.head(3) . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . Train Test Split . 여러 모델에 적용하기 위해선 스케일 조정 작업이 필요해 보이고, 연속형, 명목형, 순서형 변수들의 구분이 필요하다. 순서형 변수의 경우 스케일을 맞추기 위해 one-hot encoding을 하는 것이 좋아 보인다. . Continuous : age, trtbps, chol, thalachh, oldpeak Cat_Ordered : cp, restecg, slp, caa, thall Cat_Nominal : sex, fbs, exng . continuous = [&#39;age&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;thalachh&#39;, &#39;oldpeak&#39;] cat_ordered = [&#39;cp&#39;, &#39;restecg&#39;, &#39;slp&#39;, &#39;caa&#39;, &#39;thall&#39;] cat_nominal = [&#39;sex&#39;, &#39;fbs&#39;, &#39;exng&#39;] categorical = cat_ordered + cat_nominal . . # train_test가 나눠져 있지 않다. 따로 빼두자.. full = pd.get_dummies(df, columns=categorical) . . train, test로 나눠져있는 것이 아닌 feature - target 이 모두 있는 데이터 셋이다. 모델의 최종 검증용 데이터가 따로 있는 것이 아니기 때문에 데이터를 split 해놓을 필요가 있다.(최대한 실전 문제처럼 해보자) . from sklearn.model_selection import train_test_split train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42) train_full, test_full = train_test_split(full, test_size=0.2, shuffle=True, random_state=42) . stratify를 설정 해야할지 고민을 좀 했다. 하지만 실제 데이터 셋이라면 target(output)의 비율이 완전히 동일할리가 없다(분포는 비슷하겠지만). (default)shuffle=True 로 둔다. . print(&#39;train shape &amp; target ratio : &#39;, train.shape, train_full.shape, &#39;%.2f&#39;%train.output.mean()) print(&#39;test shape &amp; target ratio : &#39;, test.shape, test_full.shape, &#39;%.2f&#39;%test.output.mean()) . . train shape &amp; target ratio : (242, 14) (242, 31) 0.55 test shape &amp; target ratio : (61, 14) (61, 31) 0.52 . target의 비율이라도 잘 나눠졌는지 확인하고 시작하자. . EDA . Peek . Isna? . train.isna().apply(pd.value_counts) . . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . False 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | 242 | . Missing value는 없다. . General Stats . train.describe() . . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . count 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | 242.000000 | . mean 54.462810 | 0.657025 | 0.991736 | 130.359504 | 246.842975 | 0.128099 | 0.553719 | 150.115702 | 0.314050 | 1.013223 | 1.421488 | 0.681818 | 2.301653 | 0.549587 | . std 9.204492 | 0.475687 | 1.022533 | 16.828858 | 52.795465 | 0.334893 | 0.530410 | 22.352398 | 0.465098 | 1.102577 | 0.607724 | 0.990620 | 0.593811 | 0.498566 | . min 29.000000 | 0.000000 | 0.000000 | 94.000000 | 131.000000 | 0.000000 | 0.000000 | 88.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 48.000000 | 0.000000 | 0.000000 | 120.000000 | 212.000000 | 0.000000 | 0.000000 | 136.000000 | 0.000000 | 0.000000 | 1.000000 | 0.000000 | 2.000000 | 0.000000 | . 50% 55.500000 | 1.000000 | 1.000000 | 130.000000 | 239.500000 | 0.000000 | 1.000000 | 154.000000 | 0.000000 | 0.800000 | 1.000000 | 0.000000 | 2.000000 | 1.000000 | . 75% 61.000000 | 1.000000 | 2.000000 | 140.000000 | 274.750000 | 0.000000 | 1.000000 | 165.750000 | 1.000000 | 1.600000 | 2.000000 | 1.000000 | 3.000000 | 1.000000 | . max 77.000000 | 1.000000 | 3.000000 | 192.000000 | 564.000000 | 1.000000 | 2.000000 | 202.000000 | 1.000000 | 5.600000 | 2.000000 | 4.000000 | 3.000000 | 1.000000 | . Univariate Analysis . 캐글 탐색 중에 바이올린 차트를 너무 세련되게 그려놓은 분이 있어서 참고 해봤다. 주소는 아래와 같다. . https://www.kaggle.com/abhinavgargacb/heart-attack-eda-predictor-95-accuracy-score#Exploratory-Data-Analysis- . fig = plt.figure(figsize=(24, 6)) axes = [fig.add_subplot(1,5,i) for i in range(1, 6)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(5): var = continuous[i] ax = axes[i] ax.grid(axis=&#39;y&#39;, linestyle=&#39;:&#39;) ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) color = sns.color_palette(&#39;deep&#39;)[i] sns.violinplot(data=df, y=var, ax=ax, color=color) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . age : 표본이 중장년층을 중심으로 퍼져있다. 아무래도 심장병이라는 질환의 특성상 병원까지 와서 표본으로 수집되는 어린 환자는 적고, 고령층의 경우 인구수가 적어서 그런 것으로 생각된다. | trtbps : 일정 범위 내에서 정규분포와 같은 모습을 보이나 오른쪽 꼬리가 꽤 길게 늘어져 있다. | chol : 콜레스테롤은 대표적인 심장병의 원인 중 하나로 꼽힌다. 콜레스테롤이 아주 높은 outlier가 보인다. | thalachh : 왼쪽 꼬리가 꽤나 두꺼운 분포이다. (최대 심박수가 60인 사람은 평소 심박수가 몇일지 궁금해진다) | oldpeak : 오른쪽 꼬리가 아주 두껍고 멀리 떨어진 outlier가 보인다. | . fig = plt.figure(figsize=(24, 10)) axes = [fig.add_subplot(2,4,i) for i in range(1, 9)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(8): var = categorical[i] ax = axes[i] ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) sns.countplot(data=df, x=var, ax=ax) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . 모든 특성에서 범주간의 불균형이 보인다. | 특히 restecg, slp, caa, thall 에서 아주 심하고 fbs와 cp 에서도 큰 불균형이 발견된다. exng는 다른 특성들에 비해 불균형이 심해보이진 않는다. | . Bivariate Analysis . 여기서부턴 test의 target값이 df에 포함 되었기 때문에 원본 데이터를 보지 못한다.(실전이라면 univariate analysis까지만 데이터를 합하여 볼 수 있을 것이다.) train 데이터로 보도록 하자.(실전처럼!) . fig = plt.figure(figsize=(24, 10)) axes = [fig.add_subplot(2,4,i) for i in range(1, 9)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(8): var = categorical[i] ax = axes[i] ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) colorIndex = (2*(i)) % 10 color1 = sns.color_palette(&#39;deep&#39;)[colorIndex] color2 = sns.color_palette(&#39;deep&#39;)[colorIndex + 1] sns.countplot(data=train, x=var, ax=ax, hue=&#39;output&#39;, palette=[color1, color2]) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . 1이 조금 더 많은 것은 train.output의 1이 0보다 조금 더 많았던 것 때문이라 생각할 수 있겠지만 조금 더 많은 정도가 아니거나 거꾸로 0이 많은 경우가 있다면 꽤나 유의미한 특성이라 볼 수 있을 것이다. . cp, slp, caa, thall, sex, exng 특성을 보면 특정 범주에 따라 1과 0의 비율이 역전 되는 모습이 많이 보인다. | restecg, fbs의 경우엔 그다지 유의미한 변화가 없다. | . Cramer&#39;s V . 그래프를 가져온 문서에서 거의 그대로 가져왔다. Cramer&#39;s V는 두 이산형 변수의 연관성 척도로 쓰인다. 특히 비교대상 범주가 3개 이상일 때 쓰인다. . Pearson 카이제곱 통계량을 기반으로 만들어진 테스트이다.(계산적인 부분이 궁금하다면 여기를 클릭하자) . def cramers_corrected_stat(x, y): result = -1 conf_matrix = pd.crosstab(x, y) if conf_matrix.shape[0] == 2: correct = False else: correct = True chi2, p = ss.chi2_contingency(conf_matrix, correction=correct)[0:2] n = sum(conf_matrix.sum()) phi2 = chi2/n r, k = conf_matrix.shape phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1)) rcorr = r - ((r-1)**2)/(n-1) kcorr = k - ((k-1)**2)/(n-1) result = np.sqrt(phi2corr / min((kcorr-1), (rcorr-1))) return round(result, 6), round(p, 6) for var in categorical: x = train[var] y = train[&#39;output&#39;] cramersV, p = cramers_corrected_stat(x, y) print(f&#39;For variable {var}, Cramer &#39;s V: {cramersV} and p value: {p}&#39;) . . For variable cp, Cramer&#39;s V: 0.463569 and p value: 0.0 For variable restecg, Cramer&#39;s V: 0.110035 and p value: 0.08517 For variable slp, Cramer&#39;s V: 0.36724 and p value: 0.0 For variable caa, Cramer&#39;s V: 0.487659 and p value: 0.0 For variable thall, Cramer&#39;s V: 0.504115 and p value: 0.0 For variable sex, Cramer&#39;s V: 0.297875 and p value: 2e-06 For variable fbs, Cramer&#39;s V: 0.0 and p value: 0.988529 For variable exng, Cramer&#39;s V: 0.439424 and p value: 0.0 . fbs는 연관성이 없어보이고 restecg는 유의수준 $ alpha = 0.05$에서 아쉽게 기각이 되지 않는다. | 위의 두 변수를 제외한 모든 변수에서 귀무가설이 기각되었다. | . fig = plt.figure(figsize=(24, 6)) axes = [fig.add_subplot(1,5,i) for i in range(1, 6)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(5): var = continuous[i] ax = axes[i] ax.grid(axis=&#39;y&#39;, linestyle=&#39;:&#39;) ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) colorIndex = (2*(i)) % 10 color1 = sns.color_palette(&#39;deep&#39;)[colorIndex] color2 = sns.color_palette(&#39;deep&#39;)[colorIndex + 1] sns.violinplot(data=train, y=var, x=&#39;output&#39;, ax=ax, palette=[color1, color2]) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . 수집된 표본에선 의외로 환자의 연령대가 다양하다. | trtbps, chol, oldpeak 에선 output에 따라 outlier가 다르게 관측된다. | . fig = plt.figure(figsize=(24, 6)) axes = [fig.add_subplot(1,5,i) for i in range(1, 6)] fig.patch.set_facecolor(&#39;#eaeaf2&#39;) for i in range(5): var = continuous[i] ax = axes[i] ax.grid(axis=&#39;y&#39;, linestyle=&#39;:&#39;) ax.text(0.5, 1.05, var.title(), horizontalalignment=&#39;center&#39;, verticalalignment=&#39;center&#39;, fontsize=14, fontweight=&#39;bold&#39;, transform=ax.transAxes) colorIndex = (2*(i - 1)) % 10 color1 = sns.color_palette(&#39;deep&#39;)[colorIndex] color2 = sns.color_palette(&#39;deep&#39;)[colorIndex + 1] sns.kdeplot(data=train, x=var, hue=&#39;output&#39;, ax=ax, fill=True, palette=[color1, color2]) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;&#39;) . . age, thalachh, oldpeak 에서 큰 영향이 있는 것으로 보인다. | trtbps, chol 은 그다지 큰 영향이 있는 것으로 보이진 않는다. | . Kruskal-Wallis H-test . 분포 모형이 통계적으로 유의하게 차이가 있는지 확인해보자. 모든 분포가 정규성을 갖지는 않기에 평균보다는 중위수에 대한 검정을 하자. . for var in continuous: gp = train[[var, &#39;output&#39;]].groupby([&#39;output&#39;]) gp_array = [group[var].to_numpy() for name, group in gp] kstat, p = ss.kruskal(*gp_array) kstat, p = round(kstat, 6), round(p, 6) print(f&#39;For variable {var}, Kruskal-Wallis H-test: {kstat} and p value: {p}&#39;) . . For variable age, Kruskal-Wallis H-test: 14.700765 and p value: 0.000126 For variable trtbps, Kruskal-Wallis H-test: 1.288149 and p value: 0.256389 For variable chol, Kruskal-Wallis H-test: 2.346947 and p value: 0.125529 For variable thalachh, Kruskal-Wallis H-test: 36.863825 and p value: 0.0 For variable oldpeak, Kruskal-Wallis H-test: 42.479336 and p value: 0.0 . Point Biserial test . 다른 검정을 하고싶다면 피어슨 상관계수와 값이 동일하긴 하지만 Point Biserial test를 해보자 . for var in continuous: pbistat, p = ss.pointbiserialr(train[var], train[&#39;output&#39;]) pbistat, p = round(pbistat, 6), round(p, 6) print(f&#39;For variable {var}, Point Biserial : {pbistat} and p value: {p}&#39;) . . For variable age, Point Biserial : -0.233782 and p value: 0.000244 For variable trtbps, Point Biserial : -0.104257 and p value: 0.10569 For variable chol, Point Biserial : -0.057714 and p value: 0.371366 For variable thalachh, Point Biserial : 0.393415 and p value: 0.0 For variable oldpeak, Point Biserial : -0.447305 and p value: 0.0 . 두 검정 모두 동일하게 귀무가설을 채택&amp;기각 하였다. 두 결과에서 trtbps, chol 모두 생각보다 작은 값을 보인다. | . Correlation of continuous variables(pearson) . fig = plt.figure(figsize=(12, 8)) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) corr_matrix = train[continuous].corr() mask = np.triu(np.ones_like(corr_matrix)) sns.heatmap(corr_matrix, cmap=&#39;Reds&#39;, annot=True, mask=mask) plt.show() . . 크기가 커봐야 0.4 정도이다. 그다지 높은 선형 상관을 띠는 특성 쌍은 없다. | . Correlation of categorical variables(Cramer&#39;s V) . fig = plt.figure(figsize=(12, 8)) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) corr_matrix = train[categorical].corr(method=lambda x, y: cramers_corrected_stat(x, y)[0]) mask = np.triu(np.ones_like(corr_matrix)) sns.heatmap(corr_matrix, cmap=&#39;Reds&#39;, annot=True, mask=mask) plt.show() . . 커봐야 0.43정도이다. 강한 상관을 보이는 특성쌍은 없는 것으로 보인다. | . EDA Conclusion . Continuous . outlier가 많이 관측되었다. 모델 적합시 영향을 많이 줄 것으로 보인다. . | 스케일을 맞출 필요가 있다. . | oldpeak의 경우 분포가 오른쪽 꼬리를 길게 달고 있다. 하지만 변수에 대한 정확한 설명이 없어 변환이 어려울듯 하다. . | p-value가 생각보다 작아 trtbps, chol이 output 값과 아예 연관이 없다고 보기는 어려울 수 있다. . | 특성들 간의 선형 상관관계는 없는 것으로 보인다. . | . Categorical . 모든 특성에서 범주 간의 심한 불균형이 관측된다. . | fbs의 경우 심한 불균형이 있고 output과 연관이 거의 없는 것으로 검정 되었으므로 제거할 필요가 있어보인다. . | 특성들 간의 상관관계는 없는 것으로 보인다. . | . Model comparison . # Preprocessing from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, make_scorer from sklearn.metrics import mean_squared_error,r2_score from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler # Basic Model from sklearn.svm import SVC from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_validate # Boosting Model from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier from xgboost import XGBClassifier # Neural Network Model from sklearn.neural_network import MLPClassifier . . X_train = train_full.drop([&#39;fbs_0&#39;, &#39;fbs_1&#39;, &#39;output&#39;], axis=1) y_train = train_full[&#39;output&#39;] X_test = test_full.drop([&#39;fbs_0&#39;, &#39;fbs_1&#39;, &#39;output&#39;], axis=1) y_test = test_full[&#39;output&#39;] scaler = StandardScaler() scaler.fit(X_train[continuous]) X_train[continuous] = scaler.transform(X_train[continuous]) X_test[continuous] = scaler.transform(X_test[continuous]) # one-hot encoding is already done . models = { &#39;SVM&#39;: SVC(), &#39;Random Forest&#39;: RandomForestClassifier(), &#39;Logistic Regression&#39;: LogisticRegression(), &#39;Gradient Boosting&#39;: GradientBoostingClassifier(), &#39;AdaBoost Classifier&#39;: AdaBoostClassifier(), &#39;XGBoost Classifier&#39; : XGBClassifier(), &#39;MultiLayer Perceptron Classifier&#39; : MLPClassifier() } scoring = {&#39;Accuracy&#39;: make_scorer(accuracy_score), &#39;F1_score&#39;: make_scorer(f1_score), &#39;Recall&#39; : make_scorer(recall_score), &#39;Precision&#39; : make_scorer(precision_score)} . scores = pd.DataFrame({}) for name, model in models.items(): score = cross_validate(model, X_train, y_train, scoring=scoring) temp = pd.DataFrame(score).mean() scores[name] = temp scores = scores.T.drop(&#39;score_time&#39;, axis=1) scores . fit_time test_Accuracy test_F1_score test_Recall test_Precision . SVM 0.005021 | 0.826020 | 0.842251 | 0.856125 | 0.832761 | . Random Forest 0.150434 | 0.805782 | 0.819559 | 0.811681 | 0.832945 | . Logistic Regression 0.008422 | 0.834439 | 0.851540 | 0.871795 | 0.835409 | . Gradient Boosting 0.091337 | 0.818027 | 0.836435 | 0.849288 | 0.826028 | . AdaBoost Classifier 0.076684 | 0.768367 | 0.795258 | 0.819088 | 0.775694 | . XGBoost Classifier 0.058072 | 0.801531 | 0.821968 | 0.834473 | 0.810124 | . MultiLayer Perceptron Classifier 0.260454 | 0.834779 | 0.848088 | 0.841880 | 0.857307 | . fig = plt.figure(figsize=(15, 4)) fig.add_subplot(121) fig.patch.set_facecolor(&#39;#eaeaf2&#39;) sns.set_style(&#39;whitegrid&#39;) plt.title(&#39;Models CV Accuracy&#39;) sns.barplot(scores.test_Accuracy, scores.index, alpha=0.85) plt.xlim((0.75, 0.86)) fig.add_subplot(122) plt.title(&#39;Models CV F1-score&#39;) sns.barplot(scores.test_F1_score, scores.index, alpha=0.9) plt.yticks([]) plt.xlim((0.75, 0.86)) plt.tight_layout() plt.show() . . Logistic Regression 모델이 Accuracy Score, F1 Score 모두 가장 높다. | . GridSearchCV . from sklearn.model_selection import GridSearchCV model = LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=500, random_state=42) params = {&#39;C&#39; : np.arange(0.1, 1, 0.1), &#39;l1_ratio&#39; : np.arange(0, 1, 0.05)} gs = GridSearchCV(estimator=model, param_grid=params, scoring=make_scorer(accuracy_score), cv=5, n_jobs=-1) gs.fit(X_train, y_train) . . GridSearchCV(cv=5, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=500, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;elasticnet&#39;, random_state=42, solver=&#39;saga&#39;, tol=0.0001, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=-1, param_grid={&#39;C&#39;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), &#39;l1_ratio&#39;: array([0. , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=make_scorer(accuracy_score), verbose=0) . print(gs.best_params_) . {&#39;C&#39;: 0.30000000000000004, &#39;l1_ratio&#39;: 0.1} . 혹시나 해서 elasticnet penalty를 적용해봤지만 l2 만으로도 충분해보인다. . model= LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=500, C=0.3, l1_ratio=0.1, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) df_cm = pd.DataFrame(cm) sns.heatmap(df_cm, annot=True, cmap=&#39;Reds&#39;) plt.title(&#39;Confusion Matrix for Logistic&#39;, fontsize=15) plt.xlabel(&quot;Predicted&quot;) plt.ylabel(&quot;True&quot;) plt.show() print(&#39;Accuracy : &#39;,accuracy_score(y_test, y_pred)) print(&#39;F1-score : &#39;,f1_score(y_test, y_pred)) . . Accuracy : 0.8852459016393442 F1-score : 0.8888888888888888 . 점수가 기대한 것 보다 잘 나오지 않았다.혹시 데이터셋이 얼마나 잘 나눠졌느냐에 따라 모델의 점수가 달라지지 않을까? 라는 의심이 든다. 데이터 셋의 크기가 작기 때문에 random_state에 따라 점수가 크게 달라질 수도 있겠다. 우선 random_state = 65로 나눠진 데이터 셋에 대해 실험해보자. . | . &#49892;&#54744; . random_state=65 &#47196; &#49892;&#54744; . # random_state=65 dataset prep X = df[[&#39;sex&#39;, &#39;restecg&#39;, &#39;cp&#39;, &#39;exng&#39;, &#39;thall&#39;, &#39;caa&#39;, &#39;slp&#39;, &#39;age&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;thalachh&#39;, &#39;oldpeak&#39;]] y = df[&#39;output&#39;] # models &amp; scoring models = { &#39;SVM&#39;: SVC(), &#39;Random Forest&#39;: RandomForestClassifier(), &#39;Logistic Regression&#39;: LogisticRegression(), &#39;Gradient Boosting&#39;: GradientBoostingClassifier(), &#39;AdaBoost Classifier&#39;: AdaBoostClassifier(), &#39;XGBoost Classifier&#39; : XGBClassifier(), &#39;MultiLayer Perceptron Classifier&#39; : MLPClassifier() } scoring = {&#39;Accuracy&#39;: make_scorer(accuracy_score), &#39;F1_score&#39;: make_scorer(f1_score), &#39;Recall&#39; : make_scorer(recall_score), &#39;Precision&#39; : make_scorer(precision_score)} . . encode_columns = categorical.copy() encode_columns.remove(&#39;fbs&#39;) X = pd.get_dummies(X, columns=encode_columns) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65) #this scaler = StandardScaler() scaler.fit(X_train[continuous]) X_train[continuous] = scaler.transform(X_train[continuous]) X_test[continuous] = scaler.transform(X_test[continuous]) . scores = pd.DataFrame({}) for name, model in models.items(): score = cross_validate(model, X_train, y_train, scoring=scoring) temp = pd.DataFrame(score).mean() scores[name] = temp scores = scores.T.drop(&#39;score_time&#39;, axis=1) scores . . fit_time test_Accuracy test_F1_score test_Recall test_Precision . SVM 0.004664 | 0.810034 | 0.829819 | 0.856410 | 0.806929 | . Random Forest 0.150327 | 0.789456 | 0.811416 | 0.833903 | 0.792085 | . Logistic Regression 0.010863 | 0.834864 | 0.853075 | 0.886610 | 0.825573 | . Gradient Boosting 0.088992 | 0.743793 | 0.776820 | 0.818519 | 0.741690 | . AdaBoost Classifier 0.076208 | 0.747874 | 0.772512 | 0.788319 | 0.763996 | . XGBoost Classifier 0.034040 | 0.776956 | 0.805907 | 0.856695 | 0.763046 | . MultiLayer Perceptron Classifier 0.293770 | 0.809949 | 0.830393 | 0.856125 | 0.807823 | . CV라서 그런지 생각보다 드라마틱한 변화는 없다. 오히려 내려간 느낌이다. 데이터 셋이 작기 때문에 당연한 것이라 생각된다. . from sklearn.model_selection import GridSearchCV model = LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, max_iter=500, random_state=42) params = {&#39;C&#39; : np.arange(0.1, 1, 0.1), &#39;l1_ratio&#39; : np.arange(0, 1, 0.05)} gs = GridSearchCV(estimator=model, param_grid=params, scoring=make_scorer(accuracy_score), cv=5, n_jobs=-1) gs.fit(X_train, y_train) . . GridSearchCV(cv=5, error_score=nan, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=500, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;elasticnet&#39;, random_state=42, solver=&#39;saga&#39;, tol=0.0001, verbose=0, warm_start=False), iid=&#39;deprecated&#39;, n_jobs=-1, param_grid={&#39;C&#39;: array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), &#39;l1_ratio&#39;: array([0. , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=False, scoring=make_scorer(accuracy_score), verbose=0) . print(gs.best_params_) . . {&#39;C&#39;: 0.1, &#39;l1_ratio&#39;: 0.0} . model= LogisticRegression(penalty=&#39;l2&#39;, max_iter=500, C=0.1, random_state=42) model.fit(X_train, y_train) y_pred = model.predict(X_test) cm = confusion_matrix(y_test, y_pred) df_cm = pd.DataFrame(cm) sns.heatmap(df_cm, annot=True, cmap=&#39;Reds&#39;) plt.title(&#39;Confusion Matrix for Logistic&#39;, fontsize=15) plt.xlabel(&quot;Predicted&quot;) plt.ylabel(&quot;True&quot;) plt.show() print(&#39;Accuracy : &#39;,accuracy_score(y_test, y_pred)) print(&#39;F1-score : &#39;,f1_score(y_test, y_pred)) . . Accuracy : 0.9672131147540983 F1-score : 0.967741935483871 . random_state=65 에선 점수가 무려 96.72%로 나온다. 혹시 더 잘 나눠주는 random_state는 없을까? . random_state = 0 ~ 999 . Accuarcy_list = [] F1_list = [] for i in range(1000): X = df[[&#39;sex&#39;, &#39;restecg&#39;, &#39;cp&#39;, &#39;exng&#39;, &#39;thall&#39;, &#39;caa&#39;, &#39;slp&#39;, &#39;age&#39;, &#39;trtbps&#39;, &#39;chol&#39;, &#39;thalachh&#39;, &#39;oldpeak&#39;]] y = df[&#39;output&#39;] encode_columns = categorical.copy() encode_columns.remove(&#39;fbs&#39;) X = pd.get_dummies(X, columns=encode_columns) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i) scaler = StandardScaler() scaler.fit(X_train[continuous]) X_train[continuous] = scaler.transform(X_train[continuous]) X_test[continuous] = scaler.transform(X_test[continuous]) model = LogisticRegression(penalty=&#39;l2&#39;, max_iter=500) params = {&#39;C&#39; : np.arange(0.1, 1, 0.1),} gs = GridSearchCV(estimator=model, param_grid=params, scoring=make_scorer(accuracy_score), cv=5, n_jobs=-1) gs.fit(X_train, y_train) C = gs.best_params_[&#39;C&#39;] model= LogisticRegression(penalty=&#39;l2&#39;, max_iter=500, C=C) model.fit(X_train, y_train) y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) Accuarcy_list.append(acc) F1_list.append(f1) . . plt.bar(range(100),Accuarcy_list[0:100]) plt.ylim((0.78, 1)) . (0.78, 1.0) . max(Accuarcy_list) . 0.9672131147540983 . random_state = 0 ~ 99 까지의 그래프만 그려보아도 65번이 가장 높아보이는 것을 알 수 있다. random_state=65 에서의 Accuracy가 96.72% 였으니 0~999에서도 random_state=65 에서의 점수가 가장 잘 나오는 것이라 결론지을 수 있겠다. . &#45712;&#45184;&#51216; . 데이터셋이 작은만큼 어떻게 데이터가 나눠졌는가에 따라 점수가 크게 갈린다. . 오히려 모델을 잘 선정하고 튜닝하는 것보다 데이터 셋이 잘 나눠졌을 때 점수 상승 폭이 더욱 컸다. 아무래도 이번 분석은 모델, 튜닝, 이런 것들 보다도 데이터셋의 크기의 중요성과 잘 나눠진 데이터셋이란 무엇인지에 대해 생각할 수 있던 기회였다. . &#48152;&#49457;&#47928; . 의료데이터의 데이터 포인트 하나하나는 숫자가 아니라 환자다. . 현실의 문제에선 &quot;데이터셋이 잘 정제되어있는가?&quot;, &quot;오류가 있는가?&quot;에 대한 문제보다도 먼저 고민해야할 점은 &quot;과연 데이터셋이 존재하는가? 없다면 수집 비용은 얼마인가?&quot;이다. 모델의 비교와 튜닝은 당연히 데이터셋이 존재하고 정제가 된 다음의 문제이다. 의료데이터와 같이 데이터 수집 비용이 큰 데이터의 경우 데이터의 크기가 작은 것이 당연했고 outlier가 있는 이상 데이터 분할을 섬세하게 했어야 했다. EDA 후에 outlier와 범주 불균형의 존재를 확인하고 어떻게 나눌지를 다시 고민했어야했다. . 요즘의 나는 분석 문제를 자주 마주하다보니 데이터의 가치를 종종 잊곤 한다. 그저 풀어야하는 문제로 데이터의 가치가 전락하는 것이다. 빨리빨리 데이터를 통해 모델을 적용해보고 싶은 마음에 그만 분석의 목적을 잊는 것이다. 데이터의 가치를 누구보다도 잘 알아야 할 분석자로썬 아주 아이러니한 상황인 것이다. . 데이터를 분석할 때엔 그것을 나의 문제라고 생각할 때 정말 사소한 부분까지 확인하게 된다. 하지만 최근 데이터를 문제라고 생각하다보니 데이터가 수집 되기까지의 과정을 생각해보지 않았고, 데이터 속 환자를 단순히 숫자로만 인식해버렸다. . 데이터를 분석하기 위해선 당연히 수치화가 필요하다. 객관적인 분석을 위해선 편견을 되도록 배제하는 것 또한 필요하다. 하지만 그 과정속에서 목적을 잃은체 문제 풀이식으로 모델 적용하는 것은 지양해야한다. 데이터 하나하나가 갖는 의미를 잃은체 믹서기에 갈리는 일은 없어야겠다. . 조금 웃기지만 앞으로는 수집된 표본들과 데이터 제공자에게 항상 감사한 마음으로 내 일이다 생각하고 분석을 해야겠다. . 적다보니 느낀점보단 반성문이 되었버렸다.. 잘못했으니 반성하는 것이라 생각하자. .",
            "url": "https://edypidy.github.io/studyblog/kaggle/heart%20desease/jupyter/classification/xgboost/adaboost/lgbm/logistic%20regression/randomforest/mlpclassifier/2021/12/26/kaggle_study-_Heart_Attack_Analysis_&_Prediction_Dataset.html",
            "relUrl": "/kaggle/heart%20desease/jupyter/classification/xgboost/adaboost/lgbm/logistic%20regression/randomforest/mlpclassifier/2021/12/26/kaggle_study-_Heart_Attack_Analysis_&_Prediction_Dataset.html",
            "date": " • Dec 26, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "관심사1 . 학습이론, 성공학 | 메타인지 . | 인지심리학 | 행동심리학 . | 테세우스의 배 딜레마 . | 세계사 | 국제 정치 | 경제, 마케팅(플랫폼 마케팅, 라이브 커머스) | 주식, 비트코인 . | 해석학, 선형대수학, 다변수미적분학, 벡터미적분학 | 양자역학 | 다중 우주론, 시뮬레이션 우주론 | . 관심사2 . 인디음악 | 락 | 뮤지컬 | 연극 | 클래식피아노 . | 헬스 | . 좋아하는 것1 . 요리하기 | 허튼 생각하기 | 책읽기 | 글쓰기 | 노래하기 | 산책하기 | 멍때리기 | . 좋아하는 것2 . 효율화 시키기 | 구조화 시키기 | 비틀어서 생각하기 | 당연해 보이는 것에 의문 가지기 | 일반화 시키기 | . 좋아하는 것3 . 비빔냉면, 물냉면, 제육볶음, 마라샹궈 | 육류, 샐러드, 회 | . 좋아하는 것4 . 골든 리트리버, 사모예드, 시바견, 웰시코기, 프렌치 불독, 시고르자브종 | 코리안숏헤어, 스코티쉬폴드, 먼치킨 | 고슴도치, 기니피그 | 오리, 청둥오리 | 거북이, 자라, 도마뱀 | 청개구리, 팩맨 | . 좋아하는 것5 . 프랑크 소세지 두개 + 블랑 맥주 한캔 | 쥬씨 코코넛 망고/천도복숭아 마시면서 산책하기 | 뜨뜻한 목욕탕 가서 누워있다가 시원한 바나나 우유나 웰치스 한 캔 마시면서 집에오기 | 뜨거운 라면에다 시원한 김치 감싸먹기 | 괜찮은 책 하나 사서 책장에 꽂아두기 | .",
          "url": "https://edypidy.github.io/studyblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://edypidy.github.io/studyblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}