{
  
    
        "post0": {
            "title": "[ML]로지스틱 회귀 유도 및 scikit-learn api 맛보기",
            "content": ". Intro . import matplotlib.pyplot as plt import numpy as np fig, ax = plt.subplots(1,3, figsize=(12,4)) # 시그모이드 def sigmoid(z): return 1/(1 + np.exp(-z)) z = np.linspace(-3, 3, 100) ax[0].plot(z, sigmoid(z)) ax[0].text(-2.7,0.78,&#39;$y = 1/(1+e^{-z})$&#39;, fontsize = 13) ax[0].axvline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[0].set_ylim(-0.1, 1.1) ax[0].set_title(&#39;Sigmoid&#39;) ax[0].set_xlabel(&#39;z = wx&#39;) ax[0].set_ylabel(&#39;phi(z)&#39;) ax[0].set_yticks([0, 0.5, 1]) ax[0].yaxis.grid() plt.tight_layout() # 로짓 def logit(p): return np.log(p/(1-p)) p = np.linspace(0.06, 0.94, 100) ax[1].plot(p, logit(p)) ax[1].text(0.08,1.6,&#39;$y = log(p/(1-p))$&#39;, fontsize = 13) ax[1].axhline(0.0, color=&#39;red&#39;, linewidth=0.5) ax[1].set_xlim(-0.1, 1.1) ax[1].set_title(&#39;Logit&#39;) ax[1].set_xlabel(&#39;p&#39;) ax[1].set_ylabel(&#39;logit(p)&#39;) ax[1].set_xticks([0, 0.5, 1]) ax[1].xaxis.grid() plt.tight_layout() # 합성함수 p = np.linspace(0.06, 0.94, 100) ax[2].plot(p, sigmoid(logit(p))) ax[2].text(0.62,0.42,&#39;$y = p$&#39;, fontsize = 15) ax[2].set_xlim(0, 1) ax[2].set_ylim(0, 1) ax[2].set_title(&#39;Sigmoid(Logit)&#39;) ax[2].set_xlabel(&#39;p&#39;) ax[2].set_ylabel(&#39;phi(logit(p))&#39;) ax[2].grid() plt.tight_layout() plt.show() . . 나는 학교 회귀분석 과목에서 로지스틱 함수를 배울때 왜 &#39;오즈&#39; 라는 개념을 굳이 사용할까 라는 의문을 항상 가지면서 공부했다. 교수님께 여쭤보았을 땐 그저 &quot;오즈는 승산이죠~&quot; 라는 답변을 주셨지만 도통 그 의미를 이해할 수 없었다. 어차피 확률과 일대일 대응인 오즈라는 개념을 굳이 왜 쓰는 것인가? 확률이 더 직관적이고 좋은데 말이다. 고민 끝에 내린 답은 선형 회귀에서의 예측되는 확률이 엇나가는 것을 로짓 변환을 통해 미연에 방지할 수 있기 때문이라는 것이다. . 확인을 위해 확률을 단순 선형회귀로 예측하는 것과 로지스틱 회귀로 예측한 것을 비교해보자. . Data preparation . np.random.seed(1) data0_input = 0.3*np.random.randn(30) data1_input = 1 + 0.5*np.random.randn(30) data0_target = np.zeros(30) data1_target = np.ones(30) data0 = np.column_stack((data0_input, data0_target)) data1 = np.column_stack((data1_input, data1_target)) # 데이터 로드 data = np.row_stack((data0, data1)) data = np.random.permutation(data) print(data[:5]) . [[-0.20511836 0. ] [-0.03686707 0. ] [ 0.17484456 0. ] [ 0.44134483 1. ] [ 0.82532864 1. ]] . plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . &#54869;&#47456; &#50696;&#52769;&#49884; Linear Regression&#51032; &#47928;&#51228;&#51216; &#48143; activation function &#52628;&#47200; . input, target = data[:,0].reshape(-1,1), data[:,1].reshape(-1,) # 단순선형회귀 from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(input, target) print(lr.score(input, target)) print(lr.coef_, lr.intercept_) . 0.6788575359335094 [0.64269595] 0.17212798057869722 . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, 0.6427*x + 0.1721, label=&#39;lr&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.vlines(-0.1721/0.6427, 0., 0.5, linestyles=&#39;--&#39;) plt.text(-0.5, 0.7, &#39;Does it looks like p=0.5?&#39;) plt.grid() plt.legend() plt.show() . 단순 선형회귀로 확률을 추정해보았으나 확률이 0과 1을 넘어가는 기이한 형상을 띤다. 이는 잘못된 추정이라 볼 수 있다. 시그모이드가 0과 1사이의 값으로 바꿔준다는 말에 혹해서 시그모이드 함수에 단순선형회귀식을 넣어보더라도 전혀 데이터의 분포를 말해주지 못하는 것 같다. . 문제를 해결할 방법으로는 단순선형회귀로 추정값이 로그값이면 된다. 단순선형회귀 추정값의 범위로 실수 전체가 타당해질 수 있다. 그렇다면 $ log(p) = mathbf{w^T x}$ 로 추정한다면 괜찮을까? 아쉽지만 로그확률을 다시 확률로 해석할 때엔 지수함수가 쓰인다(활성화 함수).따라서 $p = e^{ log(p)} = e^{ mathbf{w^T x}}$가 0 근처에선 좋은 확률의 추정치를 줄 수는 있지만 1을 넘어가는 외삽에선 그다지 쓸모가 없을 것이다. . | 활성화 함수의 범위도 중요하다는 것을 위에서 알았다. 이제 추정값과 활성화함수 짝꿍을 위한 조건은 아래와 같다. . 단순선형회귀로 추정하는 대상이 로그값이면서 | 확률로의 활성화함수가 0과 1사이의 값을 가져야한다. | | . 마침 로그오즈가 확률$p$의 함수이면서 동시에 역함수(시그모이드 함수)가 0과 1사이의 값인 것이다. 따라서 아래와 같은 추정이 가능해졌다. . $$inverse of log( frac{p}{1-p})= frac{1}{1+e^{-z}}$$ . $$ log( frac{p}{1-p}) = z = mathbf{w^T x}$$ . $$ frac{1}{1+e^{-z}} = frac{1}{1+e^{- mathbf{w^T x}}} = frac{1}{1+e^{- log(p/1-p)}} = hat{p}$$ . 추정과정만 보면 $ hat{p} = frac{1}{1+e^{- mathbf{w^T x}}}$ 이겠다. . L2&#44144;&#47532; &#48708;&#50857;&#54632;&#49688;&#51032; &#47928;&#51228;&#51216; . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(20*(x-0.5)), label=&#39;rule of thumb sigmoid&#39;) plt.plot(x, sigmoid(0.6427*x + 0.1721), label=&#39;sigmoid(lr)&#39;) # 앞서 선형모형에서 최적화한 계수들을 그저 시그모이드 함수에 넣기만 했기 때문에 시그모이드 자체에 대해서 최적화가 이루어지진 않았음 plt.scatter(data0_input, data0_target) plt.scatter(data1_input, data1_target) plt.grid() plt.legend(loc=&#39;upper left&#39;) plt.show() . 그렇다면 $ mathbf{w^Tx} = log( frac{p}{1-p})$로 생각하여 이를 단순선형회귀로 추정하고 시그모이드 함수에 넣어 확률을 추정하는 셈이 되는 것인가?. . 그건 또 아니다. $ mathbf{w^Tx} = log( frac{p}{1-p})$에서 선형회귀로 추정해버린다면 위의 그래프에서 선형회귀식을 시그모이드 함수에 넣은 것과 같다. . (단순회귀로 추정한 것이 확률이다! 라고 정의한 것에서 사실 로그오즈로 추정한거였다! 로 바뀌었을 뿐이다.) . 결국 이 경우는 회귀직선과 데이터포인트들의 거리비용을 최적화 한 것이지 시그모이드에 회귀직선을 대입한 것과 데이터포인트들과의 거리비용을 최적화한 것이 아니기에 활성화 함수와 포인트들 간에 괴리가 있다. . 그렇다면 아예 비용함수를 $ sum(y^{(i)} - frac{1}{1+e^{- mathbf{w^T x^{(i)}}}})^2$로 정의하면 어떨까? 비용함수를 만들어보자. . $$J( mathbf{w}) = frac{1}{2} sum^{n}_{i=1}( phi( mathbf{w^Tx^{(i)}}) - y^{(i)})^2$$ . $y$들은 0 또는 1의 값을 가지고 비용함수가 L2(유클리디안) 거리비용함수로 정의되어 있으므로 최적화하여 적합되는 시그모이드 함수의 모양은 아래와 같을 것이다. 두 클래스를 구분하는 데에만 초점을 맞춘다면 의미가 없진 않겠지만 확률적 의미를 부여하기엔 너무나 부족해보인다. . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(1000*x - 500)) # 유클리디언 거리가 최소가 되도록 한다면 점에 붙으려 할 것이다. plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp . Likelihood, Log Likelyhood function&#44284; &#44221;&#49324;&#54616;&#44053;&#48277; . 비용함수를 다르게 정의해야할 필요성을 위에서 보았다. 데이터가 주어져 있으니 여러 데이터포인트들이 그렇게 나올법한 확률을 최대화 시키는 가중치들을 구하면 될 것이다. 가능도함수를 목적함수로 하여 최대화하는 가중치를 찾는 것이 적당해보인다. . 마침 $z^{(i)}$를 로그오즈로 생각하기로 했으니 $ phi(z^{(i)}) = hat{p}^{(i)}$ 즉, 각각을 i번째 데이터포인트의 양성 확률로 생각할 수 있다. 각 데이터포인트들이 서로 독립적이라는 가정하에 가능도함수는 아래와 같다. . $$L( mathbf{w}) = P( mathbf{y} | mathbf{x;w}) = prod^n_{i=1}( phi(z^{(i)}))^{y^{(i)}}(1- phi(z^{(i)}))^{1-y^{(i)}}$$ . 가능도함수를 최대화 하는 것은 로그 가능도함수를 최대화하는 것과 동일하므로 다루기 쉬운 로그 가능도함수를 최대화하자. . $$l( mathbf{w}) = sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 가능도함수에 로그를 적용하면 가능도가 매우 작을 때 0으로 생략되는 것을 미연에 방지한다. 도함수도 쉽게 구할 수 있으니 일석이조다. . 경사하강법 최적화 알고리즘 사용을 위해 로그 가능도함수를 비용함수로 표현하자. . $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$$ . 비용함수의 이해를 위해 샘플이 하나일 때의 비용을 계산해보자. 해석의 편의를 위해 가중치 대신 가중치마다의 시그모이드와 참값$y$를 변수로 생각하자.(사실 샘플이 하나라서 $ mathbf{w}$ 대신 $ phi(z)$의 함수로 봐도 좋다.) . $$J( phi(z), y; mathbf{w}) = -y log phi(z) - (1-y) log (1 - phi(z))$$ . $y=1$일 때와 $y=0$일 때를 나누어 생각하면 3차원 상의 비용함수 그래프를 2차원에 그릴 수 있다. . $$J( phi(z), y; mathbf{w}) = left { begin{matrix} - log phi(z)&amp; y=1 - log (1 - phi(z))&amp; y=0 end{matrix} right.$$ . def cost_1(z): return - np.log(sigmoid(z)) def cost_0(z): return -np.log(1 - sigmoid(z)) z = np.linspace(-4, 4, 100) phi_z = sigmoid(z) plt.plot(phi_z, cost_1(z), label=&#39;J(w) where y = 1&#39;) plt.plot(phi_z, cost_0(z), linestyle=&#39;--&#39;, label=&#39;J(w) where y = 0&#39;) plt.xlim([0, 1]) plt.ylim([0, 4.1]) plt.xlabel(&#39;$ phi$(z)&#39;) plt.ylabel(&#39;J(w)&#39;) plt.legend(loc = &#39;upper center&#39;) plt.tight_layout() plt.show() . 범주 1에 속하는 샘플이 범주 1에 속할 확률을 높게 예측할수록 그렇게 예측한 가중치의 비용은 0에 가까워졌고 범주 0에 속하는 샘플이 범주 0에 속할 확률을 높게 예측할수록 역시 비용이 0에 가까워졌다. 반대로 잘못된 예측확률에는 큰 비용을 부여한다. . 즉, (맞으면 비용감소, 틀리면 비용증가)이므로 직관과 일치한다. . 클래스 1을 1같다고 하면 비용이 작아지고 . | 클래스 1을 0이라고 하면 비용이 커진다. . | . (클래스 0의 경우 반대) . 알고있는 기존의 경사하강법 규칙으로부터 로지스틱 회귀에서의 경사하강법 규칙이 잘 일반화 되어있는지 확인해보자. . $J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ]$를 $w_j$에 대하여 편미분하면 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)} frac{1}{ phi(z^{(i)})} + (1-y^{(i)}) frac{1}{(1 - phi(z^{(i)}))} ] frac{ partial phi(z^{(i)})}{ partial w_j}$$ . 한편, $$ frac{ partial phi(z^{(i)})}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)})) frac{ partial z^{(i)}}{ partial w_j} = phi(z^{(i)}) (1 - phi(z^{(i)}))x^{(i)}_j$$ 이므로 . $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum^n_{i=1} [ y^{(i)}(1 - phi(z^{(i)})) + (1-y^{(i)}) phi(z^{(i)}) ]x^{(i)}_j = - sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 이 된다. . 간단히 나타내면 $$ frac{ partial J( mathbf{w})}{ partial w_j} = - sum (y - phi(z))x_j $$ . | 선형대수적으로 쓰면 아래와 같다. $$ triangledown J_{p times 1} = - mathbf{X_{n times p}^T (y_{p times 1} - phi(X_{n times p}w_{p times 1}))} $$ . | . 다시 돌아와서 로지스틱 비용함수를 최소화하는 가중치를 찾는 것이 목표이므로 경사하강법의 방법을 적용하면 . $$ triangle w_j := - eta frac{ partial J( mathbf{w})}{ partial w_j}$$ . $$w_j = w_j + triangle w_j$$ . $$w_j := w_j + eta sum^n_{i=1} [ y^{(i)} - phi(z^{(i)}) ]x^{(i)}_j$$ . 인데, 이는 로그 가능도함수에 경사상승법을 적용하여 가능도 함수를 최대화 하는 가중치의 업데이트 방법과 동일하다. 즉, 가능도를 최대화하는 가중치와 로지스틱 비용함수를 최소화 하는 가중치는 서로 같다 . $$ mathbf{w := w + triangle w}$$ . $$ triangle mathbf{w} = - eta triangledown J( mathbf{w}) = eta triangledown l( mathbf{w})$$ . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; &#50508;&#44256;&#47532;&#51608; &#44396;&#54788; &#49892;&#54744; . #collapse-hide class LogisticRegressionGD(object): &quot;&quot;&quot;경사 하강법을 사용한 로지스틱 회귀 분류기 매개변수 eta : float 학습률(0.0 에서 1.0 사이) n_iter : int 훈련 데이터셋 반복횟수 random_state : int 가중치 무작위 초기화를 위한 난수 생성기 시드 속성 w_ : 1d-array 학습된 가중치 cost_ : list 에포크마다 누적된 로지스틱 비용 함수 값 &quot;&quot;&quot; def __init__(self, eta=0.01, n_iter=100, random_state=1): self.eta = eta self.n_iter = n_iter self.random_state = random_state def fit(self, X, y): &quot;&quot;&quot;훈련 데이터 학습 매개변수 -- X : {array-like}, shape = [n_samples, n_features] n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련데이터 y : array-like, shape = [n_samples] 타깃값 반환값 -- self : object &quot;&quot;&quot; rgen = np.random.RandomState(self.random_state) self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) self.cost_ = [] for _ in range(self.n_iter): net_input = self.net_input(X) output = self.activation(net_input) errors = (y-output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() # 오차제곱합 대신 로지스틱 비용을 계산합니다. cost = ( -y.dot(np.log(output)) - ((1-y).dot(np.log(1-output))) ) self.cost_.append(cost) return self def net_input(self, X): &quot;&quot;&quot;입력 계산&quot;&quot;&quot; return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, z): &quot;&quot;&quot;로지스틱 시그모이드 활성화 계산&quot;&quot;&quot; return 1./(1.+ np.exp(-np.clip(z, -250, 250))) def predict(self, X): &quot;&quot;&quot;단위 계단 함수를 사용하여 클래스 레이블을 반환합니다.&quot;&quot;&quot; return np.where(self.net_input(X) &gt;= 0.0, 1, 0) # 최종 입력값이 0보다 크면 시그모이드 값도 0.5보다 크다 # 아래와 동일합니다. # return np.where(self.activation(self.net_input(X)) &gt;= 0.5, 1, 0) . . lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) coef = lgr.fit(input, target).w_ print(coef) . [-7.22145597 15.28156541] . fig, ax = plt.subplots(1, 2, figsize=(15, 4)) epochs = np.arange(0,1000) ax[0].plot(epochs, lgr.cost_) ax[0].set_title(&#39;Does it converges?&#39;) ax[0].set_xlabel(&#39;Epoch&#39;) ax[0].set_ylabel(&#39;Weights&#39;) ax[0].set_xlim((1,1000)) ax[0].set_ylim((3,35)) ax[0].grid() ax[1].plot(epochs, lgr.cost_) ax[1].set_title(&#39;Does it converges?(log scale)&#39;) ax[1].set_xlabel(&#39;Epoch(log scale)&#39;) ax[1].set_xscale(&#39;log&#39;) ax[1].set_xlim((1,1000)) ax[1].set_ylim((3,35)) ax[1].grid() plt.show() . . x = np.linspace(-0.7, 2.1, 100) plt.plot(x, sigmoid(coef[0] + coef[1]*x)) plt.scatter(data0_input, data0_target, label=&#39;data0&#39;) plt.scatter(data1_input, data1_target, label=&#39;data1&#39;) plt.grid() plt.legend() plt.show() . 로지스틱 모델을 Iris-setosa와 Iris-versicolor 붖꽃만 가지고 로지스틱 회귀의 분류모델 구현이 작동하는지 확인해보자. . from sklearn import datasets import numpy as np . iris = datasets.load_iris() print(iris.data[:3]) print(iris.target[:3]) . [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2]] [0 0 0] . X = iris.data[:, [2,3]] y = iris.target print(np.unique(y)) . [0 1 2] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3, stratify=y) . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std = ss.transform(X_test) . X_train_01subset = X_train_std[(y_train == 0) | (y_train == 1)] y_train_01subset = y_train[(y_train == 0) | (y_train == 1)] lgr = LogisticRegressionGD(eta=0.05, n_iter=1000, random_state=1) lgr.fit(X_train_01subset, y_train_01subset) . &lt;__main__.LogisticRegressionGD at 0x7f603fa314d0&gt; . from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) plot_decision_regions(X_train_01subset, y_train_01subset, classifier=lgr) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . . scikit-learn &#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 사이킷런 모듈은 로지스틱 회귀 모델을 지원한다. 이 모델은 위의 구현과 달리 세개이상의 다중분류도 지원한다. . from sklearn.linear_model import LogisticRegression lgr = LogisticRegression(C=100.0, random_state=1) lgr.fit(X_train_std, y_train) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X_combined_std, y_combined, classifier=lgr, test_idx=range(len(y_train),len(y))) plt.title(&#39;LogisticRegression - GDescent&#39;) plt.xlabel(&#39;petal length[stdzed]&#39;) plt.ylabel(&#39;petal width[stdzed]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . print(lgr.coef_, lgr.intercept_) for i, w0, w1, w2 in zip(range(3), lgr.intercept_, lgr.coef_[:, 0], lgr.coef_[:, 1]): print(&#39;model{:} : {:.2f} + PL * {:.2f} + PW * {:.2f} &#39;.format(i, w0, w1, w2) ) . [[-6.93265988 -5.76495748] [-2.03192177 -0.03413691] [ 8.96458165 5.79909439]] [-0.9576182 5.70388044 -4.74626223] model0 : -0.96 + PL * -6.93 + PW * -5.76 model1 : 5.70 + PL * -2.03 + PW * -0.03 model2 : -4.75 + PL * 8.96 + PW * 5.80 . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#50640;&#49436;&#51032; &#44508;&#51228; . 규제를 사용하여 과대적합을 피하는 것은 이미 다른 문서에서 많이 다루었으므로 살짝만 하고 지나가자(cf-편향과 분산 참고). . 높은 분산은 과대적합에 비례하고(과대적합은 일반화한 모델이 너무 많은 변동을 끌어갔기 때문) 높은 편향은 과소적합에 비례한다(과소적합은 적합이 덜돼서 구조적인 편향이 발생). 과대적합의 경우에는 모델이 가지는 파라미터의 수를 줄이거나 모델이 가지는 모수들의 크기를 제한함으로써(규제) 해결할 수 있다(이 과정을 모델의 복잡도를 줄인다고 표현하기도 한다.). 반대로 과소적합의 경우에는 모델이 가지는 파라미터의 수를 늘려보는 식으로 해결할 수 있겠다. 여기서는 로지스틱 회귀의 규제에 따른 회귀계수의 변화를 보자. . 회귀계수들의 제곱항을 패널티항으로 갖는 L2규제 로지스틱 비용은 다음과 같다. $$J( mathbf{w}) = - sum^n_{i=1} [ y^{(i)} log phi(z^{(i)}) + (1-y^{(i)}) log (1 - phi(z^{(i)})) ] + frac{ alpha}{2} left | left | mathbf{w} right | right |^2$$ . 릿지 회귀과 라쏘 회귀에서 규제항을 제어하는 파라미터는 $ alpha$였지만 사이킷런의 로지스틱 회귀에서 규제항을 제어하는 파라미터는 C이다. 주의할 점은 C는 $ alpha$의 역수이다. C가 클수록 규제는 완화된다. . petal length와 petal width 계수들의 규제 크기에 따른 크기변화를 그래프로 나타내보자. 규제가 완화될수록(C가 커질수록) 회귀계수들의 크기도 커지는 경향이 있는 것을 볼 수 있다. . weights0, weights1, params = [], [], [] for C in np.arange(-5,5): lgr = LogisticRegression(C=10.**C, random_state=1, multi_class=&#39;ovr&#39;) lgr.fit(X_train_std, y_train) weights0.append(lgr.coef_[0]) weights1.append(lgr.coef_[1]) params.append(10.**C) weights0, weights1 = np.absolute(weights0), np.absolute(weights1) plt.plot(params, weights0[:,0], label=&#39;m0:petal length&#39;) plt.plot(params, weights1[:,0], label=&#39;m1:petal length&#39;, linestyle=&#39;--&#39;) plt.plot(params, weights0[:,1], label=&#39;m0:petal width&#39;, linestyle=&#39;-.&#39;) plt.plot(params, weights1[:,1], label=&#39;m1:petal width&#39;, linestyle=&#39;:&#39;) plt.title(&#39;Absolute weights&#39;) plt.xlabel(&#39;C&#39;) plt.ylabel(&#39;Absolute weight&#39;) plt.xscale(&#39;log&#39;) plt.legend() plt.show() . scikit-learn&#51032; &#48708;&#50857;&#54632;&#49688; &#52572;&#51201;&#54868; &#50508;&#44256;&#47532;&#51608; . 로지스틱 비용함수처럼 볼록한 손실함수를 최소화하는 데는 확률적 경사 하강법(SGD) 대신에 더 고급 방법을 사용하는 것이 좋다. 실제 사이킷런은 다양한 최적화 알고리즘을 제공하며 solver= 매개변수로는 아래와 같은 것들이 있다. . &#39;newton-cg&#39; | &#39;lbfgs&#39; | &#39;liblinear&#39; | &#39;sag&#39; | &#39;saga&#39; | . 다중분류 매개변수인 LogisticRegression의 multiclass=의 기본값은 &#39;auto&#39;이다. &#39;auto&#39;로 설정하면 이진 분류이거나 solver=&#39;liblinear&#39;일 경우에 &#39;ovr&#39;(One versus Rest)를 선택하고 그 외에는 &#39;multinomial&#39;을 선택한다. 이는 &#39;liblinear&#39; 최적화 알고리즘이 다항 로지스틱 회귀 손실을 다룰 수 없고 다중클래스 분류를 위해 OvR 방법을 사용해야하기 때문이다. . predict_proba() method&#50752; &#49368;&#54540; &#54616;&#45208; &#50696;&#52769;&#49884; &#51452;&#51032;&#49324;&#54637; . 확률을 예측하고 싶다면 predict_proba 메서드를 사용하여 계산하자. . print(lgr.predict_proba(X_test_std[:3, :])) . [[1.52213484e-12 3.85303417e-04 9.99614697e-01] [9.93560717e-01 6.43928295e-03 1.14112016e-15] [9.98655228e-01 1.34477208e-03 1.76178271e-17]] . print(lgr.predict_proba(X_test_std[:3, :]).argmax(axis=1)) print(lgr.predict(X_test_std[:3, :])) . [2 0 0] [2 0 0] . lgr.predict(X_test_std[0, :].reshape(1,-1)) # lgr.predict(X_test_std[0, :]) 이렇게 하면 에러난다. . array([2]) . print(&#39;단순 인덱싱 뽑기 : &#39;, X_test_std[0, :].shape) print(&#39;2차원 배열로 변환:&#39;, X_test_std[0, :].reshape(1,-1).shape) . 단순 인덱싱 뽑기 : (2,) 2차원 배열로 변환: (1, 2) .",
            "url": "https://edypidy.github.io/studyblog/jupyter/logistic%20regression/classifying/loss%20function/2021/09/25/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.html",
            "relUrl": "/jupyter/logistic%20regression/classifying/loss%20function/2021/09/25/_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80.html",
            "date": " • Sep 25, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "[ML] SVM, Kernel-SVM with Gaussian-rbf kernel와 기본적인 하이퍼파라미터들",
            "content": ". Intro . SVM 문제는 클래스가 다른 데이터들을 구분하는 초평면을 어떻게 정할 것인가에 대한 문제이다. 일반화 오차에 대한 성능을 높이기 위해 마진을 최대로 하는 초평면을 그린다. iris 데이터에 SVM을 적용해보자. . import numpy as np import pandas as pd from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) . . Data Preparation . from sklearn import datasets iris = datasets.load_iris() input = iris.data[:, [2, 3]] target = iris.target . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(input, target, test_size=0.3, stratify = target, random_state=1) . 2차원 평면에 시각화를 위해 iris data에서의 petal length와 petal width를 피쳐로 사용하자. . petal(꽃잎)의 길이와 너비로 꽃의 종류를 분류하는 문제가 되겠다.(sepal(꽃받침)보다는 그럴듯한 상관관계가 있을 것으로 예상된다.) . 잘 알려진 데이터니 만큼 전처리 과정이나 EDA는 건너뛰고 바로 피팅 해보자. . test_size=0.3 : train과 test는 7대 3으로 나누었고. stratify=target : target 꽃의 종류의 비율에 맞추어 train과 test를 나누었으며 random_state=1 : 이건 그냥 재현을 위한 시드다. . Fitting with sklearn . from sklearn.preprocessing import StandardScaler ss = StandardScaler() ss.fit(X_train) X_train_std = ss.transform(X_train) X_test_std= ss.transform(X_test) X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) . from sklearn.svm import SVC svm = SVC(kernel=&#39;linear&#39;, C=1.0, random_state=1) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105, 150)) # 빈 동그라미가 쳐진 아이들이 test 셋이다. plt.xlabel(&#39;petal length[std]&#39;) plt.ylabel(&#39;petal width[std]&#39;) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . train set에 피팅된 SVM 모델이 test_set에도 꽤나 잘 들어맞는 모양새다. 딱 두개의 피쳐만 썼는데도 말이다!. . (사실 그냥 데이터가 원래부터 잘 구분되어있긴 했다.) . 그런데 뭔가 조금 이상하다. SVM은 서로 다른 클래스를 구분하는 평면을 만든다고 했는데 어떻게 세개의 클래스는 어떻게 구분 해야하는가? . 이유인 즉슨 디폴트로 &#39;One Versus Rest&#39;가 적용되었기 때문이다. 2번 클래스의 결정 경계와 평면은 (0번 vs 1번, 2번), (1번 vs 0번, 2번)을 거치면서 &#39;0번과 1번의 영역이 정해졌으니 나머지는 2번이겠구나!&#39; 하는 식으로 결정한 셈이다. . Linear hyperlane&#51004;&#47196; &#44396;&#48516;&#46104;&#51648; &#50506;&#45716; &#45936;&#51060;&#53552; . 다 좋고 모델이 클래스를 잘 분류하는 것 같지만 실제 세계의 데이터는 이렇게 이상적이지 못한 경우가 대부분이다. 과연 &#39;직선&#39;, &#39;평면&#39; 과 같은 선형 결정경계만으로 데이터를 잘 나눌 수 있을까? . 아래의 데이터는 그냥 임의로 만들어 본(또 다른 이상적인 것 일지도 모르는) 데이터이다. 1사분면과 3사분면에는 1번이, 2사분면과 4사분면엔 -1번 클래스가 존재하도록 만든 데이터이다.(x, y의 기울기 탄젠트와 클래스가 관련이 있다면 충분히 있을 수 있는 데이터 셋이다) . np.random.seed(1) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &lt; 0) y_xor = np.where(y_xor, 1, -1) plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], color=&#39;b&#39;, marker=&#39;^&#39;, label=&#39;1&#39;) plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1], color=&#39;r&#39;, marker=&#39;v&#39;, label=&#39;-1&#39;) plt.xlim([-3, 3]) plt.ylim([-3, 3]) plt.hlines(0, -3, 3) plt.vlines(0, -3, 3) plt.legend() plt.tight_layout() plt.show() . 이런 데이터에 Linear SVM Classifier를 적용하면 어떻게 될까? . 당연한 이야기겠지만 선형 초평면으로는 1번 클래스와 -1번 클래스를 절대 나눌 수 없다. . svm = SVC(kernel=&#39;linear&#39;,random_state=1) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend(loc=&#39;upper left&#39;) plt.tight_layout() plt.show() . 그러면 이렇게 해보자. 모든 데이터의 x,y 두 값을 곱해서 z열로 추가하는 것이다. . 투영된 데이터가 3차원 공간상에서 선형 초평면에 의해 분류되길 기대하는 것이다. . X_xor_3d = np.column_stack(( X_xor, X_xor[:, 0] * X_xor[:, 1])) x = np.linspace(-3, 3, 200) y = np.linspace(-3, -3, 200) x, y = np.meshgrid(x, y) fig, ax = plt.subplots(subplot_kw={&quot;projection&quot;: &quot;3d&quot;}) ax.scatter3D(X_xor_3d[y_xor == 1, 0], X_xor_3d[y_xor == 1, 1], X_xor_3d[y_xor == 1, 2], color=&#39;b&#39;, marker=&#39;^&#39;, label=&#39;1&#39;) ax.scatter3D(X_xor_3d[y_xor == -1, 0], X_xor_3d[y_xor == -1, 1], X_xor_3d[y_xor == -1, 2], color=&#39;r&#39;, marker=&#39;v&#39;, label=&#39;-1&#39;) ax.view_init(2,85) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;z&#39;) plt.legend() plt.tight_layout() plt.show() . 첫번째 열과 두번째 열을 곱하여 3차원 공간에 투영시킨 모습이다. . $z = 0$ 평면으로 두 클래스를 가를 수 있다. . Kenel-SVM&#44284; Kernel&#51032; &#51333;&#47448;&#46308; . 이와 같이 기존의 데이터를 통해 새로운 차원을 추가하여 선형적으로 구분이 되게끔 하는 모델을 Kernel-SVM이라고 한다. . 일반적인 커널 함수는 아래와 같다. . 선형 커널 : &#39;linear&#39; . | 다항 커널 : &#39;poly&#39; . | 가우시안 rbf : &#39;rbf&#39; . | 시그모이드 커널 : &#39;sigmoid&#39; . | . (callable한 객체를 대입할 수도 있다. 자세한 것은 도큐먼트를 참고하자.) . sklearn SVC의 디폴트 커널은 사실 linear가 아니라 가우시안 rbf 커널이다. 여기서는 rbf 커널과 규제 매개변수(하이퍼파라미터)들에 대해 살짝 알아보자. . svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=0.1, C=10, ) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend() plt.tight_layout() plt.show() . Gaussian rbf&#50752; gamma &#47588;&#44060;&#48320;&#49688; . def axes_decision_regions(X, y, classifier, axes, test_idx=None, resolution=0.02): # 마커와 컬러맵을 설정합니다. markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;) colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;) cmap = ListedColormap(colors[:len(np.unique(y))]) # 결정 경계를 그립니다. x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) axes.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap) axes.set_xlim(xx1.min(), xx1.max()) axes.set_ylim(xx2.min(), xx2.max()) # 샘플의 산점도를 그립니다. for idx, cl in enumerate(np.unique(y)): axes.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=colors[idx], marker=markers[idx], label=cl, edgecolor=&#39;black&#39;) # 테스트 샘플을 부각하여 그립니다. if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] axes.scatter(X_test[:, 0], X_test[:, 1], facecolors=&#39;none&#39;, edgecolor=&#39;black&#39;, alpha=1.0, linewidth=1, marker=&#39;o&#39;, s=100, label=&#39;test_set&#39;) gammas = [0.01, 0.1, 0.5, 5] fig, axes = plt.subplots(1,4, figsize=(20,4)) for gamma, ax in zip(gammas, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=10) svm.fit(X_xor, y_xor) axes_decision_regions(X_xor, y_xor, axes=ax, classifier=svm) ax.set_title(&#39;gamma : {:}&#39;.format(gamma)) axes[0].legend() plt.show() . . gamma가 커질수록 SVM 분류기가 1번 데이터에 과적합 되어가는 모습이다. . 과적합이 되어 가면서 1번 데이터의 결정경계가 동글동글 해지는 것을 볼 수 있는데 이를 가우시안 구라고 부른다. . gamma는 가우시안 구의 크기를 제한하는 매개변수로 이해할 수 있다(커널이 rbf인 것을 기억하자). gamma값을 크게 하면 서포트 벡터(원래 결정경계 근처의 점이라고 여길 수 있다)의 영향 범위가 줄어든다. 샘플에 더욱 적합되고 결정경계는 구불구불해진다. . &#39;rbf&#39; 커널함수(방사 기저함수)는 다음과 같다. . ($ mathbf{x}^{(i)}$는 자료 행렬의 $i$행 즉, $i$번째 데이터이다.) . $$K( mathbf{x}^{(i)}, mathbf{x}^{(j)}) = exp left ( - frac{ left | mathbf{x}^{(i)}- mathbf{x}^{(j)} right |^2}{2 sigma^2} right ) = exp left ( - gamma left | mathbf{x}^{(i)}- mathbf{x}^{(j)} right |^2 right )$$ . 여기서 $ gamma = frac{1}{2 sigma^2}$는 최적화 대상 파라미터가 아닌 하이퍼파라미터이다. . 가우시안 rbf 커널이라 불리는 이유는 가우시안 분포함수와 닮아 있어서 붙은 이름이다. 이름보다 중요한 것은 $ exp$에 있다. 직관적으로 $ exp$ 함수는 매클로린 전개시 다항식의 무한합을 가지니까 무한차원으로의 투영이라 생각할 수 있겠다. 자세한 이론적인 부분은 이 블로그{:target=&quot;_blank&quot;} 이 논문{:target=&quot;_blank&quot;} 을 참고하길 바란다. . iris 데이터에서도 확인해보자. . gammas = [0.1, 1, 5, 50] fig, axes = plt.subplots(1,4, figsize=(20,4)) for gamma, ax in zip(gammas, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=1) svm.fit(X_train_std, y_train) axes_decision_regions(X_combined_std, y_combined, axes=ax, classifier=svm, test_idx=range(105,150)) ax.set_title(&#39;gamma : {:}, test_score : {:.2f}&#39;.format(gamma, svm.score(X_test_std, y_test))) axes[0].legend() plt.show() . gamma값이 커질수록 2번 타겟을 제외한 나머지 데이터에 과적합 되어가는 것을 볼 수 있다.(One Versus Rest에 의해 n가지의 클래스 레이블이 있으므로 n-1개의 가우시안 구로 적합되어 간다.) . train_scores, test_scores=[], [] gammas = np.arange(0.1, 1000) for gamma in gammas: svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=gamma, C=1).fit(X_train_std, y_train) train_scores.append(svm.score(X_train_std, y_train)); test_scores.append(svm.score(X_test_std, y_test)) differ = [4*np.abs(tr - ts) for tr, ts in zip(train_scores, test_scores)] plt.plot(gammas, train_scores, label=&#39;train&#39;); plt.plot(gammas, test_scores, label=&#39;test&#39;); plt.plot(gammas, differ, label=&#39;differ*4&#39;) plt.vlines(34, 0, 1, linestyle=&#39;--&#39;); plt.text(1, 0.6,&#39;$gamma = 34$&#39;, fontsize=12) plt.title(&#39;Train &amp; Test scores for Gammas&#39;); plt.xlabel(&#39;Gamma&#39;); plt.ylabel(&#39;score&#39;) plt.xscale(&#39;log&#39;); plt.legend(loc=&#39;center left&#39;); plt.grid() plt.show() . 역시 gamma값이 높아질수록 과적합되어가는 모습을 볼 수 있다. . 낮았을 때는 과소적합 되어있는데 gamma값을 키우는 것으로 해결을 볼 수 있겠다. gamma = 34에서 최적인듯 하다.(데이터 수가 적어서 일반화를 하기엔 아직은 검증이 다소 필요해보인다.) . Gaussian rbf&#50752; C&#47588;&#44060;&#48320;&#49688; . 매개변수 C는 sklearn의 여러 모델들에서 과적합 규제 매개변수로써 쓰인다. 보통 $ alpha$의 역수로 생각할 수 있는데 C가 커질수록 과대적합에 대한 규제가 완화된다고 이해할 수 있다.(모델은 과적합될 가능성이 커진다.) . C = [0.01, 1, 10, 100] fig, axes = plt.subplots(1,4, figsize=(20,4)) for c, ax in zip(C, axes): svm = SVC(kernel=&#39;rbf&#39;, random_state=1, gamma=1, C=c) svm.fit(X_train_std, y_train) axes_decision_regions(X_combined_std, y_combined, axes=ax, classifier=svm, test_idx=range(105,150)) ax.set_title(&#39;C : {:}, test_score : {:.2f}&#39;.format(c, svm.score(X_test_std, y_test))) axes[0].legend() plt.show() . C가 커질수록 최적화 해야하는 비용함수에서의 패널티 함수가 커진다. 즉, C는 커질수록 분류 오차에 대한 비용을 키운다. . 주의해야 할 점은 SVM의 패널티 함수는 Ridge나 Lasso와 달리 가중치 벡터 $ mathbf{w}$의 함수가 아니라 상수 C와 분류오차 $ xi^{(i)}$를 곱하여 더한 함수라는 것이다. . 그러므로 C가 커질수록 분류 오차에 대한 비용이 커진다는 것을 모델이 분류오차 $ xi^{(i)}$의 존재에 대해 더욱 민감하게 반응한다고 이해할 수 있다. C값이 크다면 분류오차를 크게 평가하고 배제하려는 쪽으로 과대적합될 것이다.(C값이 작아서 분류오차를 무시한다면 더울 일반화 된 모델이 나올 것이다.) . 따라서 C가 커질수록 훈련 데이터셋에 과대적합이 되어가고 C가 작아질수록 모델이 일반화 된다고 확인할 수 있겠다. . 릿지와 라쏘, 로지스틱 회귀에서 $ alpha$와 C를 통해 규제의 정도를 제어할 수 있었다. SVM에서의 C도 릿지와 라쏘에서늬 $ alpha$와 반비례하고 로지스틱의 C와 동일한 역할을 수행하는 하이퍼파라미터라고 할 수 있다. . &#50836;&#50557; . svm의 SVC에는 기본적인 파라미터 X, y, kernel=&#39;rbf&#39;, gamma=&#39;scale&#39;, C=1,(random_state=None, decision_function_shape=&#39;ovr) 가 있다. 하이퍼파라미터는 kernel, gamma와 C가 있다. . kernel : 커널함수이다. 상위 차원의 매핑함수($ phi$)를 통해 상위 차원에서의 선형 초평면을 구하고 그것을 매핑함수의 역함수($ phi^{-1}$)로 원래 차원으로 가져오는 수고를 줄인 하나의 함수 $K$이다. 머서의 조건만 만족하면 $ phi$를 모르더라도 $K$가 존재하여 커널함수로 사용할 수 있음이 보장된다. 기본값은 rbf(방사 기저 함수 또는 가우시안 커널)이다. | gamma : rbf에서는 gamma가 클수록 가우시안 구의 크기를 제한하는 것으로 이해할 수 있다. 결정경계가 샘플에 가까워지고 구불구불해진다.(클수록 과적합 된다고 이해할 수도 있다.) | C : 분류오차를 얼마나 신경쓸 것인지 조절한다. 클수록 분류오차에 대해 엄격해지고 작을수록 분류오차를 무시한다. 규제 측면에서 릿지와 라쏘에서의 $ alpha$ 와 반비례하고 로지스틱의 C와 동일한 역할을 수행하는 하이퍼파라미터이다. 기본값이 1인 것을 고려하면 SVC는 하드 마진svm이 아닌 소프트 마진 svm이다. 선형 초평면으로 완전히 구분되지 않는 데이터셋에 C=0을 설정하면 에러가 난다. | . &#45908; &#44277;&#48512;&#54644;&#50556; &#54624; &#44163;&#46308; . SVM을 이론적으로 이해하려면 추가로 공부해볼만한 것들을 정리해 보았다. . 최적화 문제 라그랑주 프리멀 함수 | 라그랑주 듀얼 함수 | KKT 조건 | . | SVM 유도(하드 마진) 기본적인 아이디어 | 마진 유도 | 최적화 대상 및 제약 조건 | 라그랑주 프리멀, 듀얼 라그랑주 프리멀 함수 유도 | 프리멀 함수 w, b 편미분 = 0 | 라그랑주 듀얼 함수 유도 | | 듀얼리티 갭 KKT 조건 확인 | | . | SVM 유도(소프트 마진) 분류오차 ξ | 최적화 대상 및 제약 조건 ξ와 C | . | 라그랑주 프리멀, 듀얼 | KKT 조건 확인 | . | 커널 SVM 커널의 필요성 | 프리멀, 듀얼 문제 필요성(핸즈온 223) | 매핑 함수 | 커널 | 커널 SVM 유도 최적화 대상 및 제약 조건 | 라그랑주 프리멀, 듀얼 | KKT 조건 확인 | | 머서의 정리와 조건 | 커널 예시 | . | 추가 온라인 SVM | SVM 회귀 | 쿼드래틱 프로그래밍 | . | .",
            "url": "https://edypidy.github.io/studyblog/jupyter/svm/kernel-svm/classifying/hyper-lane/kernel/gaussian-rbf/2021/09/25/_SVM(%EC%B5%9C%EC%A2%85).html",
            "relUrl": "/jupyter/svm/kernel-svm/classifying/hyper-lane/kernel/gaussian-rbf/2021/09/25/_SVM(%EC%B5%9C%EC%A2%85).html",
            "date": " • Sep 25, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "실로 삶은 기적이다. 기적의 연속이다. 나에게 베풀어진 삶이 그러했고 앞으로 남은 삶의 하루하루도 그렇게 주어질 것이기 때문이다. .",
          "url": "https://edypidy.github.io/studyblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://edypidy.github.io/studyblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}